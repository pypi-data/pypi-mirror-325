# coding: utf-8

"""
    Shaped API

    Welcome to Shaped's API reference docs. These provide a detailed view of the endpoints and CLI commands that Shaped provides and brief explanations of how they should be used.   Shaped's API is composed of 3 components:   1. **Dataset** - used to provision and manage 'Shaped Datasets', which are persisted     data views of external data. Shaped Datasets can be created from any of our     'Shaped connectors' (e.g. S3, Segment, Snowflake, etc.) and support both batch     ingestion (up to a 15min delay) and stream ingestion (up to a 30 second     delay) depending on the specific connector used. Shaped datasets can also be     created from local files, which is particularly useful for getting started     with a snapshot of data.    2. **Model Management** - used to provision and manage 'Shaped Models', which     represent a system of data pipelines, training and serving infrastructure for     your ranking use-case.    3. **Model Inference** - a high performance API that's used to make     user-understanding requests or ranking inferences to your 'Shaped Models'. For     example, the 'rank' endpoint can be used to determine for a given user id     query, what is the content that is most engaging to that user.    The recommended workflow to interact with the Shaped API is as follows:   1. First create 'Shaped Datasets' to sync over data that your Shaped     understanding models will need. The models at the minimum need interaction     data to understand behavior of your users, so start with that and add your item     and user catalog data later.   2. Then create 'Shaped Models' that use your created 'Shaped Datasets' as     input. Your Shaped Model will will start streaming, processing and training     from your connected data immediately. After a few hours your model will have     tuned all parameters based on your data and will deploy an active model.   3. You can now use the 'Model Inference' endpoints to make real-time     inferences to your model based on your use-case. 

    The version of the OpenAPI document: 1.0.1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import json
import pprint
from pydantic import BaseModel, ConfigDict, Field, StrictStr, ValidationError, field_validator
from typing import Any, List, Optional
from shaped.autogen.models.amplitude_dataset_config import AmplitudeDatasetConfig
from shaped.autogen.models.aws_pinpoint_dataset_config import AWSPinpointDatasetConfig
from shaped.autogen.models.big_query_dataset_config import BigQueryDatasetConfig
from shaped.autogen.models.custom_dataset_config import CustomDatasetConfig
from shaped.autogen.models.mongo_db_dataset_config import MongoDBDatasetConfig
from shaped.autogen.models.my_sql_dataset_config import MySQLDatasetConfig
from shaped.autogen.models.postgres_dataset_config import PostgresDatasetConfig
from shaped.autogen.models.redshift_dataset_config import RedshiftDatasetConfig
from shaped.autogen.models.rudder_stack_dataset_config import RudderStackDatasetConfig
from shaped.autogen.models.segment_dataset_config import SegmentDatasetConfig
from shaped.autogen.models.snowflake_dataset_config import SnowflakeDatasetConfig
from pydantic import StrictStr, Field
from typing import Union, List, Set, Optional, Dict
from typing_extensions import Literal, Self

CREATEDATASETARGUMENTS_ONE_OF_SCHEMAS = ["AWSPinpointDatasetConfig", "AmplitudeDatasetConfig", "BigQueryDatasetConfig", "CustomDatasetConfig", "MongoDBDatasetConfig", "MySQLDatasetConfig", "PostgresDatasetConfig", "RedshiftDatasetConfig", "RudderStackDatasetConfig", "SegmentDatasetConfig", "SnowflakeDatasetConfig"]

class CreateDatasetArguments(BaseModel):
    """
    CreateDatasetArguments
    """
    # data type: BigQueryDatasetConfig
    oneof_schema_1_validator: Optional[BigQueryDatasetConfig] = None
    # data type: MongoDBDatasetConfig
    oneof_schema_2_validator: Optional[MongoDBDatasetConfig] = None
    # data type: SnowflakeDatasetConfig
    oneof_schema_3_validator: Optional[SnowflakeDatasetConfig] = None
    # data type: PostgresDatasetConfig
    oneof_schema_4_validator: Optional[PostgresDatasetConfig] = None
    # data type: MySQLDatasetConfig
    oneof_schema_5_validator: Optional[MySQLDatasetConfig] = None
    # data type: RedshiftDatasetConfig
    oneof_schema_6_validator: Optional[RedshiftDatasetConfig] = None
    # data type: AWSPinpointDatasetConfig
    oneof_schema_7_validator: Optional[AWSPinpointDatasetConfig] = None
    # data type: CustomDatasetConfig
    oneof_schema_8_validator: Optional[CustomDatasetConfig] = None
    # data type: AmplitudeDatasetConfig
    oneof_schema_9_validator: Optional[AmplitudeDatasetConfig] = None
    # data type: SegmentDatasetConfig
    oneof_schema_10_validator: Optional[SegmentDatasetConfig] = None
    # data type: RudderStackDatasetConfig
    oneof_schema_11_validator: Optional[RudderStackDatasetConfig] = None
    actual_instance: Optional[Union[AWSPinpointDatasetConfig, AmplitudeDatasetConfig, BigQueryDatasetConfig, CustomDatasetConfig, MongoDBDatasetConfig, MySQLDatasetConfig, PostgresDatasetConfig, RedshiftDatasetConfig, RudderStackDatasetConfig, SegmentDatasetConfig, SnowflakeDatasetConfig]] = None
    one_of_schemas: Set[str] = { "AWSPinpointDatasetConfig", "AmplitudeDatasetConfig", "BigQueryDatasetConfig", "CustomDatasetConfig", "MongoDBDatasetConfig", "MySQLDatasetConfig", "PostgresDatasetConfig", "RedshiftDatasetConfig", "RudderStackDatasetConfig", "SegmentDatasetConfig", "SnowflakeDatasetConfig" }

    model_config = ConfigDict(
        validate_assignment=True,
        protected_namespaces=(),
    )


    def __init__(self, *args, **kwargs) -> None:
        if args:
            if len(args) > 1:
                raise ValueError("If a position argument is used, only 1 is allowed to set `actual_instance`")
            if kwargs:
                raise ValueError("If a position argument is used, keyword arguments cannot be used.")
            super().__init__(actual_instance=args[0])
        else:
            super().__init__(**kwargs)

    @field_validator('actual_instance')
    def actual_instance_must_validate_oneof(cls, v):
        instance = CreateDatasetArguments.model_construct()
        error_messages = []
        match = 0
        # validate data type: BigQueryDatasetConfig
        if not isinstance(v, BigQueryDatasetConfig):
            error_messages.append(f"Error! Input type `{type(v)}` is not `BigQueryDatasetConfig`")
        else:
            match += 1
        # validate data type: MongoDBDatasetConfig
        if not isinstance(v, MongoDBDatasetConfig):
            error_messages.append(f"Error! Input type `{type(v)}` is not `MongoDBDatasetConfig`")
        else:
            match += 1
        # validate data type: SnowflakeDatasetConfig
        if not isinstance(v, SnowflakeDatasetConfig):
            error_messages.append(f"Error! Input type `{type(v)}` is not `SnowflakeDatasetConfig`")
        else:
            match += 1
        # validate data type: PostgresDatasetConfig
        if not isinstance(v, PostgresDatasetConfig):
            error_messages.append(f"Error! Input type `{type(v)}` is not `PostgresDatasetConfig`")
        else:
            match += 1
        # validate data type: MySQLDatasetConfig
        if not isinstance(v, MySQLDatasetConfig):
            error_messages.append(f"Error! Input type `{type(v)}` is not `MySQLDatasetConfig`")
        else:
            match += 1
        # validate data type: RedshiftDatasetConfig
        if not isinstance(v, RedshiftDatasetConfig):
            error_messages.append(f"Error! Input type `{type(v)}` is not `RedshiftDatasetConfig`")
        else:
            match += 1
        # validate data type: AWSPinpointDatasetConfig
        if not isinstance(v, AWSPinpointDatasetConfig):
            error_messages.append(f"Error! Input type `{type(v)}` is not `AWSPinpointDatasetConfig`")
        else:
            match += 1
        # validate data type: CustomDatasetConfig
        if not isinstance(v, CustomDatasetConfig):
            error_messages.append(f"Error! Input type `{type(v)}` is not `CustomDatasetConfig`")
        else:
            match += 1
        # validate data type: AmplitudeDatasetConfig
        if not isinstance(v, AmplitudeDatasetConfig):
            error_messages.append(f"Error! Input type `{type(v)}` is not `AmplitudeDatasetConfig`")
        else:
            match += 1
        # validate data type: SegmentDatasetConfig
        if not isinstance(v, SegmentDatasetConfig):
            error_messages.append(f"Error! Input type `{type(v)}` is not `SegmentDatasetConfig`")
        else:
            match += 1
        # validate data type: RudderStackDatasetConfig
        if not isinstance(v, RudderStackDatasetConfig):
            error_messages.append(f"Error! Input type `{type(v)}` is not `RudderStackDatasetConfig`")
        else:
            match += 1
        if match > 1:
            # more than 1 match
            raise ValueError("Multiple matches found when setting `actual_instance` in CreateDatasetArguments with oneOf schemas: AWSPinpointDatasetConfig, AmplitudeDatasetConfig, BigQueryDatasetConfig, CustomDatasetConfig, MongoDBDatasetConfig, MySQLDatasetConfig, PostgresDatasetConfig, RedshiftDatasetConfig, RudderStackDatasetConfig, SegmentDatasetConfig, SnowflakeDatasetConfig. Details: " + ", ".join(error_messages))
        elif match == 0:
            # no match
            raise ValueError("No match found when setting `actual_instance` in CreateDatasetArguments with oneOf schemas: AWSPinpointDatasetConfig, AmplitudeDatasetConfig, BigQueryDatasetConfig, CustomDatasetConfig, MongoDBDatasetConfig, MySQLDatasetConfig, PostgresDatasetConfig, RedshiftDatasetConfig, RudderStackDatasetConfig, SegmentDatasetConfig, SnowflakeDatasetConfig. Details: " + ", ".join(error_messages))
        else:
            return v

    @classmethod
    def from_dict(cls, obj: Union[str, Dict[str, Any]]) -> Self:
        return cls.from_json(json.dumps(obj))

    @classmethod
    def from_json(cls, json_str: str) -> Self:
        """Returns the object represented by the json string"""
        instance = cls.model_construct()
        error_messages = []
        match = 0

        # deserialize data into BigQueryDatasetConfig
        try:
            instance.actual_instance = BigQueryDatasetConfig.from_json(json_str)
            match += 1
        except (ValidationError, ValueError) as e:
            error_messages.append(str(e))
        # deserialize data into MongoDBDatasetConfig
        try:
            instance.actual_instance = MongoDBDatasetConfig.from_json(json_str)
            match += 1
        except (ValidationError, ValueError) as e:
            error_messages.append(str(e))
        # deserialize data into SnowflakeDatasetConfig
        try:
            instance.actual_instance = SnowflakeDatasetConfig.from_json(json_str)
            match += 1
        except (ValidationError, ValueError) as e:
            error_messages.append(str(e))
        # deserialize data into PostgresDatasetConfig
        try:
            instance.actual_instance = PostgresDatasetConfig.from_json(json_str)
            match += 1
        except (ValidationError, ValueError) as e:
            error_messages.append(str(e))
        # deserialize data into MySQLDatasetConfig
        try:
            instance.actual_instance = MySQLDatasetConfig.from_json(json_str)
            match += 1
        except (ValidationError, ValueError) as e:
            error_messages.append(str(e))
        # deserialize data into RedshiftDatasetConfig
        try:
            instance.actual_instance = RedshiftDatasetConfig.from_json(json_str)
            match += 1
        except (ValidationError, ValueError) as e:
            error_messages.append(str(e))
        # deserialize data into AWSPinpointDatasetConfig
        try:
            instance.actual_instance = AWSPinpointDatasetConfig.from_json(json_str)
            match += 1
        except (ValidationError, ValueError) as e:
            error_messages.append(str(e))
        # deserialize data into CustomDatasetConfig
        try:
            instance.actual_instance = CustomDatasetConfig.from_json(json_str)
            match += 1
        except (ValidationError, ValueError) as e:
            error_messages.append(str(e))
        # deserialize data into AmplitudeDatasetConfig
        try:
            instance.actual_instance = AmplitudeDatasetConfig.from_json(json_str)
            match += 1
        except (ValidationError, ValueError) as e:
            error_messages.append(str(e))
        # deserialize data into SegmentDatasetConfig
        try:
            instance.actual_instance = SegmentDatasetConfig.from_json(json_str)
            match += 1
        except (ValidationError, ValueError) as e:
            error_messages.append(str(e))
        # deserialize data into RudderStackDatasetConfig
        try:
            instance.actual_instance = RudderStackDatasetConfig.from_json(json_str)
            match += 1
        except (ValidationError, ValueError) as e:
            error_messages.append(str(e))

        if match > 1:
            # more than 1 match
            raise ValueError("Multiple matches found when deserializing the JSON string into CreateDatasetArguments with oneOf schemas: AWSPinpointDatasetConfig, AmplitudeDatasetConfig, BigQueryDatasetConfig, CustomDatasetConfig, MongoDBDatasetConfig, MySQLDatasetConfig, PostgresDatasetConfig, RedshiftDatasetConfig, RudderStackDatasetConfig, SegmentDatasetConfig, SnowflakeDatasetConfig. Details: " + ", ".join(error_messages))
        elif match == 0:
            # no match
            raise ValueError("No match found when deserializing the JSON string into CreateDatasetArguments with oneOf schemas: AWSPinpointDatasetConfig, AmplitudeDatasetConfig, BigQueryDatasetConfig, CustomDatasetConfig, MongoDBDatasetConfig, MySQLDatasetConfig, PostgresDatasetConfig, RedshiftDatasetConfig, RudderStackDatasetConfig, SegmentDatasetConfig, SnowflakeDatasetConfig. Details: " + ", ".join(error_messages))
        else:
            return instance

    def to_json(self) -> str:
        """Returns the JSON representation of the actual instance"""
        if self.actual_instance is None:
            return "null"

        if hasattr(self.actual_instance, "to_json") and callable(self.actual_instance.to_json):
            return self.actual_instance.to_json()
        else:
            return json.dumps(self.actual_instance)

    def to_dict(self) -> Optional[Union[Dict[str, Any], AWSPinpointDatasetConfig, AmplitudeDatasetConfig, BigQueryDatasetConfig, CustomDatasetConfig, MongoDBDatasetConfig, MySQLDatasetConfig, PostgresDatasetConfig, RedshiftDatasetConfig, RudderStackDatasetConfig, SegmentDatasetConfig, SnowflakeDatasetConfig]]:
        """Returns the dict representation of the actual instance"""
        if self.actual_instance is None:
            return None

        if hasattr(self.actual_instance, "to_dict") and callable(self.actual_instance.to_dict):
            return self.actual_instance.to_dict()
        else:
            # primitive type
            return self.actual_instance

    def to_str(self) -> str:
        """Returns the string representation of the actual instance"""
        return pprint.pformat(self.model_dump())


