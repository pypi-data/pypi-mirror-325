# coding: utf-8

"""
    Shaped API

    Welcome to Shaped's API reference docs. These provide a detailed view of the endpoints and CLI commands that Shaped provides and brief explanations of how they should be used.   Shaped's API is composed of 3 components:   1. **Dataset** - used to provision and manage 'Shaped Datasets', which are persisted     data views of external data. Shaped Datasets can be created from any of our     'Shaped connectors' (e.g. S3, Segment, Snowflake, etc.) and support both batch     ingestion (up to a 15min delay) and stream ingestion (up to a 30 second     delay) depending on the specific connector used. Shaped datasets can also be     created from local files, which is particularly useful for getting started     with a snapshot of data.    2. **Model Management** - used to provision and manage 'Shaped Models', which     represent a system of data pipelines, training and serving infrastructure for     your ranking use-case.    3. **Model Inference** - a high performance API that's used to make     user-understanding requests or ranking inferences to your 'Shaped Models'. For     example, the 'rank' endpoint can be used to determine for a given user id     query, what is the content that is most engaging to that user.    The recommended workflow to interact with the Shaped API is as follows:   1. First create 'Shaped Datasets' to sync over data that your Shaped     understanding models will need. The models at the minimum need interaction     data to understand behavior of your users, so start with that and add your item     and user catalog data later.   2. Then create 'Shaped Models' that use your created 'Shaped Datasets' as     input. Your Shaped Model will will start streaming, processing and training     from your connected data immediately. After a few hours your model will have     tuned all parameters based on your data and will deploy an active model.   3. You can now use the 'Model Inference' endpoints to make real-time     inferences to your model based on your use-case. 

    The version of the OpenAPI document: 1.0.1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictFloat, StrictInt, StrictStr
from typing import Any, ClassVar, Dict, List, Optional, Union
from shaped.autogen.models.inference_config import InferenceConfig
from typing import Optional, Set
from typing_extensions import Self

class ModelConfig(BaseModel):
    """
    ModelConfig
    """ # noqa: E501
    name: StrictStr = Field(description="Sets the model's identifier")
    description: Optional[StrictStr] = Field(default=None, description="Describe your model or add notes future reference")
    slate_size: Optional[Union[StrictFloat, StrictInt]] = Field(default=50, description="Slate size defines the number of items expected to be ranked in a single request, having this information at train time can improve the model selection process. ")
    pagination_store_ttl: Optional[Union[StrictFloat, StrictInt]] = Field(default=3600, description="All served ids are added to the pagination store to be filtered out on subsequent requests. This parameter defines the time it takes for the inserted iids to be removed. ")
    inference_config: Optional[InferenceConfig] = None
    __properties: ClassVar[List[str]] = ["name", "description", "slate_size", "pagination_store_ttl", "inference_config"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of ModelConfig from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of inference_config
        if self.inference_config:
            _dict['inference_config'] = self.inference_config.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of ModelConfig from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "name": obj.get("name"),
            "description": obj.get("description"),
            "slate_size": obj.get("slate_size") if obj.get("slate_size") is not None else 50,
            "pagination_store_ttl": obj.get("pagination_store_ttl") if obj.get("pagination_store_ttl") is not None else 3600,
            "inference_config": InferenceConfig.from_dict(obj["inference_config"]) if obj.get("inference_config") is not None else None
        })
        return _obj


