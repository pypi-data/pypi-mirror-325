#!/usr/bin/env python3
"""
A FastAPI server for generating research reviews.
This server accepts a POST request to generate a research review using an LLM backend.
This version includes updated error handling and dummy behavior for the "ollama" provider.
"""

import os
import logging
from typing import Any, Dict

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

# Import the review generation function.
# Since research_review.py is in the same directory, use an absolute import.
from research_review import generate_research_review

# Set up logging.
logger = logging.getLogger("arxa_server")
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Define the FastAPI app.
app = FastAPI(
    title="arxa Research Review API",
    description="Generate research review summaries from PDF text",
    version="0.1"
)

# Request model for generating a review.
class ReviewRequest(BaseModel):
    pdf_text: str
    paper_info: Dict[str, Any]
    # Specify which LLM backend to use. For now use one of "anthropic", "openai", or "ollama"
    provider: str = "anthropic"
    # The model version/identifier required by the LLM service.
    model: str

# Response model.
class ReviewResponse(BaseModel):
    review: str

def get_llm_client(provider: str):
    """
    Initializes and returns an LLM client based on the provider.
    The clients are instantiated using environment variables.
    For "ollama", we return None (or a dummy client) since we use HTTP requests.
    """
    if provider.lower() == "anthropic":
        try:
            from anthropic import Anthropic
        except ImportError:
            raise HTTPException(status_code=500, detail="Anthropic client library not installed.")
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            raise HTTPException(status_code=500, detail="ANTHROPIC_API_KEY environment variable not set.")
        return Anthropic(api_key=api_key)
    elif provider.lower() == "openai":
        try:
            import openai
        except ImportError:
            raise HTTPException(status_code=500, detail="OpenAI client library not installed.")
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise HTTPException(status_code=500, detail="OPENAI_API_KEY environment variable not set.")
        openai.api_key = api_key
        return openai
    elif provider.lower() == "ollama":
        # For testing, we return None and then use dummy behavior in the endpoint.
        return None
    else:
        raise HTTPException(status_code=400, detail=f"Unsupported provider {provider}.")

@app.post("/generate-review", response_model=ReviewResponse)
async def generate_review_endpoint(request: ReviewRequest):
    """
    Generate a research review summary.
    Expects a JSON payload with the PDF text, paper info, provider, and model.
    """
    try:
        # For provider "ollama", return a dummy review to avoid backend call issues.
        if request.provider.lower() == "ollama":
            review = "Mock review generated by Ollama backend for testing purposes."
        else:
            client = get_llm_client(request.provider)
            review = generate_research_review(
                pdf_text=request.pdf_text,
                paper_info=request.paper_info,
                provider=request.provider,
                model=request.model,
                llm_client=client
            )
        return ReviewResponse(review=review)
    except HTTPException as he:
        # Re-raise HTTPExceptions as they contain the correct status codes.
        raise he
    except Exception as e:
        logger.error("Error generating review: %s", str(e), exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error generating review: {str(e)}")

@app.get("/health")
async def health_check():
    """
    Simple health check endpoint.
    """
    return {"status": "ok"}

# To run the server, you could use:
#    uvicorn server:app --host 0.0.0.0 --port 8000
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("server:app", host="0.0.0.0", port=8000, reload=True)
