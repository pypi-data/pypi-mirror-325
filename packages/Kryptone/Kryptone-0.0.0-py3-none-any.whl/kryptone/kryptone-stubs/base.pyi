import dataclasses
import datetime
import pathlib
from collections.abc import Generator
from functools import cached_property
from typing import (Any, DefaultDict, List, Literal, OrderedDict, Set, Union,
                    override)
from uuid import UUID

from selenium.webdriver import Chrome, Edge

from kryptone.routing import Router
from kryptone.storages import (AirtableStorage, ApiStorage, BaseStorage,
                               FileStorage, RedisStorage)
from kryptone.utils.urls import (URL, LoadStartUrls, URLIgnoreRegexTest,
                                 URLIgnoreTest, URLPaginationGenerator,
                                 URLQueryGenerator)

DEFAULT_META_OPTIONS: set[str]


def get_selenium_browser_instance(
    browser_name: str = ...,
    headless: bool = ...,
    load_images: bool = ...,
    load_js: bool = ...
) -> Union[Edge, Chrome]: ...


class CrawlerOptions:
    spider: SiteCrawler = ...
    spider_name: str = ...
    verbose_name: str = ...
    initial_spider_meta: type = ...
    domains: list[str] = ...
    url_ignore_tests: Union[URLIgnoreRegexTest, URLIgnoreTest] = ...
    debug_mode: bool = ...
    default_scroll_step: Literal[80] = ...
    router: Router = ...
    crawl: bool = ...
    start_urls: Union[
        LoadStartUrls,
        URLPaginationGenerator,
        URLQueryGenerator,
        list[str]
    ]
    restrict_search_to: list[str] = ...
    ignore_queries: bool = ...
    ignore_images: bool = ...
    url_gather_ignore_tests: list[str] = ...
    url_rule_tests: list[str] = ...

    def __repr__(self) -> str: ...

    @property
    def has_start_urls(self) -> bool: ...

    def add_meta_options(self, options: tuple) -> None: ...
    def prepare(self) -> None: ...


@dataclasses.dataclass
class Performance:
    iteration_count: int = 0
    start_date: datetime.datetime = dataclasses.field(
        default_factor=datetime.datetime.now
    )
    end_date: datetime.datetime = dataclasses.field(
        default_factor=datetime.datetime.now
    )
    error_count: int = 0
    timezone: str = 'UTC'
    error_count: int = 0
    duration: int = 0
    count_urls_to_visit: int = 0
    count_visited_urls: int = 0

    def __post_init__(self) -> None: ...
    def calculate_duration(self) -> None: ...
    def add_error_count(self) -> None: ...
    def add_iteration_count(self) -> None: ...
    def load_statistics(self, data) -> None: ...

    def json(self) -> OrderedDict[
        str,
        Union[int, float, datetime.datetime]
    ]: ...


class Crawler(type):
    def __new__(cls: type, name: str, bases: tuple, attrs: dict) -> type: ...
    def prepare(cls: type) -> None: ...


class BaseCrawler(metaclass=Crawler):
    DATA_CONTAINER: list[str] = []
    model: dataclasses.dataclass = ...
    urls_to_visit: Set[URL] = ...
    visited_urls: Set[URL] = ...
    visited_pages_count: int = ...
    list_of_seen_urls: Set[URL] = ...
    browser_name: str = ...
    timezone: str = ...
    default_scroll_step:  Literal[80] = ...
    driver: Union[Edge, Chrome] = ...
    url_distribution: DefaultDict[list] = ...
    start_url: URL = ...
    storage: Union[FileStorage, ApiStorage, AirtableStorage, RedisStorage]
    spider_uuid: UUID = ...
    additional_storages: list = ...

    class Meta:
        ...

    def __init__(self, browser_name: str = ...): ...
    def __repr__(self) -> str: ...
    def __hash__(self) -> int: ...

    @property
    def get_page_title(self) -> str: ...

    @property
    def get_current_date(self) -> datetime.datetime: ...

    @property
    def get_origin(self) -> str: ...

    @cached_property
    def calculate_completion_percentage(self) -> Union[int, float]: ...

    def download_images(
        self,
        urls: list[str],
        page_url: URL,
        directory: Union[str, pathlib.Path] = ...,
        exclude_paths: list[str] = ...,
        filename_attrs: dict[str, str] = ...
    ) -> None: ...

    def collect_page_urls(self) -> List[str]: ...

    def save_object(
        self,
        data: List[dict[str, Any]],
        check_fields_null: List[str] = ...
    ) -> None: ...

    def backup_urls(self) -> None: ...
    def urljoin(self, path: str) -> URL: ...

    def run_url_filters(
        self,
        valid_urls: list[URL]
    ) -> set[URL]: ...

    def check_urls(
        self,
        valid_urls: list[Union[URL, str]],
        refresh: bool = ...
    ) -> set[str]: ...

    def add_urls(self, urls: Union[URL, str]) -> None: ...
    def calculate_performance(self) -> None: ...

    def current_page_actions(
        self,
        current_url: URL,
        **kwargs
    ) -> None: ...

    def post_navigation_actions(self, current_url: URL, **kwargs) -> None: ...

    def before_next_page_actions(
        self,
        current_url: URL,
        next_url: URL,
        **kwargs
    ) -> None: ...

    def after_fail(self) -> None: ...

    def after_data_save(self, data) -> None: ...

    def before_start(
        self,
        start_urls: list[str],
        *args,
        **kwargs
    ) -> None: ...


class OnPageActionsMixin:
    def click_consent_button(
        self,
        element_id: str = ...,
        element_class: str = ...,
        before_click_wait_time: int = ...,
        wait_time: int = ...
    ) -> None: ...


class SiteCrawler(OnPageActionsMixin, BaseCrawler):
    start_date: datetime.datetime = ...
    end_date: datetime.datetime = ...
    _meta: CrawlerOptions = ...
    performance_audit: Performance = ...

    @override
    def __init__(self, browser_name: str = ...) -> None: ...
    def __del__(self) -> None: ...

    @staticmethod
    def transform_string_urls(urls) -> Generator[URL, None, None]: ...
    def load_storage(self, python_path: str) -> BaseStorage: ...
    def setup_class(self) -> None: ...

    def start(
        self,
        start_urls: List[str] = ...,
        **kwargs
    ) -> None: ...

    def resume(self, windows: int = ..., **kwargs) -> None: ...
    def start_from_sitemap_xml(self, url, windows=1, **kwargs) -> None: ...
    def start_from_json(self, url: str, **kwargs) -> None: ...

    def boost_start(
        self,
        start_urls: list[str],
        *,
        windows: int = ...,
        **kwargs
    ) -> None: ...
