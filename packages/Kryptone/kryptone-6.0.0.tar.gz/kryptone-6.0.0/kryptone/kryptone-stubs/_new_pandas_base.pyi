import dataclasses
import datetime
import time
from collections.abc import Generator
from functools import cached_property
from typing import (Any, Coroutine, DefaultDict, Iterator, List, Literal,
                    OrderedDict, Union, override)
from urllib.parse import ParseResult
from urllib.robotparser import RobotFileParser

import pandas
from selenium.webdriver import Chrome, Edge
from selenium.webdriver.remote.webelement import WebElement

from kryptone.routing import Router
from kryptone.utils.iterators import AsyncIterator
from kryptone.utils.urls import (URL, LoadStartUrls, URLIgnoreRegexTest,
                                 URLIgnoreTest, URLPaginationGenerator,
                                 URLQueryGenerator)

DEFAULT_META_OPTIONS: set[str]


def get_selenium_browser_instance(
    browser_name: str = ...,
    headless: bool = ...,
    load_images: bool = ...,
    load_js: bool = ...
) -> Union[Edge, Chrome]: ...


@dataclasses.dataclass
class Performance:
    iteration_count: int = 0
    start_date: datetime.datetime = dataclasses.field(
        default_factor=datetime.datetime.now
    )
    end_date: datetime.datetime = dataclasses.field(
        default_factor=datetime.datetime.now
    )
    error_count: int = 0
    duration: float = 0

    def calculate_duration(self) -> None: ...
    def add_error_count(self) -> None: ...
    def add_iteration_count(self) -> None: ...

    def json(self) -> OrderedDict[
        str,
        Union[int, float, datetime.datetime]
    ]: ...


class CrawlerOptions:
    spider: SiteCrawler = ...
    spider_name: str = ...
    verbose_name: str = ...
    initial_spider_meta: type = ...
    domains: list[str] = ...
    url_ignore_tests: Union[URLIgnoreRegexTest, URLIgnoreTest] = ...
    debug_mode: bool = ...
    default_scroll_step: Literal[80] = ...
    router: Router = ...
    crawl: bool = ...
    start_urls: Union[
        LoadStartUrls,
        URLPaginationGenerator,
        URLQueryGenerator,
        list[str]
    ]
    restrict_search_to: list[str] = ...
    ignore_queries: bool = ...
    ignore_images: bool = ...
    url_gather_ignore_tests: list[str] = ...
    url_rule_tests: list[str] = ...

    def __repr__(self) -> str: ...

    @property
    def has_start_urls(self) -> bool: ...

    def add_meta_options(self, options: tuple) -> None: ...
    def prepare(self) -> None: ...


class Crawler(type):
    def __new__(cls: type, name: str, bases: tuple, attrs: dict) -> type: ...
    def prepare(cls: type) -> None: ...


class BaseCrawler(metaclass=Crawler):
    DATA_CONTAINER: list[str] = ...
    model = ...
    browser_name: str = ...
    after_seconds: int = ...
    driver: Union[Edge, Chrome] = ...
    debug: bool = ...
    start_url: Union[str, URL] = ...
    seen_urls: pandas.DataFrame = ...
    urls_to_visit: pandas.DataFrame = ...
    urls_to_visit_list: List[URL] = ...
    saved_data: pandas.DataFrame = ...
    spider_uuid: str = ...
    timezone: str = ...

    class Meta:
        ...

    def __init__(
        self,
        browser_name: str = ...,
        debug: bool = ...,
        after_seconds: int = ...
    ): ...

    def __repr__(self) -> str: ...
    def __hash__(self) -> int: ...

    @property
    def collect_page_urls(self) -> List[str]: ...

    @property
    def visited_urls(self) -> List[URL]: ...

    @property
    def count_of_visited_urls(self) -> int: ...

    @property
    def count_of_urls_to_visit(self) -> int: ...

    @property
    def get_page_title(self) -> str: ...

    @property
    def get_current_date(self) -> datetime.datetime: ...

    @property
    def get_origin(self) -> str: ...

    @cached_property
    def calculate_completion_percentage(self) -> float: ...

    def urljoin(self, path: str) -> URL: ...

    def create_dataframe_from_urls(
        self,
        urls: List[URL]
    ) -> pandas.DataFrame: ...

    def load_file(self) -> None: ...

    def check_urls(
        self,
        list_or_dataframe: Union[
            list[Union[str, URL]],
            pandas.DataFrame
        ]
    ) -> set[URL]: ...

    def merge_urls(
        self,
        dataframe: pandas.DataFrame
    ) -> None: ...

    def add_urls(
        self,
        list_or_dataframe: Union[
            List[Union[URL, str]],
            pandas.DataFrame]
    ) -> None: ...

    def calculate_performance(self) -> None: ...

    def current_page_actions(
        self,
        current_url: URL,
        current_json_object: pandas.DataFrame = ...,
        **kwargs
    ) -> None: ...

    def post_navigation_actions(self, current_url: URL, **kwargs) -> None: ...

    def before_next_page_actions(
        self,
        current_url: URL,
        next_url: URL,
        **kwargs
    ) -> None: ...

    def after_fail(self, current_url: URL, message: str) -> None: ...

    def after_data_save(self, dataframe: pandas.DataFrame) -> None: ...

    def before_start(
        self,
        start_urls: List[Union[str, URL]],
        *args,
        **kwargs
    ) -> None: ...


class OnPageActionsMixin:
    def click_consent_button(
        self,
        element_id: str = ...,
        element_class: str = ...,
        before_click_wait_time: int = ...,
        wait_time: int = ...
    ) -> None: ...


class SiteCrawler(OnPageActionsMixin, BaseCrawler):
    start_date: datetime.datetime = ...
    end_date: datetime.datetime = ...
    _meta: CrawlerOptions = ...
    start_url: List[Union[str, URL]] = ...
    performance_audit: Performance = ...

    @override
    def __init__(self, browser_name: str = ...) -> None: ...
    def __del__(self) -> None: ...

    @staticmethod
    def transform_string_urls(
        urls: List[str]
    ) -> Generator[URL, None, None]: ...

    @staticmethod
    def reverse_transform_url_objects(
        urls: List[URL]
    ) -> Generator[str, None, None]: ...

    @override
    def before_start(self, start_urls: list[str], *args, **kwargs) -> None: ...

    def start(
        self,
        start_urls: List[str] = ...,
        **kwargs
    ) -> None: ...

    def resume(self, windows: int = ..., **kwargs) -> None: ...
    def start_from_sitemap_xml(self, url, windows=1, **kwargs) -> None: ...
    def start_from_json(self, url: str, **kwargs) -> None: ...

    def boost_start(
        self,
        start_urls: list[str],
        *,
        windows: int = ...,
        **kwargs
    ) -> None: ...
