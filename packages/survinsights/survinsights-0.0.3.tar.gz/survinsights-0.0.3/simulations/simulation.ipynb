{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30967797",
   "metadata": {},
   "source": [
    "# Model-Agnostic Methods in Survival Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb809df",
   "metadata": {},
   "source": [
    "Model-agnostic methods in survival analysis are techniques used to interpret survival models without requiring access to their internal structure or parameters. Most model-agnostic methods rely on evaluating the survival prediction function $f(t | X)$, which represents the model's predicted outcome at time, given a feature set $X = \\begin{bmatrix}\n",
    "   x_1^1 & \\ldots & x_L^1 \\\\\n",
    "   \\vdots & \\ddots & \\vdots \\\\\n",
    "   x_1^n & \\ldots & x_L^n \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{n \\times L}.$ This could be:\n",
    "\n",
    "- Hazard function   $\\lambda(t | X)$ \n",
    "\n",
    "- Survival function  $S(t | X)$\n",
    "\n",
    "- Cummulative hazard function $\\Lambda(t | X)$\n",
    "\n",
    "Model-agnostic methods manipulate $f(t | X)$ to extract interpretable insights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eed1f6a",
   "metadata": {},
   "source": [
    "## Local explaination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29511427",
   "metadata": {},
   "source": [
    "### Individual Conditional Expectation (ICE)\n",
    "\n",
    "Individual conditional expectation (ICE) curves provide a visual representation of how changes in a specific feature affect a model's prediction for an individual data point. These curves help describe the dependency of the model’s prediction (or an approximation of the conditional expected value) on the values of a selected feature.\n",
    "\n",
    "The ICE function evaluates the model's prediction across a predefined grid of values $x_\\ell^{\\text{grid}} = \\{x_\\ell^1, \\ldots, x_\\ell^G\\}$ for the selected feature $\\ell$. It keeps all other features $x_{-\\ell}^i$ fixed at their respective values for the given individual $x^i$. Mathematically, the ICE function evaluated at a specific time point $t$ is defined as:\n",
    "\n",
    "\n",
    "$$\\text{ICE}(f, t, x^i, \\ell) = f(t | x_\\ell^{\\text{grid}}, x_{-\\ell}^i).$$\n",
    "\n",
    "***$\\underline{\\bf Limitation}$***\n",
    "- Single-Feature Focus: ICE curves are limited to visualizing one feature at a time. Including multiple features would require overlaying complex surfaces, making the plot unreadable.\n",
    "\n",
    "- Correlation Issues: When the feature of interest is correlated with others, some plotted points may represent unrealistic or invalid combinations, misrepresenting the data's true joint distribution.\n",
    "\n",
    "- Overcrowding: Drawing too many ICE curves can clutter the plot, reducing its clarity and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a359ca92",
   "metadata": {},
   "source": [
    "### SurvLIME (Local interpretable model-agnostic explanations)\n",
    "\n",
    "SurvLIME aims to explain the prediction of the black-box survival model by its local approximation using a simple, interpretable surrogate model (Cox model), which can be defined by\n",
    "$$\\Lambda^{\\text{Cox-PH}}(t) = \\Lambda_0^{\\text{Cox-PH}}(t) \\exp(\\omega^\\top x)$$\n",
    "where $\\Lambda_0^{\\text{Cox-PH}}(t)$ can be estimated bt Nelson-Aalen estimator\n",
    "\n",
    "In order to explain the prediction for an individual $x^i$:\n",
    "\n",
    "1. Generate a neiborghood set of $p$ points $\\{x^i_1, \\ldots , x^i_p\\}$ around $x^i$\n",
    "\n",
    "2. Optimize the coefficients $\\omega$ of the surrogate model so that the average distance between the CHF of explained model and the surrogate model is minimized over all $x^i_u \\in \\{x^i_1, \\ldots , x^i_p\\}$\n",
    "\n",
    "$$\\min_{\\omega} \\sum_{u=1}^p w_u \\sum_{v=1}^q r_{uv}^2 \\big[ \\text{ln} \\Lambda(t_v | x^i_u) - \\text{ln} \\Lambda_0^{\\text{Cox-PH}}(t_v | x^i_u) - \\omega^\\top x^i_u\\big]^2 (t_{v+1} - t_v)$$ \n",
    "\n",
    "where \n",
    "\n",
    "$w_u = K(x^i_u, x^i)$ is the distance weight between $x^i_u$ and $x^i$ and can be computed by some kernel methods $K$\n",
    "\n",
    "$r_{uv} = \\frac{\\Lambda(t_v | x^i_u)}{ln \\Lambda(t_v | x^i_u)}$ is “straighten” weight to reduce the possible huge difference caused by the logarithm distance.\n",
    "\n",
    "\n",
    "\n",
    "***$\\underline{\\bf Limitation}$***\n",
    "- Defining a proper neighborhood: The method struggles to correctly define a \"neighborhood\" of data points, especially when features are highly correlated. This can result in unrealistic samples that don't reflect real-world data.\n",
    "\n",
    "- Unstable results: The explanations can vary with each run because of random sampling. This makes the results less reliable and harder to reproduce.\n",
    "\n",
    "- Potential for Misleading Explanations: LIME explanations can be manipulated to hide biases, posing challenges for explanation receivers who cannot verify their truthfulness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5b88ad",
   "metadata": {},
   "source": [
    "### SurvSHAP (SHapley Additive exPlanations)\n",
    "The goal of SHAP is to explain the prediction of an instance $x$ by computing the contribution of each feature $\\phi_\\ell \\in \\{\\phi_1, \\ldots, \\phi_L\\}$ to the prediction $f(x)$. The SHAP explanation method computes Shapley values from coalitional game theory. The Shapley value for a feature $\\ell \\in \\{1, \\ldots, L\\}$ of an instance $x$ is given by:\n",
    "$$\\phi_\\ell(f_x) = \\sum_{S \\subseteq N \\setminus \\{\\ell\\}}\\frac{\\vert S \\vert ! (\\vert N \\vert - \\vert S \\vert - 1)}{\\vert N \\vert}\\big[f_x(S \\cup \\{\\ell\\}) - f_x(S)\\big]$$\n",
    "where $N$ is the set of all features, $S$ is a subset of features excluding $i$ (a coalition), $f(S)$ is the model's prediction when only the features in $S$ are used, $f(S \\cup \\{\\ell\\})$ is the model's prediction when feature $\\ell$ is added to $S$, and\n",
    "$$f_x(S) = \\int f(x_1, \\ldots, x_L)dP_{x \\notin S} - E_X(f(X))$$\n",
    "is the prediction for feature values in set $S$ that are marginalized over features that are not included in set $S$. Then, the feature contributions must add up to the difference of prediction for $x$ and the average.\n",
    "\n",
    "$$\\sum_{\\ell=1}^L \\phi_\\ell = f(x) - E_X(f(X))$$\n",
    "\n",
    "SHAP assumes that the model's prediction can be decomposed into a sum of contributions from individual features. The Shapley value explanation is represented as an additive feature attribution method, a linear model. That view connects LIME and Shapley values. SHAP specifies the explanation as:\n",
    "\n",
    "$$f(x) = \\phi_0 + \\sum_{\\ell = 1}^L \\phi_\\ell z_{\\ell},$$\n",
    "\n",
    "where $z = \\{1, \\ldots, 1\\} \\in \\mathbb{R}^L$ and $\\phi_0 = E_X(f(X))$. Calculating Shapley values $\\phi$ directly is computationally expensive. To make SHAP values feasible, approximations and optimizations for $\\phi$ are used depending on the model type.\n",
    "\n",
    "- TreeSHAP (Optimized for Tree-Based Models)\n",
    "- KernelSHAP (Model-Agnostic Method)\n",
    "\n",
    "\n",
    "KernelSHAP is a kernel-based estimation approach for Shapley values inspired by local surrogate models (like LIME, and in this case, by a linear model $\\Phi^\\top z$). KernelSHAP, which is applied to explain the prediction of individual $x_i$, consists of six steps:\n",
    "1. Sample coalitions $z_k = (z_{k1}, \\ldots, z_{kL}) \\in \\{0, 1\\}^L$, $k=\\{1, \\ldots, K\\}$ (1 = feature present in coalition, 0 = feature absent).\n",
    "2. Get prediction for each $z_k$ by first converting $z_k$ to the original feature space $h(x_i, z_k)$ by keep the value of features in $x_i$ whose $z_k=1$ and replace the value of features in $x_i$ whose $z_k=0$ by a randomly sampled value from the distribution of corresponding feature.\n",
    "3. Get prediction $f(h(x_i, z_k))$.\n",
    "4. Compute the weight for each $z_k$ with the SHAP kernel\n",
    "    $$\\omega_k = \\frac{p-1}{{p \\choose s} s(p-s)}.$$\n",
    "5. Fit weighted linear model\n",
    "      $$\\min_{\\Phi} \\sum_{k=1}^K \\omega_k \\big(f(h(x_i, z_k)) - \\Phi^\\top z_k \\big)^2$$\n",
    "      \n",
    "      where \n",
    "    \n",
    "    $$\\Phi = (\\phi_1, \\ldots, \\phi_L).$$\n",
    "6. Return Shapley values $\\Phi$  the coefficients from the linear model.\n",
    "\n",
    "    $$\\Phi = (Z^\\top \\Omega Z)^{-1} Z^\\top \\Omega Y$$\n",
    "\n",
    "    where,\n",
    "\n",
    "$$Z = [z_1, \\ldots, z_K] \\in \\mathbb{R}^{K \\times L}$$\n",
    "$$\\Omega = \\text{diag}(\\omega_1, \\ldots, \\omega_K) \\in \\mathbb{R}^{K \\times K}$$\n",
    "$$Y = \\big[f(h(x_i, z_1)), \\ldots, f(h(x_i, z_K))\\big] \\in \\mathbb{R}^{K \\times \\tau}$$\n",
    "\n",
    "***$\\underline{\\bf Limitation}$***\n",
    "- Slow computation (on KernelSHAP): Impractical for calculating Shapley values for many instances.\n",
    "- Ignores feature dependence (on KernelSHAP, like many other permutation-based interpretation methods): Assumes feature independence, which can lead to unrealistic data points when features are correlated.\n",
    "- Misinterpretation Risk (like Shapley value): Shapley values represent the contribution of a feature to the difference between the prediction and the mean prediction, not the difference of the predicted value after removing the feature from the model training. Misinterpretation is common.\n",
    "- Dependency on training data (like Shapley value): Access to data is needed for new individual data point. It is not sufficient to access the prediction function because you need the data to replace parts of the instance of interest with values from randomly drawn instances of the data. This can only be avoided if you can create data instances that look like real data instances but are not actual instances from the training data.\n",
    "- Potential for Misleading Explanations: SHAP explanations can be manipulated to hide biases, posing challenges for explanation receivers who cannot verify their truthfulness.\n",
    "\n",
    "<!-- Like many other permutation-based interpretation methods, the Shapley value method suffers from inclusion of unrealistic data instances when features are correlated. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a626e36",
   "metadata": {},
   "source": [
    "## Global explaination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57132097",
   "metadata": {},
   "source": [
    "### Partial Dependence Plot (PDP)\n",
    "\n",
    "The partial dependence function (PDP) describes how the expected value of the model prediction varies with respect to a chosen explanatory variable.\n",
    "This is achieved by evaluating the model's behavior across a predefined grid points $x_\\ell^{\\text{grid}} = \\{x_\\ell^1, \\ldots, x_\\ell^G\\}$ for a selected feature $\\ell$, while all other variables fluctuate following their respective marginal distributions.\n",
    "\n",
    "In practice, a one-dimensional PDP is estimated as the average of the Individual Conditional Expectation (ICE) profiles across all $n$ observations \n",
    "in the dataset $X = \\{x^1, \\ldots, x^n\\}$. Mathematically, this is expressed as:\n",
    "\n",
    "$$\\text{PDP}(f, t, \\ell) = \\frac{1}{n}\\sum_{i=1}^n f(t | x_\\ell^{\\text{grid}}, x_{-\\ell}^i)$$\n",
    "\n",
    "<!-- $$\\text{PDP}(f, t, \\ell) = \\frac{1}{n}\\sum_{i=1}^n f(t | x_\\ell^{\\text{grid}}, x_{\\{1, \\ldots, L\\} \\setminus \\{\\ell\\}}^i)$$ -->\n",
    "\n",
    "<!-- \\begin{align*}\n",
    "    S_{\\text{DPD}, A}(t | X_{-A}) & = E_{X_{-A}}[S(t | X_A, X_{-A})]\\\\\n",
    "    & = \\int_{-\\infty}^{+\\infty} S(t | X_A, X_{-A}) dP(X_{-A})\n",
    "\\end{align*} -->\n",
    "\n",
    "***$\\underline{\\bf Limitation}$***\n",
    "- Dimensionality Limitation: PDP can realistically handle up to two features due to the constraints of 2D visualization and human cognitive limitations.\n",
    "\n",
    "- Feature Distribution Omission: PDP may not show the feature distribution, which can mislead interpretations in regions with little or no data. This can be mitigated by including rugs or histograms.\n",
    "\n",
    "- Assumption of Independence: PDP assumes features are independent, leading to unrealistic combinations in correlated features. Accumulated Local Effect (ALE) plots address this by considering conditional distributions.\n",
    "\n",
    "- Hidden Heterogeneous Effects: PDP shows average effects, which can mask opposing effects in subgroups of the data. Individual Conditional Expectation (ICE) curves reveal such heterogeneous patterns by displaying instance-level effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e98020",
   "metadata": {},
   "source": [
    "### Accumulated Local Effects (ALE)\n",
    "\n",
    "In order to avoid the problem of correlated features as in PDP, ALE proposes to calculate the difference (instead of average) in prediction based on the conditional distribution pg features\n",
    "\n",
    "**Marginal plot (M plot)**\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{M-plot}(t, f, \\ell) &=  E_{X_{-\\ell}}(f(t | X) | X_\\ell = x_\\ell) \\\\\n",
    "                        &=  \\int_{-\\inf}^{\\inf} f(t | X) dPX_{-\\ell} | X_\\ell = x_\\ell\n",
    "\\end{align}\n",
    "\n",
    "- Estimation\n",
    "    $$\\text{M-plot}(t, f, \\ell) =  \\frac{1}{\\#N(X_\\ell)} \\sum_{i \\in N(x_\\ell)} f(t | X_{-\\ell}^i),$$\n",
    "where $N(x_\\ell)$ is a set of individuals for which the value of its feature $\\ell$ all into the small neiborghood of $x_\\ell$.\n",
    "This method can handle the problem of unrealistic sample but still uncover the pure effect of interest feature if they have correlation with others features.\n",
    "\n",
    "ALE handle this problem of M-plot by averaging the changes of the predictionns $\\frac{d f(t | X_\\ell, X_{-\\ell})}{d X_\\ell}$, not the prediction itself. The change is defined as the partial derivative (local effect). Then ALE average the local effect over the conditional distribution similar to M-plot to avoid the extrapolation (unrealistic samples). Lastly, it accumuate  (integrate) the averaged local effect\n",
    "\n",
    "$$\\text{ALE}(f, t, \\ell) = \\int_{q^0_\\ell}^{x^{\\star}} E_{X_{-\\ell} | X_\\ell = x_\\ell} \\big(\\frac{\\partial f(t | X)}{\\partial X_\\ell} | X_\\ell = q_\\ell \\big) dq_\\ell,$$\n",
    "where the value of feature $\\ell$ is splited into Kinterval $q^k_\\ell \\in \\{q^0_\\ell, \\ldots, q^K_\\ell\\}$ in which $q^0_\\ell = \\min(X_\\ell)$ and $q^K_\\ell = \\max(X_\\ell)$\n",
    "\n",
    "- Estimation\n",
    "$$\\text{ALE}(f, t, \\ell) =  \\sum_{k=1}^{k_\\ell(x^\\star)}\\frac{1}{\\#N(q^k_\\ell)} \\sum_{i \\in N(q^k_\\ell)} f(t | q_\\ell^k, X_{-\\ell}^i) - f(t | q_\\ell^{k-1}, X_{-\\ell}^i),$$\n",
    " \n",
    " where $k_\\ell(x^\\star)$ is the interval of feature $\\ell$ hold $x^\\star$\n",
    "\n",
    "***$\\underline{\\bf Limitation}$***\n",
    "\n",
    "- Interpretation and Correlation: ALE plots show interval-wise effects that are accumulated into a smooth curve, but interpretation across intervals is invalid when features are highly correlated. Each interval uses different data points, making the effects strictly local.\n",
    "\n",
    "- Deviation from Linear Models: In models with feature interactions and correlations, ALE plots may deviate from linear regression coefficients. First-order ALE effects can appear curved due to interaction terms, highlighting differences in how ALE and linear models attribute effects.\n",
    "Effect of Intervals:\n",
    "\n",
    "- The number of intervals impacts the stability of ALE plots: Too many intervals: Plots become shaky and overly detailed.\n",
    "Too few intervals: Plots smooth out complexity but lose accuracy.\n",
    "\n",
    "- Lack of ICE Curves: Unlike PDPs, ALE plots do not include ICE curves to reveal heterogeneous feature effects. While interval-level differences can be checked in ALE, they do not capture individual-level variability as ICE curves do,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29debefb",
   "metadata": {},
   "source": [
    "### Permutation Feature Importance (PFI)\n",
    "\n",
    "PFI measures how much the model's prediction accuracy decreases when the values of a specific feature are randomly shuffled, breaking the relationship between the feature and the true outcome. A feature is “important” if shuffling its values increases the model error, because in this case the model relied on the feature for the prediction. A feature is “unimportant” if shuffling its values leaves the model error unchanged, because in this case the model ignored the feature for the prediction.\n",
    "\n",
    "To compute the PFI for a specific feature $\\ell$, follow these steps:\n",
    "\n",
    "- Estimate the model loss $L(t, f(t | X))$. \n",
    "\n",
    "- Create a perturbed dataset $\\tilde{X}$ by randomly shuffling the values of $X_\\ell$ (i.e: $\\tilde{X}_\\ell = \\text{shuffle}\\{X_\\ell\\}$) while keeping other features in $X$ unchanged\n",
    "\n",
    "- Compute the loss $L(t, f(t | \\tilde{X}))$\n",
    "\n",
    "- Calculate the PFI as the differnce between $L(t, f(t | \\tilde{X}))$ and $L(t, f(t | X))$, which can be $L(t, f(t | X)) - L(t, f(t | \\tilde{X})),$ or $\\frac{L(t, f(t | X))}{L(t, f(t | \\tilde{X}))}$\n",
    "\n",
    "***$\\underline{\\bf Limitation}$***\n",
    "\n",
    "- Unstable result: The permutation feature importance depends on shuffling the feature, which adds randomness to the measurement. When the permutation is repeated, the results might vary greatly.\n",
    "\n",
    "- Unrealistic sample: If features are correlated, the permutation feature importance can be biased by unrealistic data instances. The problem is the same as with PDP\n",
    "\n",
    "- Requirement for True Outcomes: PFI requires access to the true outcome to compute the loss. Without them, it cannot be computed, limiting its applicability in some scenarios.\n",
    "\n",
    "- Performance vs. Variance: Permutation Feature Importance (PFI) is tied to model error, which may not always align with your needs. If you are interested in how much a feature influences the model's output variance (e.g., robustness to feature manipulation), PFI may not be the appropriate measure. Model variance (explained by the features) and feature importance correlate strongly when the model generalizes well (i.e. it does not overfit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96736686",
   "metadata": {},
   "source": [
    "### Feature Interation\n",
    "\n",
    "Friedman’s H-statistic measures the degree of interaction between two features relative to their individual contributions. For two specific features $\\ell$ and $k$, the H-statistic is defined as\n",
    "\n",
    "$$H_{\\ell k}^2(t) = \\frac{\\sum_{i=1}^n [\\text{PDP}(f, t , X^i_\\ell, X^i_k) - \\text{PDP}(f, t , X^i_\\ell) - \\text{PDP}(f, t , X^i_k)]^2}{\\sum_{i=1}^n [\\text{PDP}(f, t , X^i_\\ell, X^i_k)]^2}.$$\n",
    "\n",
    "***$\\underline{\\bf Limitation}$***\n",
    "- Expensive computation\n",
    "\n",
    "- Unstable result: Estimates may vary due to sampling variability if we do not use all data points to compute PDP, requiring multiple runs to ensure stability.\n",
    "\n",
    "- Interpretation challenges: The H-statistic can exceed 1, complicating interpretation or there’s no clear threshold to determine when an interaction is \"strong\". When the total effect of two features is weak, but mostly consists of interactions, than the H-statistic will be very large. These spurious interactions require a small denominator of the H-statistic and are made worse when features are correlated ==> Visualizing the unnormalized H-statistic (square root of the numerator) can mitigate overemphasis on spurious interactions.\n",
    "\n",
    "- Interaction strength vs. visualization: The H-statistic measures interaction strength but does not explain how the interaction works. Complementing it with 2D partial dependence plots is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558aaa3a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
