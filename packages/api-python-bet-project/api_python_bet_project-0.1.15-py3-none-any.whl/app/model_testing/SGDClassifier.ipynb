{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPipeline:\n",
    "    def __init__(self, csv_path, skip_rows, random_state):\n",
    "        # Initialize the class with the file path, rows to skip, and random state for reproducibility\n",
    "        self.csv_path = csv_path\n",
    "        self.skip_rows = skip_rows\n",
    "        self.df = self._load_data()  # Load the dataset\n",
    "        self.X = self.df[[\n",
    "            \"home_team_name\",\n",
    "            \"away_team_name\",\n",
    "            \"home_team_rank\",\n",
    "            \"away_team_rank\",\n",
    "            \"day_of_week\",\n",
    "            \"hour_of_day\",\n",
    "            \"home_team_points\",\n",
    "            \"away_team_points\",\n",
    "            \"home_team_goals_for\",\n",
    "            \"away_team_goals_for\",\n",
    "            \"home_team_goals_againsts\",\n",
    "            \"away_team_goals_against\",\n",
    "            \"home_team_goals_difference\",\n",
    "            \"away_team_goals_difference\"\n",
    "        ]]\n",
    "        self.y = self.df[\"result\"]  # Target variable: match result\n",
    "        self.random_state = random_state\n",
    "        self.pipeline = self._create_pipeline()  # Create the machine learning pipeline\n",
    "        self.param_grid = self._create_param_grid()  # Define the hyperparameter grid for tuning\n",
    "        self.model = GridSearchCV(\n",
    "            estimator=self.pipeline,  # The model pipeline to tune\n",
    "            param_grid=self.param_grid,  # The hyperparameters to search\n",
    "            scoring=\"accuracy\",  # Scoring method for evaluation\n",
    "            cv=TimeSeriesSplit(n_splits=5),  # Cross-validation strategy for time series\n",
    "            n_jobs=-1,  # Use all available CPU cores for parallel processing\n",
    "            verbose=4,  # Verbosity level for detailed output during fitting\n",
    "        )\n",
    "        # Initialize accuracy tracking variables\n",
    "        self.average_train_accuracy = None\n",
    "        self.average_test_accuracy = None\n",
    "        self.last_split_train_accuracy = None\n",
    "        self.last_split_test_accuracy = None\n",
    "\n",
    "    def _load_data(self):\n",
    "        # Load the CSV data, skipping the specified number of rows, and reset the index\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "        return df.iloc[self.skip_rows :].reset_index(drop=True)\n",
    "\n",
    "    def _create_pipeline(self):\n",
    "        # Create a machine learning pipeline with the following steps:\n",
    "        return Pipeline(\n",
    "            steps=[\n",
    "                (\n",
    "                    \"CategoricalFeatures\",  # Step name for categorical feature processing\n",
    "                    ColumnTransformer(\n",
    "                        transformers=[\n",
    "                            (\n",
    "                                \"cat\",  # Transformer name\n",
    "                                OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),  # One-hot encoding for categorical features\n",
    "                                [\"home_team_name\", \"away_team_name\"],  # Columns to apply one-hot encoding\n",
    "                            ),\n",
    "                            (\n",
    "                                'home_team_rank', FunctionTransformer(lambda x: np.column_stack([\n",
    "                                    np.cos(2 * np.pi * x / 20),\n",
    "                                    np.sin(2 * np.pi * x / 20)\n",
    "                                    ]), validate=True), [\"home_team_rank\"]\n",
    "                            ),\n",
    "                            (\n",
    "                                'away_team_rank', FunctionTransformer(lambda x: np.column_stack([\n",
    "                                    np.cos(2 * np.pi * x / 20),\n",
    "                                    np.sin(2 * np.pi * x / 20)\n",
    "                                    ]), validate=True), [\"away_team_rank\"]\n",
    "                            ),\n",
    "                            (\n",
    "                                'day_of_week', FunctionTransformer(lambda x: np.column_stack([\n",
    "                                    np.cos(2 * np.pi * x / 7),\n",
    "                                    np.sin(2 * np.pi * x / 7)\n",
    "                                    ]), validate=True), [\"day_of_week\"]\n",
    "                            ),\n",
    "                            (\n",
    "                                'hour_of_day', FunctionTransformer(lambda x: np.column_stack([\n",
    "                                    np.cos(2 * np.pi * x / 24),\n",
    "                                    np.sin(2 * np.pi * x / 24)\n",
    "                                    ]), validate=True), [\"hour_of_day\"]\n",
    "                            ),\n",
    "                            ('home_team_points', SimpleImputer(strategy='mean'), ['home_team_points']),\n",
    "                            ('away_team_points', SimpleImputer(strategy='mean'), ['away_team_points']),\n",
    "                            ('home_team_goals_for', SimpleImputer(strategy='mean'), ['home_team_goals_for']),\n",
    "                            ('away_team_goals_for', SimpleImputer(strategy='mean'), ['away_team_goals_for']),\n",
    "                            ('home_team_goals_againsts', SimpleImputer(strategy='mean'), ['home_team_goals_againsts']),\n",
    "                            ('away_team_goals_against', SimpleImputer(strategy='mean'), ['away_team_goals_against']),\n",
    "                            ('home_team_goals_difference', SimpleImputer(strategy='mean'), ['home_team_goals_difference']),\n",
    "                            ('away_team_goals_difference', SimpleImputer(strategy='mean'), ['away_team_goals_difference'])\n",
    "                        ]\n",
    "                    ),\n",
    "                ),\n",
    "                (\"StandardScaler\", StandardScaler(with_mean=True)),  # Standardize the features\n",
    "                (\"SGDClassifier\", SGDClassifier(random_state=self.random_state, max_iter=1000)),  # SGD classifier for classification\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _create_param_grid(self):\n",
    "        # Define the hyperparameter grid for tuning the SGDClassifier\n",
    "        return {\n",
    "            \"SGDClassifier__tol\": [1e-2, 1e-3, 1e-4],  # Tolerance for stopping criterion\n",
    "            \"SGDClassifier__alpha\": [1e-3, 1e-4, 1e-5, 1e-6, 1e-7],  # Regularization strength\n",
    "            \"SGDClassifier__penalty\": [\"l2\", \"l1\", \"elasticnet\"],  # Regularization type\n",
    "            \"SGDClassifier__loss\": [\"hinge\", \"log_loss\", \"modified_huber\"],  # Loss function\n",
    "        }\n",
    "\n",
    "    def train(self):\n",
    "        # Train the model using time series cross-validation\n",
    "        tscv = TimeSeriesSplit(n_splits=5)  # Split data for time series cross-validation\n",
    "        scores = []  # List to store accuracy scores for each fold\n",
    "\n",
    "        # Iterate over the splits\n",
    "        for split, (train_index, test_index) in enumerate(tscv.split(self.X), 1):\n",
    "            # Split data into training and test sets\n",
    "            X_train, X_test = self.X.iloc[train_index], self.X.iloc[test_index]\n",
    "            y_train, y_test = self.y.iloc[train_index], self.y.iloc[test_index]\n",
    "            self.model.fit(X_train, y_train)  # Fit the model to the training data\n",
    "            accuracy_train = self.model.score(X_train, y_train)  # Calculate training accuracy\n",
    "            accuracy_test = self.model.score(X_test, y_test)  # Calculate test accuracy\n",
    "            scores.append((accuracy_train, accuracy_test))  # Store the scores for this split\n",
    "\n",
    "        # Calculate the average accuracy across all splits\n",
    "        self.average_train_accuracy = sum([score[0] for score in scores]) / len(scores)\n",
    "        self.average_test_accuracy = sum([score[1] for score in scores]) / len(scores)\n",
    "        # Store the accuracy for the last split\n",
    "        self.last_split_train_accuracy = scores[-1][0]\n",
    "        self.last_split_test_accuracy = scores[-1][1]\n",
    "\n",
    "        # Print the accuracy results\n",
    "        print(f\"Average Train Accuracy: {self.average_train_accuracy:.4f}\")\n",
    "        print(f\"Average Test Accuracy: {self.average_test_accuracy:.4f}\")\n",
    "        print(f\"Last Split Train Accuracy: {self.last_split_train_accuracy:.4f}\")\n",
    "        print(f\"Last Split Test Accuracy: {self.last_split_test_accuracy:.4f}\")\n",
    "\n",
    "        # Verificar si el modelo tiene coef_\n",
    "        if hasattr(self.model, \"coef_\"):\n",
    "            feature_importance = np.abs(self.model.coef_).mean(axis=0)  # Promediar si es multi-clase\n",
    "            feature_importance_df = pd.DataFrame({\n",
    "                'Feature': self.X.columns,\n",
    "                'Importance': feature_importance\n",
    "            }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "            print(\"\\nFeature Importance:\")\n",
    "            print(feature_importance_df)\n",
    "\n",
    "            self.feature_importance_ = feature_importance_df  # Guardar para acceso posterior\n",
    "        else:\n",
    "            print(\"Feature importance not available for this model.\")\n",
    "\n",
    "        # Call the function to log the model to MLflow\n",
    "        self.log_to_mlflow()\n",
    "\n",
    "    def log_to_mlflow(self):\n",
    "        # Ask if the user wants to save the model to MLflow\n",
    "        save_model = input(\"Do you want to save the model in MLflow? (yes/no): \").strip().lower()\n",
    "        if save_model not in [\"yes\", \"y\"]:\n",
    "            print(\"Model was not saved to MLflow.\")\n",
    "            return  # Exit the function if the model is not to be saved\n",
    "\n",
    "        # Ask for the run name and description\n",
    "        run_name = input(\"Enter the run name: \").strip()\n",
    "        description = input(\"Enter the run description: \").strip()\n",
    "\n",
    "        # Set the experiment name and tracking URI for MLflow\n",
    "        experiment_name = \"BetPredictions\"\n",
    "        tracking_uri = os.path.abspath(\"mlruns\")\n",
    "        mlflow.set_tracking_uri(f\"file:///{tracking_uri}\")\n",
    "\n",
    "        # Start the MLflow UI if it's not already running\n",
    "        subprocess.Popen(f\"mlflow ui --backend-store-uri file:///{tracking_uri}\", shell=True)\n",
    "\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "\n",
    "        # Log the model, metrics, parameters, and features to MLflow\n",
    "        with mlflow.start_run(run_name=run_name, description=description):\n",
    "            # Log the accuracy metrics\n",
    "            mlflow.log_metric(\"Average Train Accuracy\", self.average_train_accuracy)\n",
    "            mlflow.log_metric(\"Average Test Accuracy\", self.average_test_accuracy)\n",
    "            mlflow.log_metric(\"Last Split Train Accuracy\", self.last_split_train_accuracy)\n",
    "            mlflow.log_metric(\"Last Split Test Accuracy\", self.last_split_test_accuracy)\n",
    "\n",
    "            # Log the features used in the model\n",
    "            features_used = self.X.columns.tolist()\n",
    "            mlflow.log_param(\"features\", features_used)\n",
    "\n",
    "            # Log the best hyperparameters if available\n",
    "            if hasattr(self.model, 'best_params_'):\n",
    "                mlflow.log_params(self.model.best_params_)\n",
    "\n",
    "            # Print a message indicating the model has been logged\n",
    "            print(\"Model, metrics, parameters, and features logged to MLflow.\")\n",
    "\n",
    "        # Ask if the user wants to open the MLflow UI\n",
    "        open_mlflow = input(\"Do you want to open the MLflow UI page? (yes/no): \").strip().lower()\n",
    "        if open_mlflow in [\"yes\", \"y\"]:\n",
    "            # Open the MLflow experiment page in the browser\n",
    "            experiment_id = \"492606161886242227\"  # Replace with your experiment ID\n",
    "            mlflow_url = f\"http://127.0.0.1:5000/#/experiments/{experiment_id}\"\n",
    "            # For Windows, use the 'start' command\n",
    "            subprocess.Popen(f\"start {mlflow_url}\", shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 135 candidates, totalling 675 fits\n",
      "Fitting 5 folds for each of 135 candidates, totalling 675 fits\n",
      "Fitting 5 folds for each of 135 candidates, totalling 675 fits\n",
      "Fitting 5 folds for each of 135 candidates, totalling 675 fits\n",
      "Fitting 5 folds for each of 135 candidates, totalling 675 fits\n",
      "Average Train Accuracy: 0.6877\n",
      "Average Test Accuracy: 0.3889\n",
      "Last Split Train Accuracy: 0.6085\n",
      "Last Split Test Accuracy: 0.3953\n",
      "Feature importance not available for this model.\n",
      "Model was not saved to MLflow.\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"final_dataset.csv\"\n",
    "\n",
    "skip_rows = 0\n",
    "\n",
    "random_state = 0\n",
    "\n",
    "pipeline = ModelPipeline(csv_path, skip_rows, random_state)\n",
    "\n",
    "pipeline.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class ModelPipeline:\\n    def __init__(self, csv_path, skip_rows, random_state):\\n        self.csv_path = csv_path\\n        self.skip_rows = skip_rows\\n        self.df = self._load_data()\\n        self.X = self.df.drop(columns=[\\'target\\'])\\n        self.y = self.df[\\'target\\']\\n        self.random_state = random_state\\n        self.pipeline = self._create_pipeline()\\n        self.param_grid = self._create_param_grid()\\n        self.model = GridSearchCV(\\n            estimator=self.pipeline,\\n            param_grid=self.param_grid,\\n            scoring=\\'f1_weighted\\',\\n            cv=TimeSeriesSplit(n_splits=5),\\n            n_jobs=-1,\\n            verbose=4\\n        )\\n\\n    def _load_data(self):\\n        df = pd.read_csv(self.csv_path)\\n        return df.iloc[self.skip_rows:].reset_index(drop=True)\\n\\n    def _create_pipeline(self):\\n        return Pipeline(\\n            steps=[\\n                (\\'CategoricalFeatures\\', ColumnTransformer(\\n                    transformers=[\\n                        (\\'cat\\', OneHotEncoder(handle_unknown=\\'ignore\\', sparse_output=False), [\\n                            \"home_team_name\",\\n                            \"away_team_name\",\\n                            \"referee\",\\n                            \"var\",\\n                            \"stadium\",\\n                        ]),\\n                        (\\'gameweek\\', FunctionTransformer(lambda x: np.column_stack([\\n                            np.cos(2 * np.pi * x / 38),\\n                            np.sin(2 * np.pi * x / 38)\\n                        ]), validate=True), [\"gameweek\"]),\\n                        (\\'hour_of_the_day\\', FunctionTransformer(lambda x: np.column_stack([\\n                            np.cos(2 * np.pi * x / 24),\\n                            np.sin(2 * np.pi * x / 24)\\n                        ]), validate=True), [\"hour_of_the_day\"]),\\n                        (\\'avg_home_wins_last_10\\', SimpleImputer(strategy=\\'mean\\'), [\\'avg_home_wins_last_10\\']),\\n                        (\\'avg_away_wins_last_10\\', SimpleImputer(strategy=\\'mean\\'), [\\'avg_away_wins_last_10\\']),\\n                        (\\'avg_home_wins_last_5_home\\', SimpleImputer(strategy=\\'mean\\'), [\\'avg_home_wins_last_3_home\\']),\\n                        (\\'avg_away_wins_last_5_away\\', SimpleImputer(strategy=\\'mean\\'), [\\'avg_away_wins_last_3_away\\']),\\n                        (\\'avg_home_attendance_perc_last_10\\', SimpleImputer(strategy=\\'mean\\'), [\\'avg_home_attendance_perc_last_10\\']),\\n                        (\\'avg_home_attendance_last_3\\', SimpleImputer(strategy=\\'mean\\'), [\\'avg_home_attendance_last_3\\']),\\n                        (\\'avg_home_attendance_perc_last_3\\', SimpleImputer(strategy=\\'mean\\'), [\\'avg_home_attendance_perc_last_3\\']),\\n                        (\\'avg_away_attendance_perc_last_10\\', SimpleImputer(strategy=\\'mean\\'), [\\'avg_away_attendance_perc_last_10\\']),\\n                        (\\'avg_away_attendance_perc_last_3\\', SimpleImputer(strategy=\\'mean\\'), [\\'avg_away_attendance_perc_last_3\\'])\\n                    ]\\n                )),\\n                (\"StandardScaler\", StandardScaler(with_mean=True)),\\n                (\"SGDClassifier\", SGDClassifier(\\n                    random_state=self.random_state,\\n                    max_iter=1000\\n                ))\\n            ]\\n        )\\n\\n    def _create_param_grid(self):\\n        return {\\n            \"SGDClassifier__tol\": [1e-2, 1e-3, 1e-4],\\n            \"SGDClassifier__alpha\": [1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\\n            \"SGDClassifier__penalty\": [\\'l2\\', \\'l1\\', \\'elasticnet\\'],\\n            \"SGDClassifier__loss\": [\\'hinge\\', \\'log_loss\\', \\'modified_huber\\']\\n        }\\n\\n    def train(self):\\n        tscv = TimeSeriesSplit(n_splits=5)\\n        scores = []\\n        f1_scores_train = []\\n        f1_scores_test = []\\n\\n        for split, (train_index, test_index) in enumerate(tscv.split(self.X), 1):\\n            X_train, X_test = self.X.iloc[train_index], self.X.iloc[test_index]\\n            y_train, y_test = self.y.iloc[train_index], self.y.iloc[test_index]\\n            self.model.fit(X_train, y_train)\\n            accuracy_train = self.model.score(X_train, y_train)\\n            accuracy_test = self.model.score(X_test, y_test)\\n            scores.append((accuracy_train, accuracy_test))\\n            y_pred_train = self.model.predict(X_train)\\n            y_pred_test = self.model.predict(X_test)\\n            f1_train = f1_score(y_train, y_pred_train, average=\\'weighted\\')\\n            f1_test = f1_score(y_test, y_pred_test, average=\\'weighted\\')\\n            f1_scores_train.append(f1_train)\\n            f1_scores_test.append(f1_test)\\n\\n        self.plot_splits(tscv)\\n\\n        average_train_accuracy = sum([score[0] for score in scores]) / len(scores)\\n        average_test_accuracy = sum([score[1] for score in scores]) / len(scores)\\n        average_train_f1 = sum(f1_scores_train) / len(f1_scores_train)\\n        average_test_f1 = sum(f1_scores_test) / len(f1_scores_test)\\n\\n        print(f\"Average Train Accuracy: {average_train_accuracy:.4f}\")\\n        print(f\"Average Test Accuracy: {average_test_accuracy:.4f}\")\\n        print(f\"Average Train F1-Score: {average_train_f1:.4f}\")\\n        print(f\"Average Test F1-Score: {average_test_f1:.4f}\")\\n\\n    def plot_splits(self, tscv):\\n        plt.figure(figsize=(12, 6))\\n        for i, (train_index, test_index) in enumerate(tscv.split(self.X)):\\n            plt.plot(train_index, [i + 1] * len(train_index), \\'|\\', color=\\'blue\\', label=\\'Train Set\\' if i == 0 else \"\", markersize=10)\\n            plt.plot(test_index, [i + 1] * len(test_index), \\'|\\', color=\\'orange\\', label=\\'Test Set\\' if i == 0 else \"\", markersize=10)\\n        plt.title(\"División Temporal: Train vs Test\")\\n        plt.xlabel(\"Índice\")\\n        plt.ylabel(\"División\")\\n        plt.legend()\\n        plt.grid(axis=\\'x\\', linestyle=\\'--\\', alpha=0.7)\\n        plt.show()\\n\\n    def log_to_mlflow(self):\\n        experiment_name = \"BetPredictions\"\\n        mlflow.set_experiment(experiment_name)\\n\\n        save_results = input(\"Do you want to save results to MLflow? (yes/no): \").strip().lower()\\n        if save_results == \\'yes\\':\\n            run_name = input(\"Enter run name: \").strip()\\n            description = input(\"Enter run description: \").strip()\\n\\n            with mlflow.start_run(run_name=run_name, description=description):\\n                if hasattr(self.model, \\'best_params_\\'):\\n                    mlflow.log_params(self.model.best_params_)\\n\\n                features_used = self.X.columns.tolist()\\n                mlflow.log_param(\"features\", features_used)\\n\\n                if hasattr(self.model, \\'best_estimator_\\'):\\n                    mlflow.sklearn.log_model(self.model.best_estimator_, \"best_model_pipeline\")\\n\\n                print(\"Model, metrics, parameters, and features logged to MLflow.\")'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class ModelPipeline:\n",
    "    def __init__(self, csv_path, skip_rows, random_state):\n",
    "        self.csv_path = csv_path\n",
    "        self.skip_rows = skip_rows\n",
    "        self.df = self._load_data()\n",
    "        self.X = self.df.drop(columns=['target'])\n",
    "        self.y = self.df['target']\n",
    "        self.random_state = random_state\n",
    "        self.pipeline = self._create_pipeline()\n",
    "        self.param_grid = self._create_param_grid()\n",
    "        self.model = GridSearchCV(\n",
    "            estimator=self.pipeline,\n",
    "            param_grid=self.param_grid,\n",
    "            scoring='f1_weighted',\n",
    "            cv=TimeSeriesSplit(n_splits=5),\n",
    "            n_jobs=-1,\n",
    "            verbose=4\n",
    "        )\n",
    "\n",
    "    def _load_data(self):\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "        return df.iloc[self.skip_rows:].reset_index(drop=True)\n",
    "\n",
    "    def _create_pipeline(self):\n",
    "        return Pipeline(\n",
    "            steps=[\n",
    "                ('CategoricalFeatures', ColumnTransformer(\n",
    "                    transformers=[\n",
    "                        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), [\n",
    "                            \"home_team_name\",\n",
    "                            \"away_team_name\",\n",
    "                            \"referee\",\n",
    "                            \"var\",\n",
    "                            \"stadium\",\n",
    "                        ]),\n",
    "                        ('gameweek', FunctionTransformer(lambda x: np.column_stack([\n",
    "                            np.cos(2 * np.pi * x / 38),\n",
    "                            np.sin(2 * np.pi * x / 38)\n",
    "                        ]), validate=True), [\"gameweek\"]),\n",
    "                        ('hour_of_the_day', FunctionTransformer(lambda x: np.column_stack([\n",
    "                            np.cos(2 * np.pi * x / 24),\n",
    "                            np.sin(2 * np.pi * x / 24)\n",
    "                        ]), validate=True), [\"hour_of_the_day\"]),\n",
    "                        ('avg_home_wins_last_10', SimpleImputer(strategy='mean'), ['avg_home_wins_last_10']),\n",
    "                        ('avg_away_wins_last_10', SimpleImputer(strategy='mean'), ['avg_away_wins_last_10']),\n",
    "                        ('avg_home_wins_last_5_home', SimpleImputer(strategy='mean'), ['avg_home_wins_last_3_home']),\n",
    "                        ('avg_away_wins_last_5_away', SimpleImputer(strategy='mean'), ['avg_away_wins_last_3_away']),\n",
    "                        ('avg_home_attendance_perc_last_10', SimpleImputer(strategy='mean'), ['avg_home_attendance_perc_last_10']),\n",
    "                        ('avg_home_attendance_last_3', SimpleImputer(strategy='mean'), ['avg_home_attendance_last_3']),\n",
    "                        ('avg_home_attendance_perc_last_3', SimpleImputer(strategy='mean'), ['avg_home_attendance_perc_last_3']),\n",
    "                        ('avg_away_attendance_perc_last_10', SimpleImputer(strategy='mean'), ['avg_away_attendance_perc_last_10']),\n",
    "                        ('avg_away_attendance_perc_last_3', SimpleImputer(strategy='mean'), ['avg_away_attendance_perc_last_3'])\n",
    "                    ]\n",
    "                )),\n",
    "                (\"StandardScaler\", StandardScaler(with_mean=True)),\n",
    "                (\"SGDClassifier\", SGDClassifier(\n",
    "                    random_state=self.random_state,\n",
    "                    max_iter=1000\n",
    "                ))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _create_param_grid(self):\n",
    "        return {\n",
    "            \"SGDClassifier__tol\": [1e-2, 1e-3, 1e-4],\n",
    "            \"SGDClassifier__alpha\": [1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n",
    "            \"SGDClassifier__penalty\": ['l2', 'l1', 'elasticnet'],\n",
    "            \"SGDClassifier__loss\": ['hinge', 'log_loss', 'modified_huber']\n",
    "        }\n",
    "\n",
    "    def train(self):\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        scores = []\n",
    "        f1_scores_train = []\n",
    "        f1_scores_test = []\n",
    "\n",
    "        for split, (train_index, test_index) in enumerate(tscv.split(self.X), 1):\n",
    "            X_train, X_test = self.X.iloc[train_index], self.X.iloc[test_index]\n",
    "            y_train, y_test = self.y.iloc[train_index], self.y.iloc[test_index]\n",
    "            self.model.fit(X_train, y_train)\n",
    "            accuracy_train = self.model.score(X_train, y_train)\n",
    "            accuracy_test = self.model.score(X_test, y_test)\n",
    "            scores.append((accuracy_train, accuracy_test))\n",
    "            y_pred_train = self.model.predict(X_train)\n",
    "            y_pred_test = self.model.predict(X_test)\n",
    "            f1_train = f1_score(y_train, y_pred_train, average='weighted')\n",
    "            f1_test = f1_score(y_test, y_pred_test, average='weighted')\n",
    "            f1_scores_train.append(f1_train)\n",
    "            f1_scores_test.append(f1_test)\n",
    "\n",
    "        self.plot_splits(tscv)\n",
    "\n",
    "        average_train_accuracy = sum([score[0] for score in scores]) / len(scores)\n",
    "        average_test_accuracy = sum([score[1] for score in scores]) / len(scores)\n",
    "        average_train_f1 = sum(f1_scores_train) / len(f1_scores_train)\n",
    "        average_test_f1 = sum(f1_scores_test) / len(f1_scores_test)\n",
    "\n",
    "        print(f\"Average Train Accuracy: {average_train_accuracy:.4f}\")\n",
    "        print(f\"Average Test Accuracy: {average_test_accuracy:.4f}\")\n",
    "        print(f\"Average Train F1-Score: {average_train_f1:.4f}\")\n",
    "        print(f\"Average Test F1-Score: {average_test_f1:.4f}\")\n",
    "\n",
    "    def plot_splits(self, tscv):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for i, (train_index, test_index) in enumerate(tscv.split(self.X)):\n",
    "            plt.plot(train_index, [i + 1] * len(train_index), '|', color='blue', label='Train Set' if i == 0 else \"\", markersize=10)\n",
    "            plt.plot(test_index, [i + 1] * len(test_index), '|', color='orange', label='Test Set' if i == 0 else \"\", markersize=10)\n",
    "        plt.title(\"División Temporal: Train vs Test\")\n",
    "        plt.xlabel(\"Índice\")\n",
    "        plt.ylabel(\"División\")\n",
    "        plt.legend()\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "    def log_to_mlflow(self):\n",
    "        experiment_name = \"BetPredictions\"\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "\n",
    "        save_results = input(\"Do you want to save results to MLflow? (yes/no): \").strip().lower()\n",
    "        if save_results == 'yes':\n",
    "            run_name = input(\"Enter run name: \").strip()\n",
    "            description = input(\"Enter run description: \").strip()\n",
    "\n",
    "            with mlflow.start_run(run_name=run_name, description=description):\n",
    "                if hasattr(self.model, 'best_params_'):\n",
    "                    mlflow.log_params(self.model.best_params_)\n",
    "\n",
    "                features_used = self.X.columns.tolist()\n",
    "                mlflow.log_param(\"features\", features_used)\n",
    "\n",
    "                if hasattr(self.model, 'best_estimator_'):\n",
    "                    mlflow.sklearn.log_model(self.model.best_estimator_, \"best_model_pipeline\")\n",
    "\n",
    "                print(\"Model, metrics, parameters, and features logged to MLflow.\")\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "api-python-bet-project-c88tlEpI-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
