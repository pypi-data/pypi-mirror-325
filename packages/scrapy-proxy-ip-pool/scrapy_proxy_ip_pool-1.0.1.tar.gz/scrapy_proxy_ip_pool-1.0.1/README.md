# Scrapy Proxy IP Pool

> è¿™æ˜¯ä¸€ä¸ªScrapyä¸­é—´ä»¶ï¼Œç”¨äºç®¡ç†ä»£ç†IPæ± ã€‚

* ğŸ¥³æ”¯æŒRedis:æ”¯æŒä½¿ç”¨Redisçš„IPä»£ç†æ± (ä¹Ÿå¯æœ¬åœ°Listä½œä¸ºä»£ç†æ± )
* ğŸ¥µæœ€å¤§é™åº¦æ¦¨å¹²æ¯ä¸ªIP: åªæœ‰è¯·æ±‚ä¸ºæŒ‡å®šå¼‚å¸¸orçŠ¶æ€ç (è¢«å°)æ—¶æ‰ä¼šæ›´æ¢IP
* ğŸ¤Œç®€å•é…ç½®: ezä¸‰æ­¥å³å¯ä½¿ç”¨

> å¦‚æœä½ ä¸çŸ¥é“å¦‚ä½•å†™"ä»£ç†IPæ± "å¯ä»¥èŠ±å‡ åˆ†é’Ÿçœ‹ä¸‹(<10min)ç„¶åå¯ä»¥è‡ªå·±å†™,å½“ç„¶ä¹Ÿå¯ä»¥ç”¨æˆ‘å†™çš„ç°æˆä»£ç†æ± 

## å®‰è£…

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š

```bash
pip install scrapy_proxy_ip_pool
```

## åŸºæœ¬ä½¿ç”¨

### STEP1:åœ¨`setting.py`ä¸­æ·»åŠ è¯¥ä¸­é—´ä»¶

* è¦æ±‚â—: è¿™é‡Œçš„æƒå€¼ä¸€å®šè¦å¤§äº`RetryMiddleware`(é‡è¯•ä¸­é—´ä»¶é»˜è®¤550)

```python
PROXY_POOL_SIZE = 1  # ä»£ç†æ± çš„å¤§å°(æœ‰å°ipçš„ç½‘ç«™å»ºè®®å¼€å¤§ä¸€ç‚¹)
DOWNLOADER_MIDDLEWARES = {
    'scrapy_proxy_ip_pool.proxy_pool_downloader_middleware.ProxyPoolDownloaderMiddleware': 551,
}
```

### STEP2:åœ¨ä½ çš„spiderä¸­å¿…é¡»è¦ç¼–å†™`def get_proxy_ip(self):->str`æ–¹æ³•

* è¦æ±‚â—:åå­—åªèƒ½æ˜¯è¿™ä¸ª,è¿”å›å€¼æ˜¯ä¸€ä¸ªä»£ç†IP.
* è¯´æ˜: ä»£ç†æ± ä¸­é—´ä»¶ä¼šè¯»å–è¿™ä¸ªæ–¹æ³•,ä»è€Œè·å–ä»£ç†IPåœ°å€.

```python
import requests
import scrapy


class IpSpider(scrapy.Spider):
    name = "ip"
    REDIS_KEY = name

    def parse(self, response, **kwargs):
        pass

    # ç¤ºä¾‹
    def get_proxy_ip(self):
        api_url = "ä½ è¯·æ±‚ä»˜è´¹ä»£ç†çš„åœ°å€"
        proxy_ip = requests.get(api_url).text
        username = "ä½ çš„ä»£ç†ç”¨æˆ·å"
        password = "ä½ çš„ä»£ç†å¯†ç "
        return f"http://{username}:{password}@{proxy_ip}/"  # ä¸€æ¬¡åªéœ€è¦è¿”å›ä¸€ä¸ªip
```

## å…¶ä»–é…ç½®

### ä¸€. ä½¿ç”¨Redisä½œä¸º ä»£ç†æ± 

* `settings.py`ä¸­é…ç½®å¦‚ä¸‹å†…å®¹:

```python
PROXY_POOL_ENABLED = 'True'  # ä½¿ç”¨Redisè¿›è¡Œä»£ç†æ± çš„æ„å»º(é»˜è®¤ä¸ºFalse)
REDIS_URL = "redis://127.0.0.1:6379/0"
```

* `spider`ä¸­æ·»åŠ `REDIS_KEY`å˜é‡æŒ‡æ˜å­˜å‚¨åœ¨Redisä¸­ä½¿ç”¨çš„é”®

```python
import scrapy


class IpSpider(scrapy.Spider):
    name = "ip"
    REDIS_KEY = name  # æœ€ç»ˆä¸º REDIS_KEY+":proxy_pool"
    ...
```

### äºŒ. æŒ‡å®šå¼‚å¸¸å’ŒçŠ¶æ€ç 

* `settings.py`ä¸­é…ç½®å¦‚ä¸‹å†…å®¹
* è¯´æ˜: å¦‚æœé‡åˆ°äº†è¿™äº›å¼‚å¸¸, å°±ä¼šæ›´æ–°IP; å¦‚ä¸‹é»˜è®¤é…ç½®çš„æ˜¯å¸¸è§å¯èƒ½è¢«å°å¼‚å¸¸å’ŒçŠ¶æ€ç 
* æ³¨æ„â—:è¿™é‡Œä¾æ®ç½‘å€è€Œå®š, å¦‚æœä¸ç¡®å®š, ä¸¤ä¸ªå¯ä»¥è®¾ç½®ä¸ºç©ºåˆ—è¡¨,åç»­æ ¹æ®æŠ¥é”™æ”¹

```python
# å¦‚ä¸‹åˆ—ä¸¾äº†å¸¸è§çš„è¢«å°ç¦ip(æˆ–ipä¸å¯ç”¨)æ—¶çš„å¼‚å¸¸æˆ–çŠ¶æ€ç , ç”¨æˆ·å¯ä»¥æ ¹æ®ç›®æ ‡ç½‘ç«™çš„æƒ…å†µåœ¨settings.pyä¸­è‡ªè¡Œé…ç½®
NEED_UPDATE_PROXY_EXCEPTIONS = [
    'twisted.internet.defer.TimeoutError',  # è¯·æ±‚è¶…æ—¶æœªå“åº”ï¼Œå¯èƒ½ç›®æ ‡æœåŠ¡å™¨æ£€æµ‹åˆ°ä»£ç†IPå¼‚å¸¸å¯¼è‡´æ•…æ„å»¶è¿Ÿä¸å“åº”
    'twisted.internet.error.TimeoutError',  # åº•å±‚ç½‘ç»œè¿æ¥è¶…æ—¶ï¼Œå¯èƒ½ä»£ç†æœåŠ¡å™¨IPè¢«ç›®æ ‡ç½‘ç«™å°é”å¯¼è‡´æ— æ³•å»ºç«‹è¿æ¥
    'twisted.internet.error.ConnectError',  # ä¸ä»£ç†æœåŠ¡å™¨å»ºç«‹è¿æ¥å¤±è´¥ï¼Œå¯èƒ½ä»£ç†IPå·²è¢«é˜²ç«å¢™å°ç¦æˆ–æœåŠ¡å™¨å·²ä¸‹çº¿
    'scrapy.core.downloader.handlers.http11.TunnelError',  # ä»£ç†æœåŠ¡å™¨è¦æ±‚èº«ä»½éªŒè¯æˆ–ç›®æ ‡ç½‘ç«™å°ç¦è¯¥ä»£ç†IPï¼Œå¯¼è‡´æ— æ³•å»ºç«‹HTTPSéš§é“è¿æ¥
]

NEED_UPDATE_PROXY_CODES = [
    503,  # æœåŠ¡ä¸å¯ç”¨ï¼ŒæœåŠ¡å™¨å¯èƒ½æ­£åœ¨ä¸»åŠ¨æ‹’ç»æ¥è‡ªè¯¥ä»£ç†IPçš„è¯·æ±‚ï¼ˆåçˆ¬æœºåˆ¶è§¦å‘ï¼‰
    407,  # ä»£ç†èº«ä»½éªŒè¯å¤±è´¥ï¼Œæˆ–ä»£ç†æœåŠ¡æä¾›å•†å·²å°ç¦å½“å‰IPçš„è®¿é—®æƒé™
    403,  # æœåŠ¡å™¨æ˜ç¡®æ‹’ç»è®¿é—®ï¼Œé€šå¸¸è¡¨ç¤ºå½“å‰ä»£ç†IPå·²è¢«åŠ å…¥é»‘åå•
    429,  # è¯·æ±‚é¢‘ç‡è¶…é™ï¼Œç›®æ ‡ç½‘ç«™é’ˆå¯¹è¯¥ä»£ç†IPå®æ–½äº†é€Ÿç‡é™åˆ¶
]
```

## å…³é”®ä»£ç 

#### ä¸€. æ›´æ–°IP

* â±ä»€ä¹ˆæ—¶å€™ä¼šè¢«è°ƒç”¨:åªæœ‰å‡ºç° `NEED_UPDATE_PROXY_EXCEPTIONS` æˆ– `NEED_UPDATE_PROXY_CODES` ä¸­çš„å¼‚å¸¸æˆ–çŠ¶æ€ç ,è¯¥æ–¹æ³•æ‰ä¼šè¢«è°ƒç”¨

```python
# ä¼ªä»£ç 
def update_proxy(self, request: Request) -> Request:
    last_ip = æœ¬æ¬¡(æœ‰é—®é¢˜)
    è¯·æ±‚çš„ä»£ç†ip
    if last_ip in ä»£ç†æ± :
        åœ¨ä»£ç†æ± ä¸­æ›´æ¢è¯¥(æœ‰é—®é¢˜)
        çš„ip
    request.meta['proxy'] = æ–°ip
    return request
```

```python
def update_proxy(self, request: Request) -> Request:
    """
    ä½¿ç”¨"ä¹è§‚é”"æ€æƒ³,æ›´æ–°è¿‡æ—¶IP,å¹¶è¿”å›æºå¸¦æ–°IPçš„request
    :param request: æœªæˆåŠŸçš„ç½‘ç»œè¯·æ±‚
    :return: è¿”å›æºå¸¦æ–°IPçš„request
    """
    # è§£å†³request.meta['proxy']é‰´æƒä¸¢å¤±é—®é¢˜
    last_proxy = request.meta['proxy'].split("//")[1]
    temp_proxy_list = self.client.smembers(self.redis_key) if self.proxy_pool_enabled else self.proxy_pool
    need_update = any(last_proxy in ip for ip in temp_proxy_list)  # å­˜åœ¨=>éœ€è¦æ›¿æ¢
    # å½“å‰éœ€è¦è·å–æ–°çš„ip
    if need_update:
        # æ›¿æ¢"ä»£ç†æ± "ä¸­çš„IP
        if self.proxy_pool_enabled:  # Redis
            self.client.srem(self.redis_key, last_proxy)
            self.client.sadd(self.redis_key, self.get_proxy_with_logging())
        else:  # æœ¬åœ°
            self.proxy_pool.remove(last_proxy)
            self.proxy_pool.append(self.get_proxy_with_logging())
    # æ›´æ¢ä»£ç†IPè¿›è¡Œè¯·æ±‚
    request.meta['proxy'] = self.get_proxy_from_pool()
    request.dont_filter = True  # é˜²æ­¢è¢«è¿‡æ»¤!!!
    return request
```

## Version

* `1.0.1`:ã€2025å¹´2æœˆ4æ—¥ã€‘
  1. ä¿®æ”¹`1.0.0`ä¸­çš„Bug(â‘ ä¿®æ”¹åŒ…å,â‘¡å®Œå–„README.mdæ–‡æ¡£)
  2. æ¨é€é¡¹ç›®åˆ°Pypi,ç”¨æˆ·å¯ä»¥ä½¿ç”¨pipè¿›è¡Œä¸‹è½½
  3. é¦–æ¬¡æ¨é€é¡¹ç›®åˆ°[Gitee](https://gitee.com/twilight-and-morning-mist/Scrapy-Proxy-IP-Pool): 
***
* `1.0.0`:ã€2025å¹´2æœˆ3æ—¥ã€‘
  1. é¦–æ¬¡æ¨é€è‡³[GitHub](https://github.com/Tlyer233/Scrapy-Proxy-IP-Pool),èƒ½å¤Ÿå®ç°ä»£ç†æ± åŠŸèƒ½