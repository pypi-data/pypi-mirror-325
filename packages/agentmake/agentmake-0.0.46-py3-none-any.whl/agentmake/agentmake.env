# General
DEVELOPER_MODE=FALSE
DEFAULT_AI_BACKEND=ollama
DEFAULT_SYSTEM_MESSAGE="You are an AI assistant."
DEFAULT_FOLLOW_UP_PROMPT="Please tell me more."
DEFAULT_TEXT_EDITOR=etextedit
DEFAULT_MARKDOWN_THEME="github-dark"
DEFAULT_WRITING_STYLE="standard English"
DEFAULT_MAXIMUM_ONLINE_SEARCHES=5
DEFAULT_FABRIC_PATTERNS_PATH=

# Backend: anthropic
ANTHROPIC_API_KEY=
ANTHROPIC_MODEL=claude-3-5-sonnet-latest
ANTHROPIC_TEMPERATURE=0.3
ANTHROPIC_MAX_TOKENS=8192

# Backend: azure
AZURE_API_KEY=
AZURE_API_ENDPOINT=
AZURE_API_VERSION=2024-10-21
AZURE_MODEL=gpt-4o
AZURE_TEMPERATURE=0.3
AZURE_MAX_TOKENS=16384

# Backend: cohere
# support multiple Cohere API keys, use comma ',' as separator
COHERE_API_KEY=
COHERE_MODEL=command-r-plus
COHERE_TEMPERATURE=0.3
COHERE_MAX_TOKENS=4000

# Backend: custom
CUSTOM_API_KEY=
CUSTOM_API_ENDPOINT=
CUSTOM_MODEL=
CUSTOM_TEMPERATURE=0.3
CUSTOM_MAX_TOKENS=4000

# Backend: deepseek
DEEPSEEK_API_KEY=
DEEPSEEK_MODEL=deepseek-chat
DEEPSEEK_TEMPERATURE = 0.3
DEEPSEEK_MAX_TOKENS = 8000

# Backend: github
# support multiple Github API keys, use comma ',' as separator
GITHUB_API_KEY=
GITHUB_MODEL=gpt-4o
GITHUB_TEMPERATURE=0.3
GITHUB_MAX_TOKENS=4000

# Backend: googleai
GOOGLEAI_API_KEY=${GEMINI_API_KEY}
GOOGLEAI_MODEL=gemini-1.5-pro
GOOGLEAI_TEMPERATURE=0.3
GOOGLEAI_MAX_TOKENS=8192

# Backend: vertexai
VERTEXAI_API_KEY=${GOOGLE_APPLICATION_CREDENTIALS}
VERTEXAI_API_PROJECT_ID=
VERTEXAI_API_SERVICE_LOCATION=us-central1
VERTEXAI_MODEL=gemini-1.5-pro
VERTEXAI_TEMPERATURE=0.3
VERTEXAI_MAX_TOKENS=8192

# Backend: groq
# support multiple Groq API keys, use comma ',' as separator
GROQ_API_KEY=
GROQ_MODEL=llama-3.3-70b-versatile
GROQ_TEMPERATURE=0.3
GROQ_MAX_TOKENS=32768

# Backend: llamacpp
LLAMACPP_API_ENDPOINT=http://127.0.0.1:8080/v1
LLAMACPP_TEMPERATURE=0.3
LLAMACPP_MAX_TOKENS=2048

# Backend: mistral
# support multiple Mistral API keys, use comma ',' as separator
MISTRAL_API_KEY=
MISTRAL_MODEL=mistral-large-latest
MISTRAL_TEMPERATURE=0.3
MISTRAL_MAX_TOKENS=8000

# Backend: ollama
# Empty the value of OLLAMA_ENDPOINT if you want to use dynamic local ip
OLLAMA_ENDPOINT="http://127.0.0.1:11434"
OLLAMA_MODEL=llama3.2
OLLAMA_TEMPERATURE=0.3
OLLAMA_MAX_TOKENS=-1
OLLAMA_CONTEXT_WINDOW=2048
OLLAMA_BATCH_SIZE=512
OLLAMA_KEEP_ALIVE=5m

# Backend: openai
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o
OPENAI_TEMPERATURE=0.3
OPENAI_MAX_TOKENS=16384

# Backend: xai
XAI_API_KEY=
XAI_MODEL=grok-2-latest
XAI_TEMPERATURE=0.3
XAI_MAX_TOKENS=127999

# Other APIs
# support multiple Tavily API keys, use comma ',' as separator
TAVILY_API_KEY=
OPENWEATHERMAP_API_KEY=

# Tool: Perplexica
PERPLEXICA_HOST="http://127.0.0.1"
PERPLEXICA_FRONTEND_PORT=3000
PERPLEXICA_BACKEND_PORT=3001
# local embedding options "xenova-bge-small-en-v1.5", "xenova-gte-small", "xenova-bert-base-multilingual-uncased"
PERPLEXICA_LOCAL_EMBEDDING="xenova-bge-small-en-v1.5"

# Tool: SearXNG
SEARXNG_HOST="http://127.0.0.1"
SEARXNG_PORT=4000