@article{VANBAELEN2023126636,
title = {Constraint guided gradient descent: Training with inequality constraints with applications in regression and semantic segmentation},
journal = {Neurocomputing},
volume = {556},
pages = {126636},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126636},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223007592},
author = {Quinten {Van Baelen} and Peter Karsmakers},
keywords = {Neuro-symbolic artificial intelligence, Neural networks, Constrained optimization, Flexible supervision, Learning and reasoning},
abstract = {Deep learning is typically performed by learning a neural network solely from data in the form of inputâ€“output pairs ignoring available domain knowledge. In this work, the Constraint Guided Gradient Descent (CGGD) framework is proposed that enables the injection of domain knowledge into the training procedure. The domain knowledge is assumed to be described as a conjunction of hard inequality constraints which appears to be a natural choice for several applications. Compared to other neuro-symbolic approaches, the proposed method converges to a point that makes an arbitrarily small error with respect to any inequality constraint on the training data and does not require to first transform the constraints into some ad-hoc term that is added to the learning (optimization) objective. It is empirically shown on four independent and small data sets that CGGD makes training less dependent on the initialization of the network, improves the constraint satisfiability on all data, improves the generalization of the model to unseen data, and relaxes the need for annotated data. Moreover, the method is tested for a regression and a semantic segmentation task.}
}