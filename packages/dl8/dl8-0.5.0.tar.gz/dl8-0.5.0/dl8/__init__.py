import pyperclip


def help():
    """–í—ã–≤–æ–¥–∏—Ç —Å–ø—Ä–∞–≤–∫—É –æ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–µ—Ç–æ–¥–∞—Ö."""
    help_message = "–°–ø—Ä–∞–≤–∫–∞ –ø–æ –º–µ—Ç–æ–¥–∞–º:\n"
    methods_info = {
        "mlp": "1. –ú–æ–¥–µ–ª—å –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞. –ü—Ä–æ–±–ª–µ–º–∞ –ª–∏–Ω–µ–π–Ω–æ –Ω–µ—Ä–∞–∑–¥–µ–ª–∏–º—ã—Ö –º–Ω–æ–∂–µ—Å—Ç–≤ –∏ –µ–µ —Ä–µ—à–µ–Ω–∏–µ. –õ–æ–≥–∏–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã—Ö –ò–ù–°. –õ–∏–Ω–µ–π–Ω—ã–µ —Å–ª–æ–∏ (Linear Layers) –≤ PyTorch.",
        "activ_func": "2. –§—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏. –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ñ—É–Ω–∫—Ü–∏—è–º –∞–∫—Ç–∏–≤–∞—Ü–∏–∏. –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏. –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏. –°–ª–æ–∏ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –≤ PyTorch. –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –≤ –º–æ–¥–µ–ª–∏",
        "dl_spring": "3. –í—Ç–æ—Ä–∞—è –≤–µ—Å–Ω–∞. –ü—Ä–∏—á–∏–Ω—ã –≤—Ç–æ—Ä–æ–π –≤–µ—Å–Ω—ã.",
        'grad': "4. –ì—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –º–Ω–æ–≥–∏—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö. –õ–æ–≥–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏. –ú–µ—Ç–æ–¥—ã –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞. –ü—Ä–æ–±–ª–µ–º—ã –ø–æ–∏—Å–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞",
        'cross_val': "5. –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è. –í—ã–±–æ—Ä–∫–∏ train, validation, test. –ü—Ä–æ–±–ª–µ–º–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞.",
        'softmax': "6. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ Softmax –∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å Cross Entropy Loss",
        'backprop': "7. –ú–µ—Ö–∞–Ω–∏–∑–º –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –æ—à–∏–±–∫–∏ (Backpropagation). –ü—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –≤ PyTorch. –°–ª–æ–∏ —Ñ—É–Ω–∫—Ü–∏–π –ø–æ—Ç–µ—Ä—å (Loss Functions) –≤ PyTorch",
        'dynamic': "8. –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ PyTorch",
        'sgd': "9. –°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –∏ –±–∞—Ç—á–∏ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏",
        'adagrad': "10. –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞, –º–µ—Ç–æ–¥ –∏–º–ø—É–ª—å—Å–æ–≤ –∏ –º–µ—Ç–æ–¥ –ù–µ—Å—Ç–µ—Ä–æ–≤–∞",
        'weight_init': "11. –ü—Ä–æ–±–ª–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –ò–ù–°. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ö—Å–∞–≤—å–µ",
        'dropout': "12. –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è. –ü—Ä–∏–Ω—Ü–∏–ø Dropout. –°–ª–æ–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –≤ PyTorch",
        'bn': "13. –ú–∏–Ω–∏-–±–∞—Ç—á–∏, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –º–∏–Ω–∏-–±–∞—Ç—á–∞–º –∏ —Å–ª–æ–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ PyTorch",
        'nnmodule': "14. –ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–µ —Å–µ—Ç–∏, –≥—Ä–∞—Ñ –ø–æ—Ç–æ–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ –∫–ª–∞—Å—Å nn.Module –≤ PyTorch",
        'cnn': "15. –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö, –ø—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–µ—Ç–µ–π –∏ –∏—Ö –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞",
        'conv_pool': "16. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–π –ò–ù–° –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –∏ —Å–∂–∏–º–∞—é—â–∏–µ —Å–ª–æ–∏ –≤ PyTorch",
        'cnn_ex': "17. –ü—Ä–∏–µ–º—ã –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
        'cnn_cls': "18. –°—Ö–µ–º–∞ —Ä–∞–±–æ—Ç—ã —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π —Å–µ—Ç–∏, –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å–≤–µ—Ä—Ç–∫–∏, –ø—É–ª–ª–∏–Ω–≥–∞ –∏ –æ–±—â–∏–π –≤–∏–¥ —Å–µ—Ç–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
        'kinds_rl': "19. –í–∏–¥—ã –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º",
        'rl_policy': "20. –ü–æ–¥—Ö–æ–¥—ã –∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤ RL, –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ, –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ, Q- –∏ V-—Ñ—É–Ω–∫—Ü–∏–∏, —É—Ä–∞–≤–Ω–µ–Ω–∏–µ –ë–µ–ª–ª–º–∞–Ω–∞",
        'policy_grad': "21. –ú–µ—Ç–æ–¥ Policy Gradient, —É–ª—É—á—à–µ–Ω–∏—è –∏ –º–æ–¥–µ–ª—å Actor-Critic",
        'q_learning': "22. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞ Q-Learning. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Q-—Ñ—É–Ω–∫—Ü–∏–∏. –ê–ª–≥–æ—Ä–∏—Ç–º Q-Learning",
        'vgg16': "23. –¢–∏–ø—ã –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ VGG16: –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏",
        'google_net': "24. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ GoogLeNet, –æ–ø–∏—Å–∞–Ω–∏–µ Inception module, —Å–≤–µ—Ä—Ç–∫–∞ \(1 \times 1\), DepthConcat –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏",
        'resnet': "25. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å residual connections, –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã, –Ω–∞—Ä—É—à–µ–Ω–∏–µ —Å–∏–º–º–µ—Ç—Ä–∏–∏, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏",
        'unet': "26. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ U-Net: –æ–±—â–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã, –æ–ø–∏—Å–∞–Ω–∏–µ, residual connections –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ –∑–∞–¥–∞—á–∞—Ö —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏",
        'gradcam': "27. –ú–µ—Ö–∞–Ω–∏–∑–º—ã –≤–Ω–∏–º–∞–Ω–∏—è –≤ –º–∞—à–∏–Ω–Ω–æ–º –∑—Ä–µ–Ω–∏–∏ –∏ –º–µ—Ç–æ–¥ Grad-CAM",
        'vit_swin': "28. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Transformer –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Swin Transformer",
        'decode': "29. –í–µ–∫—Ç–æ—Ä–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –≤–µ—Å–æ–≤ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–≥–æ –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞ –∏ –∑–∞–¥–∞—á–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞ –ø–æ—Å–ª–µ —Å–∂–∞—Ç–∏—è",
        'autoencoder': "30. –ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã: –ª–∏–Ω–µ–π–Ω—ã–µ, –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ, –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ, –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –∏ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è",
        'discr_models': "31. –î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω—ã–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏: –∑–∞–¥–∞—á–∏, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ",
        'gan': "32. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ GAN: –æ–±—â–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –æ–±—É—á–µ–Ω–∏–µ –∏ –ø—Ä–∏–º–µ—Ä —Å –æ–¥–Ω–æ–º–µ—Ä–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è",
        'vae': "33. –í–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä (VAE): —Ü–µ–ª–∏, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –±–∞–π–µ—Å–æ–≤—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å",
        'diffusion': "35. Denoising Diffusion Models: –ø—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã, –ø—Ä—è–º–æ–π –∏ –æ–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å—ã",
        'mlp_reg': '–†–µ–≥—Ä–µ—Å—Å–∏—è –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–∞—è. –î–∞—Ç–∞—Å–µ—Ç airline_passengers',
        'mlp_reg_2': '–†–µ–≥—Ä–µ—Å—Å–∏—è –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–∞—è. –î–∞—Ç–∞—Å–µ—Ç gold',
        'mlp_reg_3': '–†–µ–≥—Ä–µ—Å—Å–∏—è –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–∞—è. –î–∞—Ç–∞—Å–µ—Ç movie_regression',
        'mlp_class': '–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–∞—è. –î–∞—Ç–∞—Å–µ—Ç bank.csv',
        'cnn_class': '–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è CNN. –î–∞—Ç–∞—Å–µ—Ç chars',
        'cnn_class_2': '–ö–ª–∞—Å–∏—Ñ–∏–∫–∞—Ü–∏—è CNN. –î–∞—Ç–∞—Å–µ—Ç sign_language',
        'cnn_class_3': '–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è CNN. –î–∞—Ç–∞—Å–µ—Ç flower_classification'
    }
    for method, description in methods_info.items():
        help_message += f"- {method}: {description}\n"
    pyperclip.copy(help_message)


def mlp(idx: int = 0):
    """1. –ú–æ–¥–µ–ª—å –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞. –ü—Ä–æ–±–ª–µ–º–∞ –ª–∏–Ω–µ–π–Ω–æ –Ω–µ—Ä–∞–∑–¥–µ–ª–∏–º—ã—Ö –º–Ω–æ–∂–µ—Å—Ç–≤ –∏ –µ–µ —Ä–µ—à–µ–Ω–∏–µ. –õ–æ–≥–∏–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã—Ö –ò–ù–°. –õ–∏–Ω–µ–π–Ω—ã–µ —Å–ª–æ–∏ (Linear Layers) –≤ PyTorch."""
    if idx == 0:
        code = """
### 1. –ú–æ–¥–µ–ª—å –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞
–ü–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω ‚Äî —ç—Ç–æ –æ–¥–Ω–∞ –∏–∑ –ø–µ—Ä–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π. –ï–≥–æ –æ—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Å–ª–µ–¥—É—é—â–µ–º:
1. **–§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏**: –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ –≤—Ö–æ–¥–æ–≤ –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è —Å–º–µ—â–µ–Ω–∏–µ (bias). –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –Ω–µ–ª–∏–Ω–µ–π–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Å—Ç—É–ø–µ–Ω—á–∞—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è (sign).
2. **–û–±—É—á–µ–Ω–∏–µ**: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞–ª–≥–æ—Ä–∏—Ç–º –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –æ—à–∏–±–æ–∫, –æ–±–Ω–æ–≤–ª—è—è –≤–µ—Å–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –æ—à–∏–±–∫–∏ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ.

#### –§–æ—Ä–º—É–ª–∞:
$\[
y = f\left(\sum_{i=1}^n w_i x_i + b\right)
\]$
–≥–¥–µ \(w_i\) ‚Äî –≤–µ—Å–∞, \(x_i\) ‚Äî –≤—Ö–æ–¥—ã, \(b\) ‚Äî —Å–º–µ—â–µ–Ω–∏–µ, –∞ \(f\) ‚Äî —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏.

---

### 2. –ü—Ä–æ–±–ª–µ–º–∞ –ª–∏–Ω–µ–π–Ω–æ –Ω–µ—Ä–∞–∑–¥–µ–ª–∏–º—ã—Ö –º–Ω–æ–∂–µ—Å—Ç–≤
–ü–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ —Å **–ª–∏–Ω–µ–π–Ω–æ —Ä–∞–∑–¥–µ–ª–∏–º—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏**. –ù–∞–ø—Ä–∏–º–µ—Ä, –æ–Ω –Ω–µ –º–æ–∂–µ—Ç —Ä–µ—à–∏—Ç—å –∑–∞–¥–∞—á—É XOR, —Ç–∞–∫ –∫–∞–∫ –∫–ª–∞—Å—Å—ã –Ω–µ–ª—å–∑—è —Ä–∞–∑–¥–µ–ª–∏—Ç—å –ø—Ä—è–º–æ–π –ª–∏–Ω–∏–µ–π –≤ –¥–≤—É–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –≠—Ç–æ –∫–ª—é—á–µ–≤–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞.

#### –†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã:
- –î–æ–±–∞–≤–ª–µ–Ω–∏–µ **—Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤** —Å –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ReLU, sigmoid). –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –≤—Ö–æ–¥–∞–º–∏ –∏ –≤—ã—Ö–æ–¥–∞–º–∏.
- –ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (MLP) –æ–±—É—á–∞—é—Ç—Å—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º **–æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –æ—à–∏–±–∫–∏ (backpropagation)**.

---

### 3. –õ–æ–≥–∏–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã—Ö –ò–ù–°
–ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–π –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω (MLP) —Å–æ—Å—Ç–æ–∏—Ç –∏–∑:
1. **–í—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è**: –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ.
2. **–°–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤**: –≤—ã–ø–æ–ª–Ω—è—é—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º.
3. **–í—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è**: –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–ª–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏.

–ö–∞–∂–¥—ã–π —Å–ª–æ–π –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤—Ö–æ–¥—ã –≤ –≤—ã—Ö–æ–¥—ã —Å –ø–æ–º–æ—â—å—é –ª–∏–Ω–µ–π–Ω–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏:
$\[
y = f(Wx + b)
\]$

---

### 4. –õ–∏–Ω–µ–π–Ω—ã–µ —Å–ª–æ–∏ (Linear Layers) –≤ PyTorch
–í PyTorch –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –∫–ª–∞—Å—Å–∞ `torch.nn.Linear`. –û–Ω –≤—ã–ø–æ–ª–Ω—è–µ—Ç –æ–ø–µ—Ä–∞—Ü–∏—é \(y = Wx + b\).
"""
    else:
        code = """
import torch
import torch.nn as nn
import torch.optim as optim

# –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
class SimplePerceptron(nn.Module):
    def __init__(self, input_size, output_size):
        super(SimplePerceptron, self).__init__()
        self.linear = nn.Linear(input_size, output_size)  # –õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π
    
    def forward(self, x):
        return self.linear(x)  # –õ–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ

# –°–æ–∑–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ
X = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])  # XOR –¥–∞–Ω–Ω—ã–µ
y = torch.tensor([0, 1, 1, 0])  # –ú–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
input_size = 2
output_size = 1

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
model = SimplePerceptron(input_size, output_size)

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
criterion = nn.BCEWithLogitsLoss()  # –î–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
optimizer = optim.SGD(model.parameters(), lr=0.1)

# –û–±—É—á–µ–Ω–∏–µ
epochs = 1000
for epoch in range(epochs):
    # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
    outputs = model(X)
    loss = criterion(outputs.squeeze(), y.float())
    
    # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')
"""
    pyperclip.copy(code)


def activ_func(idx: int = 0):
    if idx == 0:
        code = """
### 1. –§—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏

–§—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç –ª–∏–Ω–µ–π–Ω–æ–µ –≤—ã—Ö–æ–¥–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–µ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞—Ç—å—Å—è —Å–ª–æ–∂–Ω—ã–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º. –û–Ω–∏ –≤–Ω–æ—Å—è—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å, –±–µ–∑ –∫–æ—Ç–æ—Ä–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –±—ã–ª–∞ –±—ã —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–∞ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏, –¥–∞–∂–µ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ–µ–≤.

---

### 2. –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ñ—É–Ω–∫—Ü–∏—è–º –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
1. **–ù–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å**: –¥–æ–ª–∂–Ω–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞—Ç—å –ª–∏–Ω–µ–π–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –≤—Ö–æ–¥–æ–≤ –≤ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è.
2. **–î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–æ—Å—Ç—å**: –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–º –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞.
3. **–î–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π**: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –º–æ–∂–µ—Ç —É–ª—É—á—à–∏—Ç—å —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å —Å–µ—Ç–∏.
4. **–ù–µ—É—Å–ª–æ–∂–Ω–µ–Ω–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç**: –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –∏—Å—á–µ–∑–∞—é—â–∏—Ö/–≤–∑—Ä—ã–≤–∞—é—â–∏—Ö—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.
5. **–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π**: –∏–∑–±–µ–∂–∞—Ç—å —Å–∏—Ç—É–∞—Ü–∏–π, –∫–æ–≥–¥–∞ –±–æ–ª—å—à–∞—è —á–∞—Å—Ç—å –Ω–µ–π—Ä–æ–Ω–æ–≤ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –Ω–µ–∞–∫—Ç–∏–≤–Ω–æ–π.

---

### 3. –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
1. **ReLU (Rectified Linear Unit)**:
   - –§–æ—Ä–º—É–ª–∞: \( f(x) = \max(0, x) \)
   - –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞: –ø—Ä–æ—Å—Ç–æ—Ç–∞, —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ –¥–ª—è –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π, –∏–∑–±–µ–≥–∞–µ—Ç –∏—Å—á–µ–∑–∞—é—â–∏—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.
   - –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏: "–º–µ—Ä—Ç–≤—ã–µ –Ω–µ–π—Ä–æ–Ω—ã" (–∞–∫—Ç–∏–≤–∞—Ü–∏—è –æ—Å—Ç–∞—ë—Ç—Å—è —Ä–∞–≤–Ω–æ–π 0 –¥–ª—è –≤—Å–µ—Ö –≤—Ö–æ–¥–æ–≤ \(x < 0\)).

2. **Sigmoid**:
   - –§–æ—Ä–º—É–ª–∞: \( f(x) = \frac{1}{1 + e^{-x}} \)
   - –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞: –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ \([0, 1]\), –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã—Ö –≤—ã—Ö–æ–¥–æ–≤.
   - –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏: –ø—Ä–æ–±–ª–µ–º–∞ –∏—Å—á–µ–∑–∞—é—â–∏—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.

3. **Tanh (–≥–∏–ø–µ—Ä–±–æ–ª–∏—á–µ—Å–∫–∏–π —Ç–∞–Ω–≥–µ–Ω—Å)**:
   - –§–æ—Ä–º—É–ª–∞: \( f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \)
   - –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞: –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ \([-1, 1]\), —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–∫–æ–ª–æ 0.
   - –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏: –ø–æ–¥–≤–µ—Ä–∂–µ–Ω –∏—Å—á–µ–∑–∞—é—â–∏–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º.

4. **Leaky ReLU**:
   - –§–æ—Ä–º—É–ª–∞: \( f(x) = x, \text{ –µ—Å–ª–∏ } x > 0; \ \alpha x, \text{ –µ—Å–ª–∏ } x \leq 0 \)
   - –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞: —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É "–º–µ—Ä—Ç–≤—ã—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤".
   - –ü–∞—Ä–∞–º–µ—Ç—Ä \(\alpha\) –æ–±—ã—á–Ω–æ –Ω–µ–±–æ–ª—å—à–æ–π (\(0.01\)).

5. **Softmax**:
   - –§–æ—Ä–º—É–ª–∞: \( f_i(x) = \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}} \)
   - –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–º —Å–ª–æ–µ –¥–ª—è –∑–∞–¥–∞—á –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –Ω–æ—Ä–º–∞–ª–∏–∑—É—è –≤—ã—Ö–æ–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏.

6. **Swish**:
   - –§–æ—Ä–º—É–ª–∞: \( f(x) = x \cdot \text{sigmoid}(x) \)
   - –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞: –ø–ª–∞–≤–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è, —É–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–¥–∞—á.
"""
    else:
        code = """
### 4. –°–ª–æ–∏ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –≤ PyTorch
–í PyTorch —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏–∑—É—é—Ç—Å—è –≤ –º–æ–¥—É–ª–µ `torch.nn` –ª–∏–±–æ –∫–∞–∫ –ø—Ä–æ—Å—Ç—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (`torch.nn.functional`).

#### –ü—Ä–∏–º–µ—Ä—ã –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏:
import torch
import torch.nn as nn
import torch.nn.functional as F

# –ü—Ä–∏–º–µ—Ä—ã —Ñ—É–Ω–∫—Ü–∏–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
x = torch.tensor([-1.0, 0.0, 1.0])

# ReLU
relu = nn.ReLU()
print("ReLU:", relu(x))

# Sigmoid
sigmoid = nn.Sigmoid()
print("Sigmoid:", sigmoid(x))

# Tanh
tanh = nn.Tanh()
print("Tanh:", tanh(x))

# Leaky ReLU
leaky_relu = nn.LeakyReLU(0.01)
print("Leaky ReLU:", leaky_relu(x))

# Softmax (–¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)
softmax = nn.Softmax(dim=0)
print("Softmax:", softmax(x))

### 5. –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –≤ –º–æ–¥–µ–ª–∏
class NeuralNet(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(NeuralNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)  # –õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π
        self.relu = nn.ReLU()  # –ù–µ–ª–∏–Ω–µ–π–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è
        self.fc2 = nn.Linear(hidden_size, output_size)  # –õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)  # –ü—Ä–∏–º–µ–Ω—è–µ–º ReLU
        x = self.fc2(x)
        return x

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
model = NeuralNet(input_size=3, hidden_size=5, output_size=2)
sample_input = torch.randn(1, 3)
output = model(sample_input)
print("Output:", output)
"""
    pyperclip.copy(code)


def dl_spring(idx: int = 0):
    code = """
1.
"–í—Ç–æ—Ä–∞—è –≤–µ—Å–Ω–∞" –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –ø–µ—Ä–∏–æ–¥—É —Å 2010-—Ö –≥–æ–¥–æ–≤, –∫–æ–≥–¥–∞ –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞—á–∞–ª–æ –∞–∫—Ç–∏–≤–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –≤ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö, –∏–Ω–¥—É—Å—Ç—Ä–∏–∏ –∏ —Ä–µ—à–µ–Ω–∏–∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á. –≠—Ç–æ—Ç –ø–æ–¥—ä–µ–º –±—ã–ª –æ–±—É—Å–ª–æ–≤–ª–µ–Ω —É—Å–ø–µ—Ö–∞–º–∏ –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –≤ —Ç–∞–∫–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö, –∫–∞–∫ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP), —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –∏ –¥—Ä—É–≥–∏–µ
–ö–ª—é—á–µ–≤—ã–µ —ç—Ç–∞–ø—ã:
–í 2006 –≥–æ–¥—É –î–∂–µ—Ñ—Ñ—Ä–∏ –•–∏–Ω—Ç–æ–Ω –∏ –µ–≥–æ –∫–æ–º–∞–Ω–¥–∞ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –≥–ª—É–±–æ–∫–∏–µ —Å–µ—Ç–∏ —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º (Deep Belief Networks).
–ü—Ä–æ—Ä—ã–≤ –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏ –ø—Ä–æ–∏–∑–æ—à–µ–ª –≤ 2012 –≥–æ–¥—É, –∫–æ–≥–¥–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ AlexNet (–ö—Ä–∏–∑–µ–≤—Å–∫–∏–π, –°—É—Ü–∫–µ–≤–µ—Ä, –•–∏–Ω—Ç–æ–Ω) –≤—ã–∏–≥—Ä–∞–ª–∞ ImageNet —Å –æ–≥—Ä–æ–º–Ω—ã–º –æ—Ç—Ä—ã–≤–æ–º.
–†–∞–∑–≤–∏—Ç–∏–µ –¥—Ä—É–≥–∏—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä, —Ç–∞–∫–∏—Ö –∫–∞–∫ VGG, ResNet, Transformer, —É—Å–∫–æ—Ä–∏–ª–æ —Ä–æ—Å—Ç –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.

2. –ü—Ä–∏—á–∏–Ω—ã ¬´–≤—Ç–æ—Ä–æ–π –≤–µ—Å–Ω—ã¬ª –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
1. –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –º–æ—â–Ω–æ—Å—Ç–µ–π
–ü–æ—è–≤–ª–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ (GPU) –∏ –∏—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.
–†–∞–∑–≤–∏—Ç–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ TPU (Tensor Processing Units).
2. –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö
–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è (—Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏, —Å–µ–Ω—Å–æ—Ä—ã, –∫–∞–º–µ—Ä—ã) –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –æ–≥—Ä–æ–º–Ω—ã–µ –æ–±—ä–µ–º—ã –¥–∞–Ω–Ω—ã—Ö.
–†–∞–∑–≤–∏—Ç–∏–µ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ ImageNet, COCO, Wikipedia –∏ –¥—Ä—É–≥–∏—Ö.
3. –£–ª—É—á—à–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM), Transformer.
–ú–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ Adam, RMSProp, SGD —Å –∏–º–ø—É–ª—å—Å–æ–º, –ø–æ–∑–≤–æ–ª–∏–ª–∏ —É–ª—É—á—à–∏—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è.
4. –ü—Ä–æ–≥—Ä–µ—Å—Å –≤ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π
–ú–µ—Ç–æ–¥—ã —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ Dropout, Batch Normalization, –ø–æ–∑–≤–æ–ª–∏–ª–∏ –æ–±—É—á–∞—Ç—å –≥–ª—É–±–æ–∫–∏–µ –º–æ–¥–µ–ª–∏ –±–µ–∑ —Å–∏–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è.
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Ö–Ω–∏–∫–∏ transfer learning.
5. –†–∞–∑–≤–∏—Ç–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫
–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ TensorFlow, PyTorch, Keras, Theano, —É–ø—Ä–æ—Å—Ç–∏–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É –º–æ–¥–µ–ª–µ–π –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.
6. –ü—Ä–∏–≤–ª–µ—á–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è —Å–æ —Å—Ç–æ—Ä–æ–Ω—ã –∏–Ω–¥—É—Å—Ç—Ä–∏–∏
–ö–æ–º–ø–∞–Ω–∏–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ Google, Facebook, OpenAI, –∞–∫—Ç–∏–≤–Ω–æ –∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–∏ –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏ –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.
–£—Å–ø–µ—à–Ω—ã–µ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ –≥–æ–ª–æ—Å–æ–≤—ã–µ –ø–æ–º–æ—â–Ω–∏–∫–∏ (Siri, Alexa), —Å–∏—Å—Ç–µ–º—ã —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π (Netflix, YouTube).
"""
    pyperclip.copy(code)


def grad(idx: int = 0):
    if idx == 0:
        code = """
–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –∏–≥—Ä–∞—é—Ç –∫–ª—é—á–µ–≤—É—é —Ä–æ–ª—å –≤ –æ–±—É—á–µ–Ω–∏–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–≤–µ—Å–æ–≤) –º–æ–¥–µ–ª–∏. –û—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ ‚Äî –Ω–∞–π—Ç–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏ –≤–µ–ª–∏—á–∏–Ω—É –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤–µ—Å–æ–≤, —á—Ç–æ–±—ã –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å.

---

### 1. –ì—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –º–Ω–æ–≥–∏—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö

–ì—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ \(f(x_1, x_2, \ldots, x_n)\) ‚Äî —ç—Ç–æ –≤–µ–∫—Ç–æ—Ä —á–∞—Å—Ç–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –ø–æ –∫–∞–∂–¥–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:
\[
\text{grad} \, f = \left[ \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n} \right]
\]

–ì—Ä–∞–¥–∏–µ–Ω—Ç —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞–∏–±–æ–ª—å—à–µ–≥–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏. –î–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Ñ—É–Ω–∫—Ü–∏–∏ (–≤ –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ, —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å) –≤–µ—Å–∞ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –≤ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞.

---

### 2. –õ–æ–≥–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏

1. **–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ (forward pass)**:
   - –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ö–æ–¥—è—Ç —á–µ—Ä–µ–∑ —Å–ª–æ–∏ —Å–µ—Ç–∏, –∏ –Ω–∞ –∫–∞–∂–¥–æ–º —Å–ª–æ–µ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –ª–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏.
   - –í –∫–æ–Ω—Ü–µ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å \(L\).

2. **–û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ (backpropagation)**:
   - –í—ã—á–∏—Å–ª—è—é—Ç—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å \(L\) –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º —Å–µ—Ç–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–∞–≤–∏–ª–∞ —Ü–µ–ø–æ—á–∫–∏.
   - –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è —Å —É—á–µ—Ç–æ–º –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.

3. **–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤**:
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ –∏–ª–∏ –µ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç—ã.

---

### 3. –ú–µ—Ç–æ–¥—ã –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞

#### 1. **–ß–∏—Å–ª–µ–Ω–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç**:
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–æ–π —Å –ø–æ–º–æ—â—å—é –∫–æ–Ω–µ—á–Ω—ã—Ö —Ä–∞–∑–Ω–æ—Å—Ç–µ–π:
     \[
     \frac{\partial f}{\partial x} \approx \frac{f(x + \epsilon) - f(x - \epsilon)}{2\epsilon}
     \]
   - –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞: –ø—Ä–æ—Å—Ç –≤ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏.
   - –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏: –º–µ–¥–ª–µ–Ω–Ω—ã–π –∏ –ø–æ–¥–≤–µ—Ä–∂–µ–Ω —á–∏—Å–ª–µ–Ω–Ω—ã–º –æ—à–∏–±–∫–∞–º.

#### 2. **–°–∏–º–≤–æ–ª—å–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç**:
   - –í—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏–º–≤–æ–ª—å–Ω–æ–π –∞–ª–≥–µ–±—Ä—ã.
   - –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞: —Ç–æ—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ.
   - –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏: —Å–ª–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –∫ —Å–ª–æ–∂–Ω—ã–º —Ñ—É–Ω–∫—Ü–∏—è–º.

#### 3. **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ (autograd)**:
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥—Ä–∞—Ñ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.
   - –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞—Ö –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Ç–∞–∫–∏—Ö –∫–∞–∫ PyTorch –∏ TensorFlow.
   - –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞: –≤—ã—á–∏—Å–ª—è–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏ —Ç–æ—á–Ω–æ.

---

### 4. –ü—Ä–æ–±–ª–µ–º—ã –ø–æ–∏—Å–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞

1. **–ò—Å—á–µ–∑–∞—é—â–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã**:
   - –í –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç—è—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–∞ —Ä–∞–Ω–Ω–∏—Ö —Å–ª–æ—è—Ö —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –æ—á–µ–Ω—å –º–∞–ª—ã–º–∏, —á—Ç–æ –∑–∞–º–µ–¥–ª—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ.
   - –†–µ—à–µ–Ω–∏–µ: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, —Ç–∞–∫–∏—Ö –∫–∞–∫ ReLU, –∏ –º–µ—Ç–æ–¥–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ Batch Normalization.

2. **–í–∑—Ä—ã–≤–∞—é—â–∏–µ—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã**:
   - –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–º–∏, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è.
   - –†–µ—à–µ–Ω–∏–µ: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –∫–ª–∏–ø–ø–∏–Ω–≥–∞ (gradient clipping).

3. **–ß–∏—Å–ª–µ–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏**:
   - –ü—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–º–∏ –∏–ª–∏ –º–∞–ª—ã–º–∏ —á–∏—Å–ª–∞–º–∏ –≤–æ–∑–Ω–∏–∫–∞—é—Ç –æ—à–∏–±–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏.
   - –†–µ—à–µ–Ω–∏–µ: –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è.
"""

    else:
        code = """
import torch

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é
x = torch.tensor(2.0, requires_grad=True)  # –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è, –¥–ª—è –∫–æ—Ç–æ—Ä–æ–π –Ω—É–∂–µ–Ω –≥—Ä–∞–¥–∏–µ–Ω—Ç
y = x**2 + 3*x + 5  # –§—É–Ω–∫—Ü–∏—è –æ—Ç x

# –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
print("Function value (y):", y.item())

# –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
y.backward()  # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞
print("Gradient (dy/dx):", x.grad.item())

import torch
import torch.nn as nn

# –î–∞–Ω–Ω—ã–µ
X = torch.tensor([[1.0], [2.0], [3.0]])
y = torch.tensor([[2.0], [4.0], [6.0]])

# –ú–æ–¥–µ–ª—å
model = nn.Linear(1, 1)  # –õ–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å y = wx + b
criterion = nn.MSELoss()  # –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (MSE)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫

# –û–±—É—á–µ–Ω–∏–µ
for epoch in range(100):
    # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
    predictions = model(X)
    loss = criterion(predictions, y)
    
    # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
    optimizer.zero_grad()
    loss.backward()  # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤—ã—á–∏—Å–ª–µ–Ω—ã
    optimizer.step()  # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    
    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')
"""

    pyperclip.copy(code)


def cross_val(idx: int = 0):
    if idx == 0:
        code = """
1. –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç —É–º–µ–Ω—å—à–∏—Ç—å —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ–ª—É—á–∏—Ç—å –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –ò–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ä–∞–∑–±–∏–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞—Å—Ç–µ–π (—Ñ–æ–ª–¥–æ–≤) –∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞—Ö –¥–∞–Ω–Ω—ã—Ö.

–ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥: 

k-fold –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è.
–î–∞–Ω–Ω—ã–µ –¥–µ–ª—è—Ç—Å—è –Ω–∞ 
k —Ñ–æ–ª–¥–æ–≤.
–í –∫–∞–∂–¥–æ–º —Ü–∏–∫–ª–µ –æ–¥–∏–Ω —Ñ–æ–ª–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ ‚Äî –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
–ò—Ç–æ–≥–æ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∫–∞–∫ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø–æ –≤—Å–µ–º —Ñ–æ–ª–¥–∞–º.
–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–∞, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ –º–∞–ª—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö.
–£–º–µ–Ω—å—à–µ–Ω–∏–µ –≤–ª–∏—è–Ω–∏—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.


5. –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è. –í—ã–±–æ—Ä–∫–∏ train, validation, test. –ü—Ä–æ–±–ª–µ–º–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞.
1. –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç —É–º–µ–Ω—å—à–∏—Ç—å —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ–ª—É—á–∏—Ç—å –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –ò–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ä–∞–∑–±–∏–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞—Å—Ç–µ–π (—Ñ–æ–ª–¥–æ–≤) –∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞—Ö –¥–∞–Ω–Ω—ã—Ö.

–ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥: 
ùëò
k-fold –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è.
–î–∞–Ω–Ω—ã–µ –¥–µ–ª—è—Ç—Å—è –Ω–∞ 
ùëò
k —Ñ–æ–ª–¥–æ–≤.
–í –∫–∞–∂–¥–æ–º —Ü–∏–∫–ª–µ –æ–¥–∏–Ω —Ñ–æ–ª–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ ‚Äî –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
–ò—Ç–æ–≥–æ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∫–∞–∫ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø–æ –≤—Å–µ–º —Ñ–æ–ª–¥–∞–º.
–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–∞, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ –º–∞–ª—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö.
–£–º–µ–Ω—å—à–µ–Ω–∏–µ –≤–ª–∏—è–Ω–∏—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.
–ü—Ä–∏–º–µ—Ä –∫–æ–¥–∞:
python
Copy
Edit
from sklearn.model_selection import KFold
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import numpy as np

# –î–∞–Ω–Ω—ã–µ
X = np.random.rand(100, 1)
y = 3 * X.squeeze() + np.random.rand(100)

kf = KFold(n_splits=5)
model = LinearRegression()

for train_index, val_index in kf.split(X):
    X_train, X_val = X[train_index], X[val_index]
    y_train, y_val = y[train_index], y[val_index]
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model.fit(X_train, y_train)
    
    # –û—Ü–µ–Ω–∫–∞
    predictions = model.predict(X_val)
    print("MSE:", mean_squared_error(y_val, predictions))

2. –í—ã–±–æ—Ä–∫–∏ train, validation, test
Train (–æ–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞): –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –ú–æ–¥–µ–ª—å –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –Ω–∞ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö.

Validation (–≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞): –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. –ü–æ–º–æ–≥–∞–µ—Ç –ø–æ–¥–æ–±—Ä–∞—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è.

Test (—Ç–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞): –û—Ç–¥–µ–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π —Ç–æ–ª—å–∫–æ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏. –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ –Ω–µ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –Ω–∞ —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ –ø–æ–¥–±–æ—Ä–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.

3. –ü—Ä–æ–±–ª–µ–º–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ (overfitting) –≤–æ–∑–Ω–∏–∫–∞–µ—Ç, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å —Å–ª–∏—à–∫–æ–º —Ö–æ—Ä–æ—à–æ –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –ø–æ–¥ –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ, –Ω–æ –ø–ª–æ—Ö–æ –æ–±–æ–±—â–∞–µ—Ç –∏—Ö –Ω–∞ –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ. –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è:

–í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
–ù–∏–∑–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö/—Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
–ü—Ä–∏—á–∏–Ω—ã:
–°–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω–∞—è –º–æ–¥–µ–ª—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤).
–ù–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
–î–æ–ª–≥–æ–µ –æ–±—É—á–µ–Ω–∏–µ.
–°–ø–æ—Å–æ–±—ã –±–æ—Ä—å–±—ã:
–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö: —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –æ–±—ä–µ–º–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏.
–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è: –º–µ—Ç–æ–¥—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ L1/L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –∏–ª–∏ Dropout.
–ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–∞ —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ (early stopping).

4. –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ (Early Stopping)
–†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –û–±—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è, –µ—Å–ª–∏ –æ—à–∏–±–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ –ø–µ—Ä–µ—Å—Ç–∞–µ—Ç —É–º–µ–Ω—å—à–∞—Ç—å—Å—è –≤ —Ç–µ—á–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç–ø–æ—Ö.

–ê–ª–≥–æ—Ä–∏—Ç–º:
–°–ª–µ–¥–∏—Ç–µ –∑–∞ –º–µ—Ç—Ä–∏–∫–æ–π –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, loss –∏–ª–∏ accuracy).
–ï—Å–ª–∏ –º–µ—Ç—Ä–∏–∫–∞ –ø–µ—Ä–µ—Å—Ç–∞–ª–∞ —É–ª—É—á—à–∞—Ç—å—Å—è (–ø–æ—Ä–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö ‚Äî patience), –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ.
"""

    else:
        code = """
import torch
import torch.nn as nn
import torch.optim as optim

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
X = torch.randn(100, 10)
y = torch.randn(100, 1)

# –ú–æ–¥–µ–ª—å
model = nn.Sequential(
    nn.Linear(10, 50),
    nn.ReLU(),
    nn.Linear(50, 1)
)

criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# Early stopping
patience = 5
best_loss = float('inf')
trigger_times = 0

for epoch in range(100):
    # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
    predictions = model(X)
    loss = criterion(predictions, y)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è
    if loss.item() < best_loss:
        best_loss = loss.item()
        trigger_times = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫
    else:
        trigger_times += 1
        if trigger_times >= patience:
            print(f"Early stopping at epoch {epoch}")
            break
    
    # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    print(f"Epoch {epoch}, Loss: {loss.item():.4f}")
"""

    pyperclip.copy(code)


def softmax(idx: int = 0):
    if idx == 0:
        code = """
### 6. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ Softmax –∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å Cross Entropy Loss

---

### 1. **–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ Softmax**

**Softmax** ‚Äî —ç—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤–µ–∫—Ç–æ—Ä –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö —á–∏—Å–µ–ª –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ. –û–Ω–∞ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–º —Å–ª–æ–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –¥–ª—è –∑–∞–¥–∞—á –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.

#### –§–æ—Ä–º—É–ª–∞ Softmax:
–î–ª—è –≤–µ–∫—Ç–æ—Ä–∞ –≤—Ö–æ–¥–æ–≤ \(z = [z_1, z_2, \ldots, z_n]\):
\[
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^n e^{z_j}}
\]
–≥–¥–µ \(e^{z_i}\) ‚Äî —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ, –∞ –∑–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è, —á—Ç–æ–±—ã –∏—Ö —Å—É–º–º–∞ –±—ã–ª–∞ —Ä–∞–≤–Ω–∞ \(1\).

#### –°–≤–æ–π—Å—Ç–≤–∞ Softmax:
1. –ó–Ω–∞—á–µ–Ω–∏—è –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ \([0, 1]\).
2. –°—É–º–º–∞ –≤—Å–µ—Ö –≤—ã—Ö–æ–¥–æ–≤ —Ä–∞–≤–Ω–∞ \(1\), —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∏—Ö –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–º–∏ –∫–∞–∫ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏.

---

### 2. **–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å Cross Entropy Loss**

**–ö—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏—è (Cross Entropy)** –∏–∑–º–µ—Ä—è–µ—Ç —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–º–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏:
1. –ò—Å—Ç–∏–Ω–Ω—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –º–µ—Ç–æ–∫ –∫–ª–∞—Å—Å–æ–≤ (\(y\), –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –∫–∞–∫ one-hot encoding).
2. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º –º–æ–¥–µ–ª—å—é (\(p\)).

#### –§–æ—Ä–º—É–ª–∞:
–î–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞:
\[
\text{Loss} = -\sum_{i=1}^n y_i \log(p_i)
\]
–≥–¥–µ \(y_i\) ‚Äî –∏—Å—Ç–∏–Ω–Ω–∞—è –º–µ—Ç–∫–∞ (–æ–±—ã—á–Ω–æ \(1\) –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ –∏ \(0\) –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö), –∞ \(p_i\) ‚Äî –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è –∫–ª–∞—Å—Å–∞ \(i\).

–ï—Å–ª–∏ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –∏—Å—Ç–∏–Ω–Ω—É—é –º–µ—Ç–∫—É –∫–∞–∫ \(y_c = 1\) —Ç–æ–ª—å–∫–æ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ \(c\), —Ç–æ:
\[
\text{Loss} = -\log(p_c)
\]

---

### 3. **Softmax + Cross Entropy**

–ß–∞—Å—Ç–æ Softmax –∏ Cross Entropy –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –≤ –æ–¥–Ω—É –æ–ø–µ—Ä–∞—Ü–∏—é:
1. –ù–∞ —ç—Ç–∞–ø–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø–æ—Ç–µ—Ä—å –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –ª–æ–≥–∞—Ä–∏—Ñ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π (–ø–æ—Å–ª–µ Softmax) –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏–∏.
2. –≠—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —ç—Ç–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π —É–ª—É—á—à–∞–µ—Ç —á–∏—Å–ª–µ–Ω–Ω—É—é —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å.

### 5. **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤**

1. **Softmax**:
   –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ª–æ–≥–∏—Ç—ã (–Ω–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è) –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:
   \[
   \text{logits} = [2.0, 1.0, 0.1] \implies \text{probabilities} = [0.6590, 0.2424, 0.0986]
   \]

2. **Cross Entropy Loss**:
   –ü–æ—Ç–µ—Ä—è –¥–ª—è –∏—Å—Ç–∏–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ \(0\) –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∫–∞–∫:
   \[
   \text{Loss} = -\log(0.6590) \approx 0.4161
   \]

"""

    else:
        code = """
#### –ü—Ä–∏–º–µ—Ä:
import torch
import torch.nn as nn

# –ú–æ–¥–µ–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–ª–æ–≥–∏—Ç—ã, –¥–æ Softmax)
logits = torch.tensor([[2.0, 1.0, 0.1]])  # –í—ã—Ö–æ–¥—ã –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ (3 –∫–ª–∞—Å—Å–∞)
labels = torch.tensor([0])  # –ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å: –∫–ª–∞—Å—Å 0

# 1. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ Softmax
softmax = nn.Softmax(dim=1)
probabilities = softmax(logits)
print("Probabilities:", probabilities)

# 2. –ö—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏–π–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
criterion = nn.CrossEntropyLoss()
loss = criterion(logits, labels)  # logits –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é
print("Cross Entropy Loss:", loss.item())

# –ü—Ä–∏–º–µ—Ä –º–æ–¥–µ–ª–∏ —Å CrossEntropyLoss
model = nn.Linear(10, 3)  # 10 –≤—Ö–æ–¥–æ–≤, 3 –∫–ª–∞—Å—Å–∞
criterion = nn.CrossEntropyLoss()

# –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
inputs = torch.randn(5, 10)  # 5 –ø—Ä–∏–º–µ—Ä–æ–≤, –∫–∞–∂–¥—ã–π —Å 10 –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
labels = torch.tensor([0, 1, 2, 1, 0])  # –ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤

# –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
outputs = model(inputs)

# –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å
loss = criterion(outputs, labels)
print("Loss:", loss.item())
"""

    pyperclip.copy(code)


def backprop(idx: int = 0):
    if idx == 0:
        code = """
### 1. **–ú–µ—Ö–∞–Ω–∏–∑–º –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –æ—à–∏–±–∫–∏ (Backpropagation)**

–û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏. –≠—Ç–æ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ **–ø—Ä–∞–≤–∏–ª–µ —Ü–µ–ø–æ—á–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö**.

#### –≠—Ç–∞–ø—ã –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è:
1. **–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ (Forward Pass)**:
   - –î–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ö–æ–¥—è—Ç —á–µ—Ä–µ–∑ —Å–µ—Ç—å, –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –≤—ã—Ö–æ–¥—ã –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è –∏ –∏—Ç–æ–≥–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å.

2. **–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—à–∏–±–∫–∏**:
   - –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏, –∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å.

3. **–û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ (Backward Pass)**:
   - –í—ã—á–∏—Å–ª—è—é—Ç—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–∞–≤–∏–ª–æ —Ü–µ–ø–æ—á–∫–∏. –≠—Ç–æ —à–∞–≥ backward propagation.

4. **–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤**:
   - –ò—Å–ø–æ–ª—å–∑—É—è –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–µ—Ç–∏ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, —Ç–∞–∫–∏—Ö –∫–∞–∫ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫.

---

### 2. **–ü—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –≤ PyTorch**

–¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤ PyTorch —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö —à–∞–≥–æ–≤:

1. **–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏, —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞**.
2. **–¶–∏–∫–ª –ø–æ —ç–ø–æ—Ö–∞–º**:
   - –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
   - –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å.
   - –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ: –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é `loss.backward()`.
   - –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (`optimizer.step()`).
3. **–û—Ü–µ–Ω–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ** (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ).

#### –ü—Ä–∏–º–µ—Ä –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ü–∏–∫–ª–∞:
```python
import torch
import torch.nn as nn
import torch.optim as optim

# –î–∞–Ω–Ω—ã–µ
X = torch.tensor([[1.0], [2.0], [3.0]])
y = torch.tensor([[2.0], [4.0], [6.0]])

# –ú–æ–¥–µ–ª—å
model = nn.Linear(1, 1)

# –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# –û–±—É—á–µ–Ω–∏–µ
epochs = 100
for epoch in range(epochs):
    # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
    predictions = model(X)
    loss = criterion(predictions, y)

    # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
    optimizer.zero_grad()  # –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
    loss.backward()  # –í—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
    optimizer.step()  # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

    # –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∫–∞–∂–¥—ã–µ 10 —ç–ø–æ—Ö
    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}")
```

---

### 3. **–°–ª–æ–∏ —Ñ—É–Ω–∫—Ü–∏–π –ø–æ—Ç–µ—Ä—å (Loss Functions) –≤ PyTorch**

–í PyTorch —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –≤ –º–æ–¥—É–ª–µ `torch.nn`. –û–Ω–∏ –∏–∑–º–µ—Ä—è—é—Ç –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ –∏ –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏.

#### –ß–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å:

1. **MSELoss (Mean Squared Error)**:
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –∑–∞–¥–∞—á —Ä–µ–≥—Ä–µ—Å—Å–∏–∏.
   - –§–æ—Ä–º—É–ª–∞:
     \[
     \text{Loss} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2
     \]
   ```python
   criterion = nn.MSELoss()
   ```

2. **CrossEntropyLoss**:
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.
   - –í–∫–ª—é—á–∞–µ—Ç Softmax –≤–Ω—É—Ç—Ä–∏.
   ```python
   criterion = nn.CrossEntropyLoss()
   ```

3. **BCELoss (Binary Cross-Entropy)**:
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.
   - –¢—Ä–µ–±—É–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è `Sigmoid`.
   ```python
   criterion = nn.BCELoss()
   ```

4. **BCEWithLogitsLoss**:
   - –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç `Sigmoid` –∏ `BCELoss` –≤ –æ–¥–Ω—É —Ñ—É–Ω–∫—Ü–∏—é.
   ```python
   criterion = nn.BCEWithLogitsLoss()
   ```

5. **SmoothL1Loss**:
   - –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–µ–≥—Ä–µ—Å—Å–∏–∏, –≥–¥–µ –Ω—É–∂–Ω–æ –±—ã—Ç—å –º–µ–Ω–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º –∫ –≤—ã–±—Ä–æ—Å–∞–º.
   ```python
   criterion = nn.SmoothL1Loss()
   ```

---

### –ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:
1. **–û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏** –≤—ã—á–∏—Å–ª—è–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ –ø—Ä–∞–≤–∏–ª–æ —Ü–µ–ø–æ—á–∫–∏.
2. –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –≤–∫–ª—é—á–∞–µ—Ç —ç—Ç–∞–ø—ã –ø—Ä—è–º–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞, –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø–æ—Ç–µ—Ä—å, –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
3. **–§—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å** –≤ PyTorch (–Ω–∞–ø—Ä–∏–º–µ—Ä, MSE, CrossEntropy) —è–≤–ª—è—é—Ç—Å—è —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–º —ç–ª–µ–º–µ–Ω—Ç–æ–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏.
"""

    else:
        code = """
# –î–∞–Ω–Ω—ã–µ
inputs = torch.randn(3, 5)  # 3 –ø—Ä–∏–º–µ—Ä–∞, 5 –∫–ª–∞—Å—Å–æ–≤
labels = torch.tensor([0, 2, 1])  # –ò—Å—Ç–∏–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã

# –ú–æ–¥–µ–ª—å
model = nn.Linear(5, 5)

# CrossEntropyLoss (–¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)
criterion = nn.CrossEntropyLoss()

# –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
outputs = model(inputs)

# –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å
loss = criterion(outputs, labels)
print("Cross Entropy Loss:", loss.item())
```

#### –ü—Ä–∏–º–µ—Ä: –ó–∞–¥–∞—á–∞ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
```python
# –î–∞–Ω–Ω—ã–µ
inputs = torch.tensor([[1.0], [2.0], [3.0]])
labels = torch.tensor([[2.0], [4.0], [6.0]])

# –ú–æ–¥–µ–ª—å
model = nn.Linear(1, 1)

# MSELoss (–¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏)
criterion = nn.MSELoss()

# –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
outputs = model(inputs)

# –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å
loss = criterion(outputs, labels)
print("MSE Loss:", loss.item())
"""

    pyperclip.copy(code)


def dynamic(idx: int = 0):
    if idx == 0:
        code = """
### 8. –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ PyTorch

---

### 1. **–î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ**

–î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ ‚Äî —ç—Ç–æ –ø–æ–¥—Ö–æ–¥, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä—É–µ–º–æ–π —Å–∏—Å—Ç–µ–º—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –º–æ–¥–µ–ª–∏) –º–æ–∂–Ω–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞—Ç—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π, –≥–¥–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è (–≤–∫–ª—é—á–∞—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å, –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∏ —Ç. –¥.) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –≤ –≤–∏–¥–µ –≥—Ä–∞—Ñ–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.

#### –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤**: –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –≤ —Å–∏—Å—Ç–µ–º–µ.
- **–ì–∏–±–∫–æ—Å—Ç—å –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –º–æ–¥–µ–ª–µ–π**: –ª—é–±—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏, –µ—Å–ª–∏ –æ–Ω–∏ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã, –º–æ–≥—É—Ç –±—ã—Ç—å –≤–∫–ª—é—á–µ–Ω—ã –≤ –≥—Ä–∞—Ñ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.

---

### 2. **–û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏**

–û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ (backpropagation) ‚Äî —ç—Ç–æ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∞–≤–∏–ª–∞ —Ü–µ–ø–æ—á–∫–∏ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö —Å–ª–æ–∂–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π:
1. –ù–∞ **–ø—Ä—è–º–æ–º –ø—Ä–æ—Ö–æ–¥–µ** –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ—Ç–µ—Ä—å).
2. –ù–∞ **–æ–±—Ä–∞—Ç–Ω–æ–º –ø—Ä–æ—Ö–æ–¥–µ** –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤—ã—Ö–æ–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –Ω–∞–∑–∞–¥ —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ, —á—Ç–æ–±—ã –≤—ã—á–∏—Å–ª–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.

---

### 3. **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ PyTorch**

PyTorch –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ—â–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –º–æ–¥—É–ª—å **`torch.autograd`**. –û–Ω —Å—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏, —Ç–æ –µ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞.

#### –ü—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã:
1. **–û–±—Ä–∞—Ç–Ω—ã–π –≥—Ä–∞—Ñ**:
   - PyTorch –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, —Å–≤—è–∑—ã–≤–∞—è –æ–ø–µ—Ä–∞—Ü–∏–∏ –Ω–∞–¥ —Ç–µ–Ω–∑–æ—Ä–∞–º–∏.
2. **`requires_grad`**:
   - –ï—Å–ª–∏ —Ç–µ–Ω–∑–æ—Ä —Å–æ–∑–¥–∞–Ω —Å `requires_grad=True`, PyTorch –±—É–¥–µ—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å —ç—Ç–∏–º —Ç–µ–Ω–∑–æ—Ä–æ–º.
3. **`backward()`**:
   - –í—ã–∑—ã–≤–∞–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —É—á–∞—Å—Ç–≤—É—é—â–∏—Ö –≤ –≥—Ä–∞—Ñ–µ.

---

### 4. **–ü—Ä–∏–º–µ—Ä –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è**

#### –ü—Ä–∏–º–µ—Ä 1: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –≤—Ä—É—á–Ω—É—é
```python
import torch

# –°–æ–∑–¥–∞–µ–º —Ç–µ–Ω–∑–æ—Ä —Å –≤–∫–ª—é—á–µ–Ω–Ω—ã–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ–º
x = torch.tensor(2.0, requires_grad=True)

# –í—ã—á–∏—Å–ª—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é
y = x**2 + 3*x + 5  # y = x^2 + 3x + 5

# –í—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç
y.backward()  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ
print(f"Gradient (dy/dx): {x.grad}")  # dy/dx = 2x + 3
```

–í—ã–≤–æ–¥:
```
Gradient (dy/dx): 7.0
```
---

### 5. **–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ –æ–±—É—á–µ–Ω–∏–∏ –ò–ù–°**

#### –ö–ª—é—á–µ–≤—ã–µ —ç—Ç–∞–ø—ã:
1. **–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥**:
   - –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ö–æ–¥—è—Ç —á–µ—Ä–µ–∑ –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å.
   - –í—ã—Ö–æ–¥ —Å–µ—Ç–∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è —Å –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å.

2. **–û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ**:
   - –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ —Å–µ—Ç–∏.

3. **–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤**:
   - –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ SGD –∏–ª–∏ Adam.

---

### 6. **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è –≤ PyTorch**
1. **–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞**:
   - –ì—Ä–∞—Ñ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π —Å—Ç—Ä–æ–∏—Ç—Å—è "–Ω–∞ –ª–µ—Ç—É" –∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –∫–æ–¥–∞.

2. **–ì–∏–±–∫–æ—Å—Ç—å**:
   - –õ–µ–≥–∫–æ –∏–∑–º–µ–Ω—è—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–µ–π –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏.

3. **–ß–∏—Å–ª–µ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å**:
   - –ò–∑–±–µ–≥–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å —Ä—É—á–Ω—ã–º –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.

---

### –í—ã–≤–æ–¥
- **–î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ** –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—É—á–∞—Ç—å –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏.
- PyTorch –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ—â–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ `autograd`.
- –û—Å–Ω–æ–≤–Ω—ã–µ —ç—Ç–∞–ø—ã –æ–±—É—á–µ–Ω–∏—è ‚Äî —ç—Ç–æ –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥, –æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
"""

    else:
        code = """ 
#### –ü—Ä–∏–º–µ—Ä —Å–ª–æ–∂–Ω–æ–π —Å–µ—Ç–∏ —Å PyTorch:
class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(2, 10)  # –í—Ö–æ–¥ -> —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π
        self.relu = nn.ReLU()       # –ê–∫—Ç–∏–≤–∞—Ü–∏—è
        self.fc2 = nn.Linear(10, 1)  # –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π -> –≤—ã—Ö–æ–¥

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# –î–∞–Ω–Ω—ã–µ
inputs = torch.tensor([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])
targets = torch.tensor([[1.0], [2.0], [3.0]])

# –ú–æ–¥–µ–ª—å, —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
model = SimpleNet()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# –û–±—É—á–µ–Ω–∏–µ
for epoch in range(50):
    predictions = model(inputs)
    loss = criterion(predictions, targets)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch+1}/50], Loss: {loss.item():.4f}")
"""

    pyperclip.copy(code)


def sgd(idx: int = 0):
    if idx == 0:
        code = """
### 9. –°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –∏ –±–∞—Ç—á–∏ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏

---

### 1. **–°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ (SGD)**

**–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫** ‚Äî —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø—É—Ç—ë–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –≤ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –µ—ë –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞.

#### –í–∞—Ä–∏–∞–Ω—Ç—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞:
1. **–ü–∞–∫–µ—Ç–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ (Batch Gradient Descent)**:
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–µ—Å—å –æ–±—É—á–∞—é—â–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.
   - –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞: —Ç–æ—á–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã.
   - –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏: –º–µ–¥–ª–µ–Ω–Ω—ã–π, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö.

2. **–°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ (Stochastic Gradient Descent, SGD)**:
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ **–æ–¥–∏–Ω —Å–ª—É—á–∞–π–Ω—ã–π –ø—Ä–∏–º–µ—Ä** –Ω–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞.
   - –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞: –±—ã—Å—Ç—Ä–µ–µ –æ–±–Ω–æ–≤–ª—è–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –ª—É—á—à–µ –∏–∑–±–µ–≥–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤.
   - –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏: —à—É–º–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è, –≤–æ–∑–º–æ–∂–Ω—ã –∫–æ–ª–µ–±–∞–Ω–∏—è.

3. **–ú–∏–Ω–∏-–±–∞—Ç—á –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ (Mini-batch Gradient Descent)**:
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç **–º–∞–ª–µ–Ω—å–∫–∏–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö (–±–∞—Ç—á–∏)** –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.
   - –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞: –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ —Å–∫–æ—Ä–æ—Å—Ç—å—é.

---

### 2. **–ë–∞—Ç—á–∏ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏**

#### –ß—Ç–æ —Ç–∞–∫–æ–µ –±–∞—Ç—á?
- –ë–∞—Ç—á (mini-batch) ‚Äî —ç—Ç–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏, –∫–æ—Ç–æ—Ä–æ–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–¥–Ω–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
- –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (\(B\)) –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.

#### –¢–∏–ø–∏—á–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –±–∞—Ç—á–∞:
- **–ú–∞–ª–µ–Ω—å–∫–∏–µ –±–∞—Ç—á–∏** (\(B = 32\), \(B = 64\)): –ø–æ–∑–≤–æ–ª—è—é—Ç –±—ã—Å—Ç—Ä–µ–µ –æ–±–Ω–æ–≤–ª—è—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –Ω–æ –º–µ–Ω–µ–µ —Ç–æ—á–Ω—ã–µ.
- **–ë–æ–ª—å—à–∏–µ –±–∞—Ç—á–∏** (\(B = 256\), \(B = 512\)): –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã, –Ω–æ —Ç—Ä–µ–±—É—é—Ç –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏ –∏ –º–æ–≥—É—Ç –º–µ–¥–ª–µ–Ω–Ω–µ–µ —Å—Ö–æ–¥–∏—Ç—å—Å—è.

---

### 3. **–≠—Ç–∞–ø—ã –æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–∞—Ç—á–µ–π**

1. **–†–∞–∑–±–∏–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö**:
   - –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –±–∞—Ç—á–∏ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.
2. **–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤**:
   - –î–ª—è –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥, –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –ø–æ—Ç–µ—Ä–∏ –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã.
   - –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–æ –±–∞—Ç—á—É.

---

### 4. **–ê–ª–≥–æ—Ä–∏—Ç–º SGD —Å –º–∏–Ω–∏-–±–∞—Ç—á–∞–º–∏**

1. –ü–µ—Ä–µ–º–µ—à–∞–π—Ç–µ –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ (shuffling).
2. –†–∞–∑–±–µ–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –±–∞—Ç—á–∏.
3. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞:
   - –í—ã–ø–æ–ª–Ω–∏—Ç–µ –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥.
   - –í—ã—á–∏—Å–ª–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å.
   - –í—ã–ø–æ–ª–Ω–∏—Ç–µ –æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏.
   - –û–±–Ω–æ–≤–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.

---

### 6. **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–∞—Ç—á–µ–π**

1. **–ö–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ —Å–∫–æ—Ä–æ—Å—Ç—å—é**:
   - –ú–∏–Ω–∏-–±–∞—Ç—á–∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã, —á–µ–º SGD, –∏ –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º –ø–∞–∫–µ—Ç–Ω—ã–π –º–µ—Ç–æ–¥.
2. **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª—è GPU**:
   - –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –Ω–∞ GPU.
3. **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è**:
   - –®—É–º –≤ –æ—Ü–µ–Ω–∫–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ —Å–≥–ª–∞–∂–∏–≤–∞–µ—Ç—Å—è, —á—Ç–æ —É–ª—É—á—à–∞–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å.

---

### –í—ã–≤–æ–¥—ã:
- –°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ ‚Äî —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –±–æ–ª—å—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏.
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–∞—Ç—á–µ–π –ø–æ–º–æ–≥–∞–µ—Ç —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.
- PyTorch –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —É–¥–æ–±–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã (`DataLoader`) –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞—Ç—á–∞–º–∏, —á—Ç–æ —É–ø—Ä–æ—â–∞–µ—Ç —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é SGD.
"""
    else:
        code = """
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# –î–∞–Ω–Ω—ã–µ
X = torch.randn(100, 1)  # 100 –ø—Ä–∏–º–µ—Ä–æ–≤, 1 –ø—Ä–∏–∑–Ω–∞–∫
y = 3 * X + torch.randn(100, 1) * 0.5  # –ò—Å—Ç–∏–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å —à—É–º–æ–º

# –î–∞—Ç–∞—Å–µ—Ç –∏ –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö
dataset = TensorDataset(X, y)
dataloader = DataLoader(dataset, batch_size=10, shuffle=True)  # –ë–∞—Ç—á —Ä–∞–∑–º–µ—Ä–æ–º 10

# –ú–æ–¥–µ–ª—å
model = nn.Linear(1, 1)

# –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# –û–±—É—á–µ–Ω–∏–µ
epochs = 50
for epoch in range(epochs):
    for batch_X, batch_y in dataloader:
        # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
        predictions = model(batch_X)
        loss = criterion(predictions, batch_y)
        
        # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    print(f"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}")
"""

    pyperclip.copy(code)


def adagrad(idx: int = 0):
    if idx == 0:
        code = """
### 10. –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞, –º–µ—Ç–æ–¥ –∏–º–ø—É–ª—å—Å–æ–≤ –∏ –º–µ—Ç–æ–¥ –ù–µ—Å—Ç–µ—Ä–æ–≤–∞

---

### 1. **–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞**

–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–µ–≥—É–ª–∏—Ä—É—é—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (learning rate) –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É—Å–∫–æ—Ä–∏—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –∏ —É–ª—É—á—à–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –≤ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.

#### –û—Å–Ω–æ–≤–Ω—ã–µ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã:
1. **Adagrad**:
   - –†–µ–≥—É–ª–∏—Ä—É–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞, —É–º–µ–Ω—å—à–∞—è —à–∞–≥–∏ –¥–ª—è —á–∞—Å—Ç–æ –æ–±–Ω–æ–≤–ª—è–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ —É–≤–µ–ª–∏—á–∏–≤–∞—è –¥–ª—è —Ä–µ–¥–∫–æ –æ–±–Ω–æ–≤–ª—è–µ–º—ã—Ö.
   - –§–æ—Ä–º—É–ª–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è:
     \[
     \theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{G_t + \epsilon}} \cdot g_t
     \]
     –≥–¥–µ \(G_t\) ‚Äî –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –∫–≤–∞–¥—Ä–∞—Ç—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.

2. **RMSProp**:
   - –ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è Adagrad, –∫–æ—Ç–æ—Ä–∞—è —É—á–∏—Ç—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–µ–¥–∞–≤–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤–º–µ—Å—Ç–æ –≤—Å–µ—Ö –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö.
   - –§–æ—Ä–º—É–ª–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è:
     \[
     G_t = \rho G_{t-1} + (1 - \rho) g_t^2
     \]
     \[
     \theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{G_t + \epsilon}} \cdot g_t
     \]

3. **Adam (Adaptive Moment Estimation)**:
   - –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –∏–¥–µ–∏ Adagrad –∏ RMSProp, –≤–∫–ª—é—á–∞—è –º–æ–º–µ–Ω—Ç—ã –ø–µ—Ä–≤–æ–≥–æ (\(m_t\)) –∏ –≤—Ç–æ—Ä–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞ (\(v_t\)).
   - –§–æ—Ä–º—É–ª—ã:
     \[
     m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
     \]
     \[
     v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
     \]
     \[
     \hat{m_t} = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v_t} = \frac{v_t}{1 - \beta_2^t}
     \]
     \[
     \theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{\hat{v_t}} + \epsilon} \cdot \hat{m_t}
     \]

---

### 2. **–ú–µ—Ç–æ–¥ –∏–º–ø—É–ª—å—Å–æ–≤ (Momentum)**

–ú–µ—Ç–æ–¥ –∏–º–ø—É–ª—å—Å–æ–≤ –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç –∫ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—é –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —á—Ç–æ–±—ã —É—Å–∫–æ—Ä–∏—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ —Å–∏–ª—å–Ω–æ –≤—ã—Ç—è–Ω—É—Ç—ã—Ö –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—è—Ö —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å.

#### –ò–¥–µ—è:
- –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –∏–∑ –ø—Ä–æ—à–ª—ã—Ö —à–∞–≥–æ–≤ —É—á–∏—Ç—ã–≤–∞—é—Ç—Å—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–∫—É—â–µ–≥–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è.
- –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤ –∏ —É–ª—É—á—à–∞–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å.

#### –§–æ—Ä–º—É–ª—ã:
1. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–º–ø—É–ª—å—Å–∞:
   \[
   v_t = \gamma v_{t-1} + \eta g_t
   \]
   –≥–¥–µ \(v_t\) ‚Äî —Å–∫–æ—Ä–æ—Å—Ç—å (momentum), \(\gamma\) ‚Äî –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∏–º–ø—É–ª—å—Å–∞ (–æ–±—ã—á–Ω–æ \(\gamma \in [0.9, 0.99]\)).

2. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:
   \[
   \theta_t = \theta_{t-1} - v_t
   \]

#### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
- –£—Å–∫–æ—Ä—è–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ –≤—ã—Ç—è–Ω—É—Ç—ã—Ö –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—è—Ö.
- –£–º–µ–Ω—å—à–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∑–∞—Å—Ç—Ä–µ–≤–∞–Ω–∏—è –≤ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–∞—Ö.

---

### 3. **–ú–µ—Ç–æ–¥ –ù–µ—Å—Ç–µ—Ä–æ–≤–∞ (Nesterov Accelerated Gradient, NAG)**

–ú–µ—Ç–æ–¥ –ù–µ—Å—Ç–µ—Ä–æ–≤–∞ —É–ª—É—á—à–∞–µ—Ç –º–µ—Ç–æ–¥ –∏–º–ø—É–ª—å—Å–æ–≤, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—è –ø–æ–ª–æ–∂–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –±—É–¥—É—â–µ–º –∏ –≤—ã—á–∏—Å–ª—è—è –≥—Ä–∞–¥–∏–µ–Ω—Ç –Ω–µ –≤ —Ç–µ–∫—É—â–µ–π —Ç–æ—á–∫–µ, –∞ –≤ —Ç–æ—á–∫–µ, —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å —É—á–µ—Ç–æ–º –∏–º–ø—É–ª—å—Å–∞.

#### –§–æ—Ä–º—É–ª—ã:
1. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ "–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–π" –ø–æ–∑–∏—Ü–∏–∏:
   \[
   \theta_{\text{lookahead}} = \theta_{t-1} - \gamma v_{t-1}
   \]

2. –ì—Ä–∞–¥–∏–µ–Ω—Ç –≤ –Ω–æ–≤–æ–π —Ç–æ—á–∫–µ:
   \[
   v_t = \gamma v_{t-1} + \eta g(\theta_{\text{lookahead}})
   \]

3. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:
   \[
   \theta_t = \theta_{t-1} - v_t
   \]

#### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
- –ë–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞.
- –£–ª—É—á—à–∞–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –∑–∞ —Å—á—ë—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –±—É–¥—É—â–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.

---

### 5. **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤**

| –ú–µ—Ç–æ–¥              | –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞                                      | –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏                                   |
|---------------------|---------------------------------------------------|---------------------------------------------|
| **SGD**            | –ü—Ä–æ—Å—Ç–æ—Ç–∞, –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö             | –ú–µ–¥–ª–µ–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å                        |
| **Momentum**        | –£—Å–∫–æ—Ä—è–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å, —É–º–µ–Ω—å—à–∞–µ—Ç –∫–æ–ª–µ–±–∞–Ω–∏—è          | –¢—Ä–µ–±—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (\(\gamma\)) |
| **Nesterov**        | –ë–æ–ª–µ–µ —Ç–æ—á–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞                | –ß—É—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è               |
| **Adagrad**         | –•–æ—Ä–æ—à–æ –¥–ª—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö                     | –°–ª–∏—à–∫–æ–º –º–∞–ª—ã–π —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º      |
| **RMSProp**         | –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∑–∞–¥–∞—á —Å —à—É–º–Ω—ã–º–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏          | –ù–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –º–æ–º–µ–Ω—Ç—ã –ø–µ—Ä–≤–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞        |
| **Adam**            | –ë—ã—Å—Ç—Ä–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å, –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á    | –ú–æ–∂–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å—Å—è, —Ç—Ä–µ–±—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏      |

---

### –í—ã–≤–æ–¥—ã:

- **Momentum** —É—Å–∫–æ—Ä—è–µ—Ç SGD, –ø–æ–º–æ–≥–∞—è –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–∏–Ω–∏–º—É–º—ã.
- **–ú–µ—Ç–æ–¥ –ù–µ—Å—Ç–µ—Ä–æ–≤–∞** –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞, —É–ª—É—á—à–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å.
- **–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã**, —Ç–∞–∫–∏–µ –∫–∞–∫ Adam, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –º–æ—â–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–µ–≥—É–ª–∏—Ä—É—è —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è.    
"""

    else:
        code = """
import torch
import torch.nn as nn
import torch.optim as optim

# –î–∞–Ω–Ω—ã–µ
X = torch.randn(100, 1)
y = 3 * X + torch.randn(100, 1) * 0.5

# –ú–æ–¥–µ–ª—å
model = nn.Linear(1, 1)

# –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
criterion = nn.MSELoss()

# –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã
adam_optimizer = optim.Adam(model.parameters(), lr=0.01)
momentum_optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)

# –í—ã–±–æ—Ä Adam –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
optimizer = adam_optimizer

# –û–±—É—á–µ–Ω–∏–µ
epochs = 100
for epoch in range(epochs):
    predictions = model(X)
    loss = criterion(predictions, y)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}")
"""
    pyperclip.copy(code)


def weight_init(idx: int = 0):
    if idx == 0:
        code = """
### 11. –ü—Ä–æ–±–ª–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –ò–ù–°. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ö—Å–∞–≤—å–µ

---

### 1. **–ü—Ä–æ–±–ª–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π**

–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –∏–≥—Ä–∞–µ—Ç –≤–∞–∂–Ω—É—é —Ä–æ–ª—å, —Ç–∞–∫ –∫–∞–∫ –ø–ª–æ—Ö–∞—è –Ω–∞—á–∞–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø—Ä–æ–±–ª–µ–º–∞–º –≤ –æ–±—É—á–µ–Ω–∏–∏.

#### –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:
1. **–ó–∞—Ç—É—Ö–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤**:
   - –ï—Å–ª–∏ –Ω–∞—á–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ —Å–ª–∏—à–∫–æ–º –º–∞–ª—ã, —Ç–æ —Å–∏–≥–Ω–∞–ª—ã, –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—ã–µ —á–µ—Ä–µ–∑ —Å–µ—Ç—å, —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –æ—á–µ–Ω—å –º–∞–ª—ã–º–∏.
   - –≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Ç–æ–º—É, —á—Ç–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —Ç–∞–∫–∂–µ –∑–∞—Ç—É—Ö–∞—é—Ç.

2. **–í–∑—Ä—ã–≤–∞—é—â–∏–µ—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã**:
   - –ï—Å–ª–∏ –Ω–∞—á–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ, —Ç–æ —Å–∏–≥–Ω–∞–ª—ã, –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—ã–µ —á–µ—Ä–µ–∑ —Å–µ—Ç—å, —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–º–∏.
   - –≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º, –∏–∑-–∑–∞ —á–µ–≥–æ –æ–±—É—á–µ–Ω–∏–µ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –Ω–µ–≤–æ–∑–º–æ–∂–Ω—ã–º.

3. **–°–∏–º–º–µ—Ç—Ä–∏—è –≤–µ—Å–æ–≤**:
   - –ï—Å–ª–∏ –Ω–∞—á–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—ã –∏–ª–∏ –±–ª–∏–∑–∫–∏ –¥—Ä—É–≥ –∫ –¥—Ä—É–≥—É, –≤—Å–µ –Ω–µ–π—Ä–æ–Ω—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ –±—É–¥—É—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è –æ–¥–∏–Ω–∞–∫–æ–≤–æ, —á—Ç–æ –ª–∏—à–∞–µ—Ç –º–æ–¥–µ–ª—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–±—É—á–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏.

---

### 2. **–†–µ—à–µ–Ω–∏–µ: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ö—Å–∞–≤—å–µ (Xavier Initialization)**

–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ö—Å–∞–≤—å–µ –±—ã–ª–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –≤ —Ä–∞–±–æ—Ç–µ "Understanding the difficulty of training deep feedforward neural networks" (2010). –¶–µ–ª—å ‚Äî —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –¥–∏—Å–ø–µ—Ä—Å–∏—é –≤—Ö–æ–¥–æ–≤ –∏ –≤—ã—Ö–æ–¥–æ–≤ –Ω–∞ –∫–∞–∂–¥–æ–º —É—Ä–æ–≤–Ω–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏.

#### –ò–¥–µ—è:
- –í–µ—Å–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è —Ç–∞–∫, —á—Ç–æ–±—ã –¥–∏—Å–ø–µ—Ä—Å–∏—è –≤—ã—Ö–æ–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∫–∞–∂–¥–æ–≥–æ –Ω–µ–π—Ä–æ–Ω–∞ –±—ã–ª–∞ –ø—Ä–∏–º–µ—Ä–Ω–æ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π.
- –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –∑–∞—Ç—É—Ö–∞–Ω–∏–µ –∏–ª–∏ –≤–∑—Ä—ã–≤ —Å–∏–≥–Ω–∞–ª–æ–≤.

#### –§–æ—Ä–º—É–ª–∞:
- –î–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:
  \[
  W \sim U\left[-\frac{\sqrt{6}}{\sqrt{n_{\text{in}} + n_{\text{out}}}}, \frac{\sqrt{6}}{\sqrt{n_{\text{in}} + n_{\text{out}}}}\right]
  \]
- –î–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:
  \[
  W \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}} + n_{\text{out}}}\right)
  \]
–≥–¥–µ \(n_{\text{in}}\) ‚Äî —á–∏—Å–ª–æ –≤—Ö–æ–¥–æ–≤ –Ω–µ–π—Ä–æ–Ω–∞, \(n_{\text{out}}\) ‚Äî —á–∏—Å–ª–æ –≤—ã—Ö–æ–¥–æ–≤.

---

### 3. **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ö—Å–∞–≤—å–µ**
1. –£—Å–∫–æ—Ä—è–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è.
2. –£–º–µ–Ω—å—à–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∑–∞—Ç—É—Ö–∞–Ω–∏—è –∏–ª–∏ –≤–∑—Ä—ã–≤–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.
3. –†–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã—Ö –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ 0 (–Ω–∞–ø—Ä–∏–º–µ—Ä, sigmoid, tanh).

---

### 4. **–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –≤ PyTorch**

PyTorch –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥—ã –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤, –≤–∫–ª—é—á–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –ö—Å–∞–≤—å–µ, —á–µ—Ä–µ–∑ –º–æ–¥—É–ª—å `torch.nn.init`.

---

### 5. **–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –ö—Å–∞–≤—å–µ**

- **–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ sigmoid –∏–ª–∏ tanh**, –ø–æ—Å–∫–æ–ª—å–∫—É —ç—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –∏–º–µ—é—Ç —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã, –∏ –ö—Å–∞–≤—å–µ –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å —Å–∏–≥–Ω–∞–ª—ã –≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ.
- –î–ª—è **ReLU** –∏ –µ—ë –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–π –ª—É—á—à–µ –ø–æ–¥—Ö–æ–¥–∏—Ç **–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è He**, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∞ —É—á–∏—Ç—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω—ã.

### –í—ã–≤–æ–¥—ã

- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –≤–∞–∂–Ω–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –∏ –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏.
- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ö—Å–∞–≤—å–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã—Ö –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω—É–ª—è.
- PyTorch –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏, —á—Ç–æ —É–ø—Ä–æ—â–∞–µ—Ç —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é.        
"""
    else:
        code = """
#### –ü—Ä–∏–º–µ—Ä:
import torch
import torch.nn as nn
import torch.nn.init as init

# –ú–æ–¥–µ–ª—å
class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(10, 50)  # –õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
model = SimpleNet()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é –ö—Å–∞–≤—å–µ
def initialize_weights(m):
    if isinstance(m, nn.Linear):
        init.xavier_uniform_(m.weight)  # –ö—Å–∞–≤—å–µ –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        init.zeros_(m.bias)             # –°–±—Ä–æ—Å —Å–º–µ—â–µ–Ω–∏—è –≤ –Ω–æ–ª—å

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫ –º–æ–¥–µ–ª–∏
model.apply(initialize_weights)

# –î–∞–Ω–Ω—ã–µ
X = torch.randn(100, 10)  # 100 –ø—Ä–∏–º–µ—Ä–æ–≤, 10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
y = torch.randint(0, 2, (100,))  # –ë–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏ (0 –∏–ª–∏ 1)

# –ú–æ–¥–µ–ª—å
model = SimpleNet()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
model.apply(initialize_weights)

# –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# –û–±—É—á–µ–Ω–∏–µ
epochs = 50
for epoch in range(epochs):
    # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
    predictions = model(X)
    loss = criterion(predictions, y)
    
    # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}")
"""

    pyperclip.copy(code)


def dropout(idx: int = 0):
    if idx == 0:
        code = """
### 12. –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è. –ü—Ä–∏–Ω—Ü–∏–ø Dropout. –°–ª–æ–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –≤ PyTorch

---

### 1. **–ü—Ä–æ–±–ª–µ–º–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏**

**–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ (overfitting)** –≤–æ–∑–Ω–∏–∫–∞–µ—Ç, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å —Ö–æ—Ä–æ—à–æ –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –ø–æ–¥ –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ, –≤–∫–ª—é—á–∞—è —à—É–º –∏ —Å–ª—É—á–∞–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏, –Ω–æ –ø–ª–æ—Ö–æ –æ–±–æ–±—â–∞–µ—Ç –∏—Ö –Ω–∞ –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ.

#### –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è:
1. –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ.
2. –ù–∏–∑–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π/—Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ.

#### –ü—Ä–∏—á–∏–Ω—ã –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è:
1. –ù–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
2. –°–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω–∞—è –º–æ–¥–µ–ª—å —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
3. –î–ª–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –ø–æ–¥—Ö–æ–¥—è—â–µ–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏.

---

### 2. **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è**

**–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è** ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥—ã, —É–º–µ–Ω—å—à–∞—é—â–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –∑–∞ —Å—á—ë—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –≤–Ω–µ—Å–µ–Ω–∏—è —à—É–º–∞.

#### –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã:
1. **L1/L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è**:
   - –î–æ–±–∞–≤–ª—è–µ—Ç —à—Ç—Ä–∞—Ñ –∫ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å, –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∑–Ω–∞—á–µ–Ω–∏—è–º –≤–µ—Å–æ–≤.
   - L1-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (Lasso): \(\lambda \sum |w_i|\).
   - L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (Ridge): \(\lambda \sum w_i^2\).
   
2. **Dropout**:
   - –ò—Å–∫–ª—é—á–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω—ã –∏–∑ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏.

3. **–†–∞–Ω–Ω–µ–µ –ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è (Early Stopping)**:
   - –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ, –µ—Å–ª–∏ –æ—à–∏–±–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ –ø–µ—Ä–µ—Å—Ç–∞—ë—Ç —É–º–µ–Ω—å—à–∞—Ç—å—Å—è.

---

### 3. **–ü—Ä–∏–Ω—Ü–∏–ø –º–µ—Ö–∞–Ω–∏–∑–º–∞ Dropout**

Dropout ‚Äî —ç—Ç–æ —Ç–µ—Ö–Ω–∏–∫–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –≤ 2014 –≥–æ–¥—É. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è:
- –ù–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ —Å–ª—É—á–∞–π–Ω–æ "–æ—Ç–∫–ª—é—á–∞—Ç—å" –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—É—é –¥–æ–ª—é –Ω–µ–π—Ä–æ–Ω–æ–≤.
- –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ–¥–Ω–æ–≥–æ –Ω–µ–π—Ä–æ–Ω–∞ –æ—Ç –¥—Ä—É–≥–æ–≥–æ –∏ –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç —Å–µ—Ç—å –æ–±—É—á–∞—Ç—å—Å—è –±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º.

#### –ú–µ—Ö–∞–Ω–∏–∑–º —Ä–∞–±–æ—Ç—ã:
1. –ù–∞ —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è:
   - –° –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é \(p\) –∫–∞–∂–¥—ã–π –Ω–µ–π—Ä–æ–Ω –≤—Ä–µ–º–µ–Ω–Ω–æ –∏—Å–∫–ª—é—á–∞–µ—Ç—Å—è –∏–∑ —Å–µ—Ç–∏.
   - –≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –±–æ–ª–µ–µ "—à—É–º–Ω–æ–º—É" –ø—Ä–æ—Ü–µ—Å—Å—É –æ–±—É—á–µ–Ω–∏—è, —É–ª—É—á—à–∞—è –æ–±–æ–±—â–µ–Ω–∏–µ.
2. –ù–∞ —ç—Ç–∞–ø–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:
   - Dropout –æ—Ç–∫–ª—é—á–∞–µ—Ç—Å—è, –∏ —Å–µ—Ç—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–µ –Ω–µ–π—Ä–æ–Ω—ã.
   - –í—ã—Ö–æ–¥—ã –Ω–µ–π—Ä–æ–Ω–æ–≤ –º–∞—Å—à—Ç–∞–±–∏—Ä—É—é—Ç—Å—è, —á—Ç–æ–±—ã —É—á–µ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤.

#### –§–æ—Ä–º—É–ª–∞:
–î–ª—è –Ω–µ–π—Ä–æ–Ω–∞ \(h_i\) —Å –≤–∫–ª—é—á—ë–Ω–Ω—ã–º Dropout:
\[
h_i^{\text{dropout}} = \begin{cases} 
0, & \text{–µ—Å–ª–∏ –æ—Ç–∫–ª—é—á—ë–Ω} \\
\frac{h_i}{1-p}, & \text{–µ—Å–ª–∏ –≤–∫–ª—é—á—ë–Ω (–Ω–∞ —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è)} 
\end{cases}
\]

---

### 4. **–°–ª–æ–∏ Dropout –≤ PyTorch**

–í PyTorch Dropout —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è –∫–∞–∫ —Å–ª–æ–π `torch.nn.Dropout`.

#### –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Dropout:
```python
import torch
import torch.nn as nn

# –ú–æ–¥–µ–ª—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Dropout
class DropoutNet(nn.Module):
    def __init__(self):
        super(DropoutNet, self).__init__()
        self.fc1 = nn.Linear(10, 50)
        self.dropout = nn.Dropout(p=0.5)  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç–∫–ª—é—á–µ–Ω–∏—è 50%
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)  # Dropout –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è
        x = self.fc2(x)
        return x

# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
model = DropoutNet()
```
---

### 6. **–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã –æ Dropout**

1. **Dropout**:
   - –ü—Ä–æ—Å—Ç–∞—è, –Ω–æ –º–æ—â–Ω–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏.
   - –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö.

2. **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã Dropout**:
   - –ó–Ω–∞—á–µ–Ω–∏–µ \(p\) –æ–±—ã—á–Ω–æ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ \(0.2 \leq p \leq 0.5\).
   - –î–ª—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ–Ω—å—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ \(p\).

3. **–°–æ—á–µ—Ç–∞–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏**:
   - Dropout –º–æ–∂–µ—Ç –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å—Å—è —Å L1/L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π –∏ —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏.

---

### –í—ã–≤–æ–¥—ã

- Dropout ‚Äî —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏.
- PyTorch –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —É–¥–æ–±–Ω—ã–π —Å–ª–æ–π Dropout, –∫–æ—Ç–æ—Ä—ã–π –ª–µ–≥–∫–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏.
- –°–æ–≤–º–µ—â–µ–Ω–∏–µ Dropout —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π) –ø–æ–º–æ–≥–∞–µ—Ç –±–æ—Ä–æ—Ç—å—Å—è —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º.    
"""

    else:
        code = """
### 5. **–ü—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å Dropout**

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# –î–∞–Ω–Ω—ã–µ
X = torch.randn(100, 10)  # 100 –ø—Ä–∏–º–µ—Ä–æ–≤, 10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
y = torch.randint(0, 2, (100,))  # –ë–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏

# –î–∞—Ç–∞—Å–µ—Ç –∏ –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö
dataset = TensorDataset(X, y)
dataloader = DataLoader(dataset, batch_size=10, shuffle=True)

# –ú–æ–¥–µ–ª—å
model = DropoutNet()

# –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# –û–±—É—á–µ–Ω–∏–µ
epochs = 50
for epoch in range(epochs):
    model.train()  # –í–∫–ª—é—á–∞–µ–º —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è (Dropout –∞–∫—Ç–∏–≤–µ–Ω)
    for batch_X, batch_y in dataloader:
        predictions = model(batch_X)
        loss = criterion(predictions, batch_y)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    model.eval()  # –û—Ç–∫–ª—é—á–∞–µ–º Dropout
    with torch.no_grad():
        val_loss = criterion(model(X), y)
    print(f"Epoch [{epoch+1}/{epochs}], Training Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}")
"""

    pyperclip.copy(code)


def bn(idx: int = 0):
    if idx == 0:
        code = """
### 13. –ú–∏–Ω–∏-–±–∞—Ç—á–∏, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –º–∏–Ω–∏-–±–∞—Ç—á–∞–º –∏ —Å–ª–æ–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ PyTorch

---

### 1. **–ú–∏–Ω–∏-–±–∞—Ç—á–∏ ‚Äî –ø—Ä–∏—á–∏–Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è**

–ú–∏–Ω–∏-–±–∞—Ç—á (mini-batch) ‚Äî —ç—Ç–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä–æ–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞ –æ–¥–Ω–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏.

#### –ü—Ä–∏—á–∏–Ω—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–∏–Ω–∏-–±–∞—Ç—á–µ–π:
1. **–£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è**:
   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–∞—Ç—á–µ–π –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∑–∞–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å GPU –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö.
   - –ü–æ–ª–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç –Ω–∞ –≤—Å–µ–π –≤—ã–±–æ—Ä–∫–µ (batch gradient descent) –º–µ–¥–ª–µ–Ω–Ω—ã–π, –∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–∞ –æ–¥–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ (stochastic gradient descent) —à—É–º–Ω—ã–µ.

2. **–ö–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É —Å–∫–æ—Ä–æ—Å—Ç—å—é –∏ —Ç–æ—á–Ω–æ—Å—Ç—å—é**:
   - –ú–∏–Ω–∏-–±–∞—Ç—á–∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Å–∫–æ—Ä–æ—Å—Ç—å—é –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å—é –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞.

3. **–ú–µ–Ω—å—à–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏**:
   - –ü–æ–ª–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ —Ç—Ä–µ–±—É–µ—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ–π –≤—ã–±–æ—Ä–∫–∏ –≤ –ø–∞–º—è—Ç—å, —á—Ç–æ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö.
   - –ë–∞—Ç—á–∏ —Å–Ω–∏–∂–∞—é—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø–∞–º—è—Ç–∏, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—è –¥–∞–Ω–Ω—ã–µ —á–∞—Å—Ç—è–º–∏.

4. **–°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤**:
   - –°—Ä–µ–¥–Ω–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç –ø–æ –º–∏–Ω–∏-–±–∞—Ç—á—É –ø–æ–º–æ–≥–∞–µ—Ç —Å–≥–ª–∞–¥–∏—Ç—å —à—É–º—ã, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤—ã–º.

---

### 2. **–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –º–∏–Ω–∏-–±–∞—Ç—á–∞–º (Batch Normalization)**

**Batch Normalization (BatchNorm)** ‚Äî —ç—Ç–æ —Ç–µ—Ö–Ω–∏–∫–∞, –∫–æ—Ç–æ—Ä–∞—è —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –∏ —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π, –Ω–æ—Ä–º–∞–ª–∏–∑—É—è –≤—Ö–æ–¥—ã –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ—è.

#### –ò–¥–µ—è:
- –î–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–µ–π—Ä–æ–Ω–∞ –≤ —Å–ª–æ–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –≤—ã—Ö–æ–¥—ã –ø–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ –≤–Ω—É—Ç—Ä–∏ –º–∏–Ω–∏-–±–∞—Ç—á–∞ (—Å—Ä–µ–¥–Ω–µ–µ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ).

#### –§–æ—Ä–º—É–ª—ã:
–î–ª—è –≤—Ö–æ–¥–∞ \(x_i\) –Ω–µ–π—Ä–æ–Ω–∞:
1. –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –ø–æ –º–∏–Ω–∏-–±–∞—Ç—á—É:
   \[
   \mu = \frac{1}{m} \sum_{i=1}^m x_i, \quad \sigma^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu)^2
   \]
   –≥–¥–µ \(m\) ‚Äî —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞.

2. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è:
   \[
   \hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}
   \]
   –≥–¥–µ \(\epsilon\) ‚Äî –Ω–µ–±–æ–ª—å—à–æ–µ —á–∏—Å–ª–æ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0.

3. –õ–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ:
   \[
   y_i = \gamma \hat{x}_i + \beta
   \]
   –≥–¥–µ \(\gamma\) –∏ \(\beta\) ‚Äî –æ–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.

---

### 3. **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Batch Normalization**
1. **–°—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã**:
   - –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –∑–∞—Ç—É—Ö–∞–Ω–∏–µ –∏–ª–∏ –≤–∑—Ä—ã–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç—è—Ö.
   
2. **–£—Å–∫–æ—Ä—è–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å**:
   - –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è.

3. **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è**:
   - –í–Ω–æ—Å–∏—Ç –Ω–µ–±–æ–ª—å—à–æ–π —à—É–º –∏–∑-–∑–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–∏–Ω–∏-–±–∞—Ç—á–∞, —á—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ.

4. **–£–º–µ–Ω—å—à–∞–µ—Ç —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤**:
   - –£–ø—Ä–æ—â–∞–µ—Ç –≤—ã–±–æ—Ä –Ω–∞—á–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏.

---

### 4. **–°–ª–æ–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ PyTorch**

PyTorch –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≥–æ—Ç–æ–≤—ã–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ Batch Normalization, –∞ —Ç–∞–∫–∂–µ –¥—Ä—É–≥–∏—Ö –≤–∏–¥–æ–≤ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.

#### 1. Batch Normalization
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–µ–≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏.
- –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω –∫–∞–∫ `nn.BatchNorm1d`, `nn.BatchNorm2d`, –∏ `nn.BatchNorm3d` –¥–ª—è –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞–∑–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏.

–ü—Ä–∏–º–µ—Ä –¥–ª—è –¥–∞–Ω–Ω—ã—Ö —Å –¥–≤—É–º—è –∏–∑–º–µ—Ä–µ–Ω–∏—è–º–∏:
```python
import torch
import torch.nn as nn

# –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: –º–∏–Ω–∏-–±–∞—Ç—á —Å 3 –ø—Ä–∏–º–µ—Ä–∞–º–∏, 5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
x = torch.randn(3, 5)

# Batch Normalization
batch_norm = nn.BatchNorm1d(5)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ 5 –ø—Ä–∏–∑–Ω–∞–∫–∞–º
output = batch_norm(x)

print("Input:", x)
print("Normalized Output:", output)
```

---

#### 2. Layer Normalization
- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ –≤—Å–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞, –∞ –Ω–µ –ø–æ –º–∏–Ω–∏-–±–∞—Ç—á—É.
- –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –∫–∞–∫ `nn.LayerNorm`.

–ü—Ä–∏–º–µ—Ä:
```python
layer_norm = nn.LayerNorm(5)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ 5 –ø—Ä–∏–∑–Ω–∞–∫–∞–º
output = layer_norm(x)

print("Layer Normalized Output:", output)
```

---

#### 3. Instance Normalization
- –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –∏–ª–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–º —Ä—è–¥–∞–º, –Ω–æ—Ä–º–∞–ª–∏–∑—É—è –¥–∞–Ω–Ω—ã–µ –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞.
- –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –∫–∞–∫ `nn.InstanceNorm2d`.

### 6. **–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Batch Normalization?**

- –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –¥–∞–Ω–Ω—ã—Ö, –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤ –±–∞—Ç—á–∏.
- –ù–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –∑–∞–¥–∞—á —Å –Ω–µ–±–æ–ª—å—à–∏–º–∏ –±–∞—Ç—á–∞–º–∏, —Ç–∞–∫ –∫–∞–∫ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞—Ç—á–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–π.

---

### –í—ã–≤–æ–¥—ã

1. **–ú–∏–Ω–∏-–±–∞—Ç—á–∏** –ø–æ–º–æ–≥–∞—é—Ç —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –∏ —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ —É–º–µ–Ω—å—à–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏.
2. **Batch Normalization** —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ, —É–ª—É—á—à–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∏ –ø–æ–º–æ–≥–∞–µ—Ç –±–æ—Ä–æ—Ç—å—Å—è —Å –∑–∞—Ç—É—Ö–∞—é—â–∏–º–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏.
3. PyTorch –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —É–¥–æ–±–Ω—ã–µ —Å–ª–æ–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (`BatchNorm`, `LayerNorm`), –∫–æ—Ç–æ—Ä—ã–µ –ª–µ–≥–∫–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –≤ –º–æ–¥–µ–ª—å.
"""
    else:
        code = """
class NetWithBatchNorm(nn.Module):
    def __init__(self):
        super(NetWithBatchNorm, self).__init__()
        self.fc1 = nn.Linear(10, 50)
        self.bn1 = nn.BatchNorm1d(50)  # BatchNorm –¥–ª—è 50 –Ω–µ–π—Ä–æ–Ω–æ–≤
        self.fc2 = nn.Linear(50, 10)
        self.bn2 = nn.BatchNorm1d(10)  # BatchNorm –¥–ª—è 10 –Ω–µ–π—Ä–æ–Ω–æ–≤

    def forward(self, x):
        x = torch.relu(self.bn1(self.fc1(x)))  # BatchNorm –ø–æ—Å–ª–µ —Å–ª–æ—è
        x = self.bn2(self.fc2(x))
        return x

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
model = NetWithBatchNorm()

# –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
inputs = torch.randn(32, 10)  # –ú–∏–Ω–∏-–±–∞—Ç—á –∏–∑ 32 –ø—Ä–∏–º–µ—Ä–æ–≤, –∫–∞–∂–¥—ã–π —Å 10 –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
outputs = model(inputs)

print("Output shape:", outputs.shape)
"""

    pyperclip.copy(code)


def nnmodule(idx: int = 0):
    code = """
### 14. –ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–µ —Å–µ—Ç–∏, –≥—Ä–∞—Ñ –ø–æ—Ç–æ–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ –∫–ª–∞—Å—Å `nn.Module` –≤ PyTorch

---

### 1. **–ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏**

–ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (Multilayer Perceptrons, MLP) —Å–æ—Å—Ç–æ—è—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ–µ–≤:
1. **–í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π**: –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.
2. **–°–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏**: –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç –¥–∞–Ω–Ω—ã–µ, –æ–±—É—á–∞—è —Å–ª–æ–∂–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏.
3. **–í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π**: –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–ª–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏.

–ö–∞–∂–¥—ã–π —Å–ª–æ–π —Å–æ—Å—Ç–æ–∏—Ç –∏–∑:
- –õ–∏–Ω–µ–π–Ω–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è (\(Wx + b\)).
- –ù–µ–ª–∏–Ω–µ–π–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ReLU, Tanh).

---

### 2. **–ì—Ä–∞—Ñ –ø–æ—Ç–æ–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π**

–ì—Ä–∞—Ñ –ø–æ—Ç–æ–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –≥–¥–µ —É–∑–ª—ã ‚Äî —ç—Ç–æ –æ–ø–µ—Ä–∞—Ü–∏–∏, –∞ —Ä—ë–±—Ä–∞ ‚Äî —ç—Ç–æ –¥–∞–Ω–Ω—ã–µ (—Ç–µ–Ω–∑–æ—Ä—ã), –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—ã–µ –º–µ–∂–¥—É –æ–ø–µ—Ä–∞—Ü–∏—è–º–∏.

#### –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
1. **–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ**:
   - –í PyTorch –≥—Ä–∞—Ñ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π —Å–æ–∑–¥–∞—ë—Ç—Å—è –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã (–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –≥—Ä–∞—Ñ).
   - –≠—Ç–æ –¥–µ–ª–∞–µ—Ç PyTorch –≥–∏–±–∫–∏–º –∏ —É–¥–æ–±–Ω—ã–º –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—è–º–∏.

2. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∞**:
   - **–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ (Forward Pass)**: –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –æ—Ç –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–æ –≤—ã—Ö–æ–¥–∞ —Å–µ—Ç–∏.
   - **–û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ (Backward Pass)**: –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è.

---

### 3. **–ö–ª–∞—Å—Å `nn.Module` –≤ PyTorch**

`nn.Module` ‚Äî —ç—Ç–æ –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –≤ PyTorch. –û–Ω –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–ª–æ—è–º–∏, –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è–º–∏ –º–æ–¥–µ–ª–∏.

#### –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:
- –£–ø—Ä–æ—â–∞–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π.
- –£–ø—Ä–∞–≤–ª—è–µ—Ç —Å–ª–æ—è–º–∏ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –º–æ–¥–µ–ª–∏.
- –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –º–µ—Ç–æ–¥—ã –¥–ª—è –ø—Ä—è–º–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞, –æ–±—É—á–µ–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.

---

### 4. **–û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è –∏ –º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Å–∞ `nn.Module`**

#### –ü–æ–ª—è:
1. **`parameters`**:
   - –°–æ–¥–µ—Ä–∂–∏—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–≤–µ—Å–∞ –∏ —Å–º–µ—â–µ–Ω–∏—è) –º–æ–¥–µ–ª–∏.
   - –ù–∞–ø—Ä–∏–º–µ—Ä: –≤–µ—Å–∞ –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è, –æ–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã BatchNorm.

2. **`modules`**:
   - –°–ø–∏—Å–æ–∫ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –º–æ–¥—É–ª–µ–π (—Å–ª–æ–µ–≤).
   - –ü–æ–∑–≤–æ–ª—è–µ—Ç —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ—Ä–≥–∞–Ω–∏–∑–æ–≤—ã–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã.

#### –ú–µ—Ç–æ–¥—ã:
1. **`__init__()`**:
   - –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ª–æ—ë–≤ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏.

2. **`forward()`**:
   - –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ –º–æ–¥–µ–ª–∏.

3. **`parameters()`**:
   - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Ç–µ—Ä–∞—Ç–æ—Ä –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏.

4. **`train()` –∏ `eval()`**:
   - –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Ç –º–æ–¥–µ–ª—å –º–µ–∂–¥—É —Ä–µ–∂–∏–º–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ (–≤–∫–ª—é—á–∞—é—Ç/–≤—ã–∫–ª—é—á–∞—é—Ç Dropout, BatchNorm –∏ —Ç. –¥.).

5. **`state_dict()`**:
   - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –±—É—Ñ–µ—Ä–æ–≤ –º–æ–¥–µ–ª–∏.

6. **`load_state_dict()`**:
   - –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∏–∑ —Å–ª–æ–≤–∞—Ä—è.

---

### 5. **–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è `nn.Module`**

#### –°–æ–∑–¥–∞–Ω–∏–µ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–π –º–æ–¥–µ–ª–∏:
```python
import torch
import torch.nn as nn

class MultiLayerNet(nn.Module):
    def __init__(self):
        super(MultiLayerNet, self).__init__()
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ª–æ–∏
        self.fc1 = nn.Linear(10, 50)  # –õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π: 10 -> 50
        self.relu = nn.ReLU()         # –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
        self.fc2 = nn.Linear(50, 10)  # –õ–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π: 50 -> 10

    def forward(self, x):
        # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
model = MultiLayerNet()

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
print(model)
```

–í—ã–≤–æ–¥:
```
MultiLayerNet(
  (fc1): Linear(in_features=10, out_features=50, bias=True)
  (relu): ReLU()
  (fc2): Linear(in_features=50, out_features=10, bias=True)
)
```

---

#### –†–∞–±–æ—Ç–∞ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:
```python
# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
for name, param in model.named_parameters():
    print(f"{name}: {param.shape}")

# –í—ã–≤–æ–¥:
# fc1.weight: torch.Size([50, 10])
# fc1.bias: torch.Size([50])
# fc2.weight: torch.Size([10, 50])
# fc2.bias: torch.Size([10])
```

---

#### –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏:
```python
# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
torch.save(model.state_dict(), "model.pth")

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
model.load_state_dict(torch.load("model.pth"))
```

---

#### –ü—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è:
```python
# –î–∞–Ω–Ω—ã–µ
X = torch.randn(100, 10)
y = torch.randint(0, 10, (100,))

# –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# –û–±—É—á–µ–Ω–∏–µ
epochs = 50
for epoch in range(epochs):
    predictions = model(X)  # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
    loss = criterion(predictions, y)  # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
    
    optimizer.zero_grad()  # –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
    loss.backward()  # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
    optimizer.step()  # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    
    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}")
```

---

### –í—ã–≤–æ–¥—ã:

1. **–ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–µ —Å–µ—Ç–∏** –ø–æ–∑–≤–æ–ª—è—é—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –ª–∏–Ω–µ–π–Ω—ã—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π –∏ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö –∞–∫—Ç–∏–≤–∞—Ü–∏–π.
2. **–ì—Ä–∞—Ñ –ø–æ—Ç–æ–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π** –≤ PyTorch –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫—É –≥–∏–±–∫–æ–π –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.
3. **–ö–ª–∞—Å—Å `nn.Module`** ‚Äî –æ—Å–Ω–æ–≤–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –≤ PyTorch, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—â–∞—è —É–¥–æ–±–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–ª–æ—è–º–∏, –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ –æ–±—É—á–µ–Ω–∏–µ–º.
"""
    pyperclip.copy(code)


def cnn(idx: int = 0):
    if idx == 0:
        code = """
### 15. –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö, –ø—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–µ—Ç–µ–π –∏ –∏—Ö –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞

---

### 1. **–°–ø–µ—Ü–∏—Ñ–∏–∫–∞ –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö**

–†–∞–±–æ—Ç–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏–º–µ–µ—Ç —Å–≤–æ–∏ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:

#### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:
1. **–í—ã—Å–æ–∫–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö**:
   - –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–∞—Å—Ç–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –≤ –≤–∏–¥–µ –±–æ–ª—å—à–∏—Ö –º–∞—Å—Å–∏–≤–æ–≤ –ø–∏–∫—Å–µ–ª–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, \(224 \times 224 \times 3\) –¥–ª—è —Ü–≤–µ—Ç–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è), —á—Ç–æ —Ç—Ä–µ–±—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤.
   
2. **–õ–æ–∫–∞–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞**:
   - –ë–ª–∏–∑–∫–∏–µ –ø–∏–∫—Å–µ–ª–∏ —Å–≤—è–∑–∞–Ω—ã –∏ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ –∫—Ä–∞—è, —Ç–µ–∫—Å—Ç—É—Ä—ã –∏ —Ñ–æ—Ä–º—ã.

3. **–ò–Ω–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è–º**:
   - –ê–ª–≥–æ—Ä–∏—Ç–º—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É—Å—Ç–æ–π—á–∏–≤—ã –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º –≤ –º–∞—Å—à—Ç–∞–±–µ, –ø–æ–≤–æ—Ä–æ—Ç–∞—Ö –∏ —Å–¥–≤–∏–≥–∞—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.

4. **–ë–æ–ª—å—à–∏–µ –æ–±—ä–µ–º—ã –¥–∞–Ω–Ω—ã—Ö**:
   - –î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ –≥–ª—É–±–æ–∫–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏, —Ç—Ä–µ–±—É—é—Ç—Å—è –±–æ–ª—å—à–∏–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, ImageNet).

#### –ü—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á:
- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤).
- –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è (–≤—ã–¥–µ–ª–µ–Ω–∏–µ –æ–±–ª–∞—Å—Ç–µ–π –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏).
- –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ (–ø–æ–∏—Å–∫ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –∏—Ö –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏).
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å –ø–æ–º–æ—â—å—é GAN).

---

### 2. **–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–µ—Ç–µ–π (CNN)**

**–°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (Convolutional Neural Networks, CNN)** –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω—ã –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å —Å–µ—Ç—á–∞—Ç–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π, —Ç–∞–∫–æ–π –∫–∞–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.

#### –û—Å–Ω–æ–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã CNN:

1. **–°–≤–µ—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π (Convolutional Layer)**:
   - –í—ã–ø–æ–ª–Ω—è–µ—Ç —Å–≤–µ—Ä—Ç–∫—É, –∏—Å–ø–æ–ª—å–∑—É—è —Ñ–∏–ª—å—Ç—Ä—ã (—è–¥—Ä–∞), –∫–æ—Ç–æ—Ä—ã–µ –∏–∑–≤–ª–µ–∫–∞—é—Ç –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∫—Ä–∞—è, —Ç–µ–∫—Å—Ç—É—Ä—ã –∏ —Ç. –¥.).
   - –ü—Ä–∏–Ω—Ü–∏–ø:
     \[
     (I * K)(x, y) = \sum_m \sum_n I(x+m, y+n) \cdot K(m, n)
     \]
     –≥–¥–µ \(I\) ‚Äî –≤—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, \(K\) ‚Äî —Ñ–∏–ª—å—Ç—Ä, \(x, y\) ‚Äî –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –ø–∏–∫—Å–µ–ª—è.

2. **–§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏**:
   - –ù–∞–ø—Ä–∏–º–µ—Ä, ReLU (\(\max(0, x)\)) –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å.

3. **–°–ª–æ–∏ –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏ (Pooling Layers)**:
   - –£–º–µ–Ω—å—à–∞—é—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö, —Å–æ—Ö—Ä–∞–Ω—è—è –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, MaxPooling –±–µ—Ä–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –æ–∫–Ω–µ).

4. **–ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ (Fully Connected Layers)**:
   - –í –∫–æ–Ω—Ü–µ —Å–µ—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞.

5. **Dropout**:
   - –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏.

#### –ü—Ä–∏–º–µ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã:
\[
\text{–í—Ö–æ–¥ (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)} \rightarrow \text{–°–≤–µ—Ä—Ç–∫–∞} \rightarrow \text{ReLU} \rightarrow \text{–ü—É–ª–ª–∏–Ω–≥} \rightarrow \ldots \rightarrow \text{–ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π} \rightarrow \text{–í—ã—Ö–æ–¥ (–∫–ª–∞—Å—Å—ã)}.
\]

---

### 3. **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–µ—Ç–µ–π –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏**

1. **–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏**:
   - –°–≤–µ—Ä—Ç–∫–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É—á–∏—Ç—ã–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –ø–∏–∫—Å–µ–ª—è–º–∏.

2. **–ò–Ω–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫ —Å–¥–≤–∏–≥–∞–º**:
   - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é —Å–≤–µ—Ä—Ç–∫–∏ –∏ –ø—É–ª–ª–∏–Ω–≥–∞ –¥–µ–ª–∞–µ—Ç —Å–µ—Ç—å —É—Å—Ç–æ–π—á–∏–≤–æ–π –∫ –Ω–µ–±–æ–ª—å—à–∏–º —Å–¥–≤–∏–≥–∞–º –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏.

3. **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏–π**:
   - –ë–ª–∞–≥–æ–¥–∞—Ä—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Å–≤—è–∑–Ω–æ—Å—Ç–∏ (–∫–∞–∂–¥—ã–π –Ω–µ–π—Ä–æ–Ω –ø–æ–¥–∫–ª—é—á–µ–Ω —Ç–æ–ª—å–∫–æ –∫ —á–∞—Å—Ç–∏ –≤—Ö–æ–¥–∞), CNN —Ç—Ä–µ–±—É—é—Ç –º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏.

4. **–ò–µ—Ä–∞—Ä—Ö–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤**:
   - –ù–∏–∑–∫–∏–µ —Å–ª–æ–∏ –∏–∑—É—á–∞—é—Ç –ø—Ä–æ—Å—Ç—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∫—Ä–∞—è, —É–≥–ª—ã), –∞ –≤—ã—Å–æ–∫–∏–µ ‚Äî –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ (—Ñ–æ—Ä–º—ã, —Ç–µ–∫—Å—Ç—É—Ä—ã, –æ–±—ä–µ–∫—Ç—ã).

5. **–ü–µ—Ä–µ–Ω–æ—Å–∏–º–æ—Å—Ç—å**:
   - –û–±—É—á–µ–Ω–Ω—ã–µ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–µ—Ç–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω—ã –Ω–∞ –Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ (transfer learning).

---

### 5. **–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã**

1. **–°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–µ—Ç–∏ (CNN)** ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏.
2. **–°–≤–µ—Ä—Ç–∫–∏** –ø–æ–∑–≤–æ–ª—è—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤—ã–¥–µ–ª—è—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —É–º–µ–Ω—å—à–∞—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö.
3. CNN –æ–±–ª–∞–¥–∞—é—Ç –≤—ã—Å–æ–∫–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –±–ª–∞–≥–æ–¥–∞—Ä—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Å–≤—è–∑–Ω–æ—Å—Ç–∏ –∏ –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è–º.
4. PyTorch –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —É–¥–æ–±–Ω—ã–π –Ω–∞–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–∏—è —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–µ—Ç–µ–π.
"""

    else:
        code = """
### 4. **–ü—Ä–∏–º–µ—Ä —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π —Å–µ—Ç–∏ –≤ PyTorch**
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        # –ü–µ—Ä–≤—ã–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π: 3 –≤—Ö–æ–¥–Ω—ã—Ö –∫–∞–Ω–∞–ª–∞, 16 —Ñ–∏–ª—å—Ç—Ä–æ–≤, —è–¥—Ä–æ 3x3
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)
        # –í—Ç–æ—Ä–æ–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π: 16 –≤—Ö–æ–¥–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤, 32 —Ñ–∏–ª—å—Ç—Ä–∞
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π
        self.fc = nn.Linear(32 * 8 * 8, 10)  # –í—ã—Ö–æ–¥–Ω–æ–π —Ä–∞–∑–º–µ—Ä: 10 –∫–ª–∞—Å—Å–æ–≤

    def forward(self, x):
        # –°–≤–µ—Ä—Ç–∫–∞, –∞–∫—Ç–∏–≤–∞—Ü–∏—è, –ø—É–ª–ª–∏–Ω–≥
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2)  # –£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2)
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –ø–ª–æ—Å–∫–∏–π –≤–µ–∫—Ç–æ—Ä
        x = torch.flatten(x, 1)
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π
        x = self.fc(x)
        return x

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
model = SimpleCNN()

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ñ–µ–π–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
dummy_input = torch.randn(1, 3, 32, 32)  # –ú–∏–Ω–∏-–±–∞—Ç—á –∏–∑ 1 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è 3x32x32
output = model(dummy_input)
print("Output shape:", output.shape)
"""

    pyperclip.copy(code)


def conv_pool(idx: int = 0):
    if idx == 0:
        code = """
### 16. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–π –ò–ù–° –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –∏ —Å–∂–∏–º–∞—é—â–∏–µ —Å–ª–æ–∏ –≤ PyTorch

---

### 1. **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–π –ò–ù–° –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π**

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ (CNN) –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤:

#### –û—Å–Ω–æ–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã:
1. **–°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏ (Convolution Layers)**:
   - –ò–∑–≤–ª–µ–∫–∞—é—Ç –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
   - –§–∏–ª—å—Ç—Ä—ã (—è–¥—Ä–∞ —Å–≤–µ—Ä—Ç–∫–∏) –æ–±—É—á–∞—é—Ç—Å—è –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –∫—Ä–∞—ë–≤, —Ç–µ–∫—Å—Ç—É—Ä, —Ñ–æ—Ä–º –∏ –¥—Ä—É–≥–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

2. **–°–ª–æ–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (Activation Layers)**:
   - –î–æ–±–∞–≤–ª—è—é—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ –∏–∑—É—á–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏.
   - –û–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è ReLU –∏–ª–∏ –µ–≥–æ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏.

3. **–°–∂–∏–º–∞—é—â–∏–µ —Å–ª–æ–∏ (Pooling Layers)**:
   - –£–º–µ–Ω—å—à–∞—é—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ, —Å–æ—Ö—Ä–∞–Ω—è—è –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏.
   - –ù–∞–∏–±–æ–ª–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã:
     - Max Pooling (–≤—ã–±–∏—Ä–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –∫–∞–∂–¥–æ–º –æ–∫–Ω–µ).
     - Average Pooling (–≤—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ).

4. **–ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ (Fully Connected Layers)**:
   - –ü—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç –¥–≤—É–º–µ—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.

5. **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è**:
   - Dropout –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è.

#### –ü—Ä–∏–º–µ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–µ—Ç–∏:
\[
\text{–í—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (RGB, HxW)} \rightarrow \text{–°–≤–µ—Ä—Ç–∫–∞ + ReLU} \rightarrow \text{–ü—É–ª–ª–∏–Ω–≥} \rightarrow \ldots \rightarrow \text{–ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π} \rightarrow \text{–ö–ª–∞—Å—Å—ã}
\]

---

### 2. **–°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏ (Convolution Layers)**

#### –§—É–Ω–∫—Ü–∏—è —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–≥–æ —Å–ª–æ—è:
- –ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ñ–∏–ª—å—Ç—Ä (—è–¥—Ä–æ) —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é, –ø–µ—Ä–µ–º–µ—â–∞—è –µ–≥–æ –ø–æ –≤—Å–µ–π –ø–ª–æ—â–∞–¥–∏.
- –§–∏–ª—å—Ç—Ä—ã –æ–±—É—á–∞—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

#### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–≥–æ —Å–ª–æ—è:
1. **`in_channels`**: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–¥–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 3 –¥–ª—è RGB-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è).
2. **`out_channels`**: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–ª—å—Ç—Ä–æ–≤ (–∫–∞–Ω–∞–ª–æ–≤ –≤—ã—Ö–æ–¥–∞).
3. **`kernel_size`**: —Ä–∞–∑–º–µ—Ä —Ñ–∏–ª—å—Ç—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, \(3 \times 3\)).
4. **`stride`**: —à–∞–≥ —Ñ–∏–ª—å—Ç—Ä–∞ (–æ–±—ã—á–Ω–æ 1).
5. **`padding`**: –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω—É–ª–µ–π –≤–æ–∫—Ä—É–≥ –≥—Ä–∞–Ω–∏—Ü –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.

#### –ü—Ä–∏–º–µ—Ä —Å–æ–∑–¥–∞–Ω–∏—è —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–≥–æ —Å–ª–æ—è –≤ PyTorch:
```python
import torch
import torch.nn as nn

conv = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)
input_image = torch.randn(1, 3, 32, 32)  # 1 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, 3 –∫–∞–Ω–∞–ª–∞, —Ä–∞–∑–º–µ—Ä 32x32
output = conv(input_image)
print("Output shape:", output.shape)  # –†–µ–∑—É–ª—å—Ç–∞—Ç: [1, 16, 32, 32]
```

---

### 3. **–°–∂–∏–º–∞—é—â–∏–µ —Å–ª–æ–∏ (Pooling Layers)**

#### –§—É–Ω–∫—Ü–∏—è —Å–∂–∏–º–∞—é—â–∏—Ö —Å–ª–æ–µ–≤:
- –£–º–µ–Ω—å—à–∞—é—Ç —Ä–∞–∑–º–µ—Ä –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω—è—è –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏.
- –°–≥–ª–∞–∂–∏–≤–∞—é—Ç —à—É–º—ã –∏ —É–ª—É—á—à–∞—é—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –º–æ–¥–µ–ª–∏.

#### –í–∏–¥—ã –ø—É–ª–ª–∏–Ω–≥–∞:
1. **Max Pooling**:
   - –í—ã–±–∏—Ä–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –∫–∞–∂–¥–æ–≥–æ –æ–∫–Ω–∞.
2. **Average Pooling**:
   - –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –∫–∞–∂–¥–æ–≥–æ –æ–∫–Ω–∞.

#### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—É–ª–ª–∏–Ω–≥–∞:
1. **`kernel_size`**: —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞.
2. **`stride`**: —à–∞–≥ –æ–∫–Ω–∞.
3. **`padding`**: –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω—É–ª–µ–π –≤–æ–∫—Ä—É–≥ –≥—Ä–∞–Ω–∏—Ü.

#### –ü—Ä–∏–º–µ—Ä —Å–æ–∑–¥–∞–Ω–∏—è MaxPooling —Å–ª–æ—è:
```python
pool = nn.MaxPool2d(kernel_size=2, stride=2)  # –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ 2x2, —à–∞–≥ 2
output = pool(output)
print("Output shape after pooling:", output.shape)  # –†–µ–∑—É–ª—å—Ç–∞—Ç: [1, 16, 16, 16]
```

---

### 5. **–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã**

1. **–°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏ (Convolution Layers)**:
   - –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–∫—Ä–∞—ë–≤, —Ç–µ–∫—Å—Ç—É—Ä, —Ñ–æ—Ä–º).
   - –ü–∞—Ä–∞–º–µ—Ç—Ä—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ `kernel_size` –∏ `stride`, –∑–∞–¥–∞—é—Ç, –∫–∞–∫ —Ñ–∏–ª—å—Ç—Ä –ø–µ—Ä–µ–º–µ—â–∞–µ—Ç—Å—è –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.

2. **–°–∂–∏–º–∞—é—â–∏–µ —Å–ª–æ–∏ (Pooling Layers)**:
   - –£–º–µ–Ω—å—à–∞—é—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö, —Å–æ—Ö—Ä–∞–Ω—è—è –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.
   - MaxPooling ‚Äî –Ω–∞–∏–±–æ–ª–µ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—ë–Ω–Ω—ã–π –≤—ã–±–æ—Ä.

3. **–ü—Ä–∏–º–µ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã CNN**:
   - –°–æ—Å—Ç–æ–∏—Ç –∏–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö –∏ –ø—É–ª–ª–∏–Ω–≥–æ–≤—ã—Ö —Å–ª–æ–µ–≤, –∑–∞ –∫–æ—Ç–æ—Ä—ã–º–∏ —Å–ª–µ–¥—É—é—Ç –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.

4. **PyTorch –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≥–æ—Ç–æ–≤—ã–µ —Å–ª–æ–∏**, –∫–æ—Ç–æ—Ä—ã–µ –ª–µ–≥–∫–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–µ—Ç–µ–π.        
"""
    else:
        code = """
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        # –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
        
        # –ü—É–ª–ª–∏–Ω–≥
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        self.fc1 = nn.Linear(32 * 8 * 8, 128)  # 32 –∫–∞–Ω–∞–ª–∞, 8x8 —Ä–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ –¥–≤—É—Ö –ø—É–ª–ª–∏–Ω–≥–æ–≤
        self.fc2 = nn.Linear(128, 10)  # 10 –∫–ª–∞—Å—Å–æ–≤

    def forward(self, x):
        # –ü–µ—Ä–≤—ã–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫
        x = self.pool(torch.relu(self.conv1(x)))
        
        # –í—Ç–æ—Ä–æ–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫
        x = self.pool(torch.relu(self.conv2(x)))
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –ø–ª–æ—Å–∫–∏–π –≤–µ–∫—Ç–æ—Ä
        x = torch.flatten(x, 1)
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
model = SimpleCNN()

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö
dummy_input = torch.randn(1, 3, 32, 32)  # 1 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 3x32x32
output = model(dummy_input)
print("Output shape:", output.shape)  # –†–µ–∑—É–ª—å—Ç–∞—Ç: [1, 10]
"""

    pyperclip.copy(code)


def cnn_ex(idx: int = 0):
    if idx == 0:
        code = """
### 17. –ü—Ä–∏–µ–º—ã –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

---

–û–±—É—á–µ–Ω–∏–µ –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –Ω–∞ –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ‚Äî —Å–ª–æ–∂–Ω–∞—è –∑–∞–¥–∞—á–∞ –∏–∑-–∑–∞ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∏—Å–∫–∞ **–ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è** –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –û–¥–Ω–∞–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –ø—Ä–∏–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ–≤—ã—Å–∏—Ç—å —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –º–æ–¥–µ–ª–∏.

---

### 1. **–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (Data Augmentation)**

–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ–º–æ–≥–∞–µ—Ç –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É, —Å–æ–∑–¥–∞–≤–∞—è –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö. –≠—Ç–æ —Å–Ω–∏–∂–∞–µ—Ç —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è.

#### –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–µ—Ç–æ–¥—ã:
1. **–ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏**:
   - –ü–æ–≤–æ—Ä–æ—Ç—ã, —Å–¥–≤–∏–≥–∏, –æ—Ç—Ä–∞–∂–µ–Ω–∏—è, –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ.
2. **–¶–≤–µ—Ç–æ–≤—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è**:
   - –ò–∑–º–µ–Ω–µ–Ω–∏–µ —è—Ä–∫–æ—Å—Ç–∏, –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç–∏, –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç–∏.
3. **–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞**:
   - –ì–∞—É—Å—Å–æ–≤—Å–∫–∏–π —à—É–º, —Å–ª—É—á–∞–π–Ω–æ–µ —Å—Ç–∏—Ä–∞–Ω–∏–µ (Random Erasing).
4. **Random Cropping**:
   - –°–ª—É—á–∞–π–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ –∏ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.

#### –ü—Ä–∏–º–µ—Ä –≤ PyTorch:
```python
from torchvision import transforms

transform = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(degrees=15),
    transforms.RandomResizedCrop(size=32, scale=(0.8, 1.0)),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.ToTensor()
])
```

---

### 2. **Transfer Learning (–¢—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ)**

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, ImageNet) –ø–æ–º–æ–≥–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö.

#### –ü–æ–¥—Ö–æ–¥—ã:
1. **–§–∏–∫—Å–∞—Ü–∏—è –≤–µ—Å–æ–≤ (Fine-tuning)**:
   - –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞—é—Ç—Å—è –≤–µ—Å–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö —Å–ª–æ–µ–≤, –∏–∑–º–µ–Ω—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤–µ—Ä—Ö–Ω–∏–µ (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ) —Å–ª–æ–∏.
2. **–ü–æ–ª–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ (Full training)**:
   - –í—Å–µ –≤–µ—Å–∞ —Å–µ—Ç–∏ –¥–æ–æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ –Ω–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö.

#### –ü—Ä–∏–º–µ—Ä –≤ PyTorch:
```python
from torchvision import models

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
model = models.resnet18(pretrained=True)

# –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –≤–µ—Å–∞ –≤—Å–µ—Ö —Å–ª–æ–µ–≤
for param in model.parameters():
    param.requires_grad = False

# –ú–µ–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π –ø–æ–¥ —Å–≤–æ—é –∑–∞–¥–∞—á—É
model.fc = nn.Linear(512, 10)  # 10 –∫–ª–∞—Å—Å–æ–≤
```

---

### 3. **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è**

–î–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –≤ —É—Å–ª–æ–≤–∏—è—Ö –º–∞–ª–æ–≥–æ –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –º–µ—Ç–æ–¥—ã —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏.

#### –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏:
1. **Dropout**:
   - –ò—Å–∫–ª—é—á–µ–Ω–∏–µ —Å–ª—É—á–∞–π–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è.
   ```python
   nn.Dropout(p=0.5)
   ```
   
2. **L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (Weight Decay)**:
   - –î–æ–±–∞–≤–ª—è–µ—Ç —à—Ç—Ä–∞—Ñ –∑–∞ –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤–µ—Å–æ–≤ –≤ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å.
   ```python
   optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)
   ```

3. **–†–∞–Ω–Ω–µ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è (Early Stopping)**:
   - –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è, –µ—Å–ª–∏ –æ—à–∏–±–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ –ø–µ—Ä–µ—Å—Ç–∞–µ—Ç —É–º–µ–Ω—å—à–∞—Ç—å—Å—è.

---

### 4. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–µ–±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π**

–ì–ª—É–±–æ–∫–∏–µ –º–æ–¥–µ–ª–∏ —Ç—Ä–µ–±—É—é—Ç –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö. –î–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã.

#### –ü—Ä–∏–º–µ—Ä—ã:
- MobileNet, SqueezeNet, ResNet-18.
- –°–æ–∑–¥–∞–Ω–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –Ω–µ–±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Å —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.

---

### 5. **–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è**

–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ–º–æ–≥–∞–µ—Ç –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ–±–æ–ª—å—à–æ–π –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö. –ù–∞–ø—Ä–∏–º–µ—Ä:
- –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ \(k\) —Ñ–æ–ª–¥–æ–≤.
- –û–±—É—á–µ–Ω–∏–µ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –∫–∞–∂–¥–æ–º —Ñ–æ–ª–¥–µ, —á–µ—Ä–µ–¥—É—è –¥–∞–Ω–Ω—ã–µ.

---

### 6. **–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö**

–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ GAN –∏–ª–∏ VAE, –º–æ–≥—É—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø–æ—Ö–æ–∂–∏—Ö –Ω–∞ –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É.

---

### 7. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö**

1. **Pretraining**:
   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.
   - –ù–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ—Ç–∫—Ä—ã—Ç—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–æ–π –∂–µ –æ–±–ª–∞—Å—Ç–∏.

2. **Weakly Supervised Learning**:
   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å –º–µ–Ω–µ–µ —Ç–æ—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π.

---

### 8. **–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö**

–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–º–æ–≥–∞–µ—Ç —É–ª—É—á—à–∏—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è:
- –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –ø–∏–∫—Å–µ–ª–µ–π –≤ –¥–∏–∞–ø–∞–∑–æ–Ω \([0, 1]\).
- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ —Å—Ä–µ–¥–Ω–µ–º—É –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—é –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞.
```python
transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
```

---

### 9. **–≠–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π**

–≠–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∏ —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π, –∫–æ–º–±–∏–Ω–∏—Ä—É—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π.

#### –ü—Ä–∏–º–µ—Ä:
1. –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π —Å —Ä–∞–∑–Ω—ã–º–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è–º–∏.
2. –£—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π.

---

### –í—ã–≤–æ–¥—ã

1. **–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö** –∏ **—Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ** ‚Äî –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–µ–±–æ–ª—å—à–∏–º–∏ –Ω–∞–±–æ—Ä–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
2. –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –∏ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ–º–æ–≥–∞—é—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è.
3. PyTorch –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ—â–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —ç—Ç–∏—Ö –º–µ—Ç–æ–¥–æ–≤.
"""
    else:
        code = """
### –ü—Ä–∏–º–µ—Ä –ø–æ–ª–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –≤ PyTorch
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torchvision.models import resnet18

# –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(degrees=15),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
train_data = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)
train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)

# –ú–æ–¥–µ–ª—å —Å —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º
model = resnet18(pretrained=True)
model.fc = nn.Linear(512, 10)  # CIFAR-10: 10 –∫–ª–∞—Å—Å–æ–≤

# –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)

# –û–±—É—á–µ–Ω–∏–µ
for epoch in range(10):
    model.train()
    for images, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")
"""

    pyperclip.copy(code)


def cnn_cls(idx: int = 0):
    if idx == 0:
        code = """
### 18. –°—Ö–µ–º–∞ —Ä–∞–±–æ—Ç—ã —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π —Å–µ—Ç–∏, –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å–≤–µ—Ä—Ç–∫–∏, –ø—É–ª–ª–∏–Ω–≥–∞ –∏ –æ–±—â–∏–π –≤–∏–¥ —Å–µ—Ç–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

---

### 1. **–°—Ö–µ–º–∞ —Ä–∞–±–æ—Ç—ã —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π —Å–µ—Ç–∏**

–°–≤–µ—Ä—Ç–æ—á–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å (CNN) ‚Äî —ç—Ç–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å —Å–µ—Ç—á–∞—Ç–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –û–Ω–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ—ç—Ç–∞–ø–Ω–æ, –∏–∑–≤–ª–µ–∫–∞—è –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ä–∞–∑–ª–∏—á–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏.

#### –û—Å–Ω–æ–≤–Ω—ã–µ —ç—Ç–∞–ø—ã —Ä–∞–±–æ—Ç—ã:
1. **–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤**:
   - –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏ (Convolution Layers) –ø—Ä–∏–º–µ–Ω—è—é—Ç —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–∫—Ä–∞—ë–≤, —Ç–µ–∫—Å—Ç—É—Ä, —Ñ–æ—Ä–º).

2. **–£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏**:
   - –°–∂–∏–º–∞—é—â–∏–µ —Å–ª–æ–∏ (Pooling Layers) —É–º–µ–Ω—å—à–∞—é—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω—è—è –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —É—Å—Ç—Ä–∞–Ω—è—è —à—É–º.

3. **–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤**:
   - –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–ª–æ–∏ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç –∏—Ö –≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é.

---

### 2. **–û–ø–µ—Ä–∞—Ü–∏–∏ —Å–≤–µ—Ä—Ç–∫–∏**

–°–≤–µ—Ä—Ç–∫–∞ ‚Äî —ç—Ç–æ –æ—Å–Ω–æ–≤–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è –≤ CNN, –∫–æ—Ç–æ—Ä–∞—è –≤—ã–¥–µ–ª—è–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.

#### –ú–µ—Ö–∞–Ω–∏–∑–º:
1. –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ñ–∏–ª—å—Ç—Ä (—è–¥—Ä–æ) ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, \(3 \times 3\) –∏–ª–∏ \(5 \times 5\)).
2. –§–∏–ª—å—Ç—Ä –ø–µ—Ä–µ–º–µ—â–∞–µ—Ç—Å—è –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é —Å –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–º —à–∞–≥–æ–º (\(stride\)), –≤—ã—á–∏—Å–ª—è—è —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏ –æ–±–ª–∞—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
3. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –≤ –Ω–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ ‚Äî **–∫–∞—Ä—Ç—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (feature map)**.

#### –§–æ—Ä–º—É–ª–∞ —Å–≤–µ—Ä—Ç–∫–∏:
–î–ª—è –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è \(I\) –∏ —Ñ–∏–ª—å—Ç—Ä–∞ \(K\):
\[
O(x, y) = \sum_{i=1}^{m} \sum_{j=1}^{n} I(x+i, y+j) \cdot K(i, j)
\]

#### –ü—Ä–∏–º–µ—Ä —Å–≤–µ—Ä—Ç–∫–∏ –≤ PyTorch:
```python
import torch
import torch.nn as nn

# –í—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: 1 –∫–∞–Ω–∞–ª, 32x32
input_image = torch.randn(1, 1, 32, 32)

# –°–≤–µ—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π: 1 –≤—Ö–æ–¥–Ω–æ–π –∫–∞–Ω–∞–ª, 16 –≤—ã—Ö–æ–¥–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤, —è–¥—Ä–æ 3x3
conv = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–≤–µ—Ä—Ç–∫–∏
output = conv(input_image)
print("Output shape:", output.shape)  # –†–µ–∑—É–ª—å—Ç–∞—Ç: [1, 16, 32, 32]
```

---

### 3. **–û–ø–µ—Ä–∞—Ü–∏–∏ –ø—É–ª–ª–∏–Ω–≥–∞ (Pooling)**

–ü—É–ª–ª–∏–Ω–≥ —É–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —Å–æ—Ö—Ä–∞–Ω—è—è –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.

#### –í–∏–¥—ã –ø—É–ª–ª–∏–Ω–≥–∞:
1. **Max Pooling**:
   - –í—ã–±–∏—Ä–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –æ–∫–Ω–∞.
2. **Average Pooling**:
   - –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –æ–∫–Ω–µ.

#### –ü—Ä–∏–º–µ—Ä Max Pooling:
```python
# MaxPooling: —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ 2x2, —à–∞–≥ 2
pool = nn.MaxPool2d(kernel_size=2, stride=2)

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—É–ª–ª–∏–Ω–≥–∞
pooled_output = pool(output)
print("Pooled Output shape:", pooled_output.shape)  # –†–µ–∑—É–ª—å—Ç–∞—Ç: [1, 16, 16, 16]
```

---

### 4. **–û–±—â–∏–π –≤–∏–¥ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π —Å–µ—Ç–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π**

CNN —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —á–µ—Ä–µ–¥—É—é—â–∏—Ö—Å—è —Å–ª–æ–µ–≤ —Å–≤–µ—Ä—Ç–∫–∏ –∏ –ø—É–ª–ª–∏–Ω–≥–∞, –∑–∞ –∫–æ—Ç–æ—Ä—ã–º–∏ —Å–ª–µ–¥—É—é—Ç –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏.

#### –°—Ç—Ä—É–∫—Ç—É—Ä–∞:
1. **–í—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ** (\(H \times W \times C\)).
2. **–°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –±–ª–æ–∫–∏**:
   - –°–ª–æ–π —Å–≤–µ—Ä—Ç–∫–∏ (\(Conv\)) + –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å (\(ReLU\)) + –ø—É–ª–ª–∏–Ω–≥ (\(Pooling\)).
3. **–ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π –±–ª–æ–∫**:
   - –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä.
   - –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ (\(Fully Connected\)).
4. **–í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π**:
   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ —Ä–∞–≤–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∫–ª–∞—Å—Å–æ–≤.

---

### 6. **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ CNN –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π**

1. **–õ–æ–∫–∞–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤**:
   - –°–≤–µ—Ä—Ç–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏–∑–≤–ª–µ–∫–∞—é—Ç –ª–æ–∫–∞–ª—å–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.

2. **–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤**:
   - –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–º–∏ —Å–ª–æ—è–º–∏, —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏ —Ç—Ä–µ–±—É—é—Ç –º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.

3. **–ò–Ω–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è–º**:
   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—É–ª–ª–∏–Ω–≥–∞ –¥–µ–ª–∞–µ—Ç CNN —É—Å—Ç–æ–π—á–∏–≤–æ–π –∫ —Å–¥–≤–∏–≥–∞–º –∏ –º–∞—Å—à—Ç–∞–±–∞–º –æ–±—ä–µ–∫—Ç–æ–≤.

4. **–ò–µ—Ä–∞—Ä—Ö–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤**:
   - –ù–∏–∑–∫–∏–µ —Å–ª–æ–∏ –∏–∑—É—á–∞—é—Ç –ø—Ä–æ—Å—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–∫—Ä–∞—è, —Ç–µ–∫—Å—Ç—É—Ä—ã), –≤—ã—Å–æ–∫–∏–µ ‚Äî —Å–ª–æ–∂–Ω—ã–µ (—Ñ–æ—Ä–º—ã, –æ–±—ä–µ–∫—Ç—ã).

---

### –í—ã–≤–æ–¥

- CNN —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏—è —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö –∏ –ø—É–ª–ª–∏–Ω–≥–æ–≤—ã—Ö —Å–ª–æ–µ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∏–∑–≤–ª–µ–∫–∞—é—Ç –∏ —Å–∂–∏–º–∞—é—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏.
- –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.
- PyTorch –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —É–¥–æ–±–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –≤—Å–µ—Ö —ç—Ç–∞–ø–æ–≤ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è CNN.        
"""

    else:
        code = """
#### –ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è CNN:
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        # –ü–µ—Ä–≤—ã–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # –í—Ç–æ—Ä–æ–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        self.fc1 = nn.Linear(32 * 8 * 8, 128)  # –†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ –¥–≤—É—Ö –ø—É–ª–ª–∏–Ω–≥–æ–≤: 8x8
        self.fc2 = nn.Linear(128, 10)  # 10 –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

    def forward(self, x):
        # –ü–µ—Ä–≤—ã–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫
        x = F.relu(self.conv1(x))
        x = self.pool1(x)
        
        # –í—Ç–æ—Ä–æ–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫
        x = F.relu(self.conv2(x))
        x = self.pool2(x)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –ø–ª–æ—Å–∫–∏–π –≤–µ–∫—Ç–æ—Ä
        x = torch.flatten(x, 1)
        
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
model = SimpleCNN()

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö
dummy_input = torch.randn(1, 3, 32, 32)  # 1 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (RGB), 32x32
output = model(dummy_input)
print("Output shape:", output.shape)  # –†–µ–∑—É–ª—å—Ç–∞—Ç: [1, 10]
"""
    pyperclip.copy(code)


def kinds_rl(idx: int = 0):
    if idx == 0:
        code = """
### 19. –í–∏–¥—ã –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º

---

### 1. **–û—Å–Ω–æ–≤–Ω—ã–µ –≤–∏–¥—ã –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è**

–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Ü–µ–ª–∏ –æ–±—É—á–µ–Ω–∏—è. –û—Å–Ω–æ–≤–Ω—ã–µ –≤–∏–¥—ã:

---

#### 1.1 **–û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º (Supervised Learning)**

- **–¶–µ–ª—å**: –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö —Å –º–µ—Ç–∫–∞–º–∏ (–≤—Ö–æ–¥-–≤—ã—Ö–æ–¥), —á—Ç–æ–±—ã –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –º–µ—Ç–∫–∏ –¥–ª—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
- **–ü—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á**:
  - **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è**: –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–ø–∞–º/–Ω–µ —Å–ø–∞–º).
  - **–†–µ–≥—Ä–µ—Å—Å–∏—è**: –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ü–µ–Ω—ã –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏).

**–ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏**:
\[
\text{–î–∞–Ω–æ: } \{(x_i, y_i)\}_{i=1}^n \quad \text{–ù–∞–π—Ç–∏: } f(x) \approx y
\]
–≥–¥–µ \(x_i\) ‚Äî –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, \(y_i\) ‚Äî –º–µ—Ç–∫–∏.

**–ü—Ä–∏–º–µ—Ä**:
- –î–∞–Ω–Ω—ã–µ: \(x\) ‚Äî –ø–ª–æ—â–∞–¥—å –¥–æ–º–∞, \(y\) ‚Äî —Ü–µ–Ω–∞.
- –¶–µ–ª—å: –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å —Ü–µ–Ω—É –¥–ª—è –Ω–æ–≤–æ–≥–æ –¥–æ–º–∞.

---

#### 1.2 **–û–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è (Unsupervised Learning)**

- **–¶–µ–ª—å**: –û–±–Ω–∞—Ä—É–∂–∏—Ç—å —Å–∫—Ä—ã—Ç—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏–ª–∏ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –º–µ—Ç–æ–∫.
- **–ü—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á**:
  - **–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è**: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –≥—Ä—É–ø–ø—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤).
  - **–°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏**: –£–º–µ–Ω—å—à–µ–Ω–∏–µ —á–∏—Å–ª–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, PCA, t-SNE).

**–ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏**:
\[
\text{–î–∞–Ω–æ: } \{x_i\}_{i=1}^n \quad \text{–ù–∞–π—Ç–∏: } \text{—Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏–ª–∏ –≥—Ä—É–ø–ø—ã –≤ –¥–∞–Ω–Ω—ã—Ö.}
\]

**–ü—Ä–∏–º–µ—Ä**:
- –î–∞–Ω–Ω—ã–µ: –ü–æ–≤–µ–¥–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –Ω–∞ —Å–∞–π—Ç–µ.
- –¶–µ–ª—å: –†–∞–∑–¥–µ–ª–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç—ã.

---

#### 1.3 **–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (Reinforcement Learning)**

- **–¶–µ–ª—å**: –ù–∞—É—á–∏—Ç—å –∞–≥–µ–Ω—Ç–∞ –ø—Ä–∏–Ω–∏–º–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è, –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É—è –Ω–∞–≥—Ä–∞–¥—É.
- **–ü—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á**:
  - –ò–≥—Ä–∞ –≤ —à–∞—Ö–º–∞—Ç—ã –∏–ª–∏ –≥–æ.
  - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–∞–º–∏.
  - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–∞—Ä—à—Ä—É—Ç–æ–≤.

**–ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏**:
- –ê–≥–µ–Ω—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç —Å –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥–æ–π, –≤—ã–ø–æ–ª–Ω—è—è –¥–µ–π—Å—Ç–≤–∏—è \(a_t\) –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ \(s_t\), —á—Ç–æ–±—ã –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–∂–∏–¥–∞–µ–º—É—é —Å–æ–≤–æ–∫—É–ø–Ω—É—é –Ω–∞–≥—Ä–∞–¥—É.

**–§–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ MDP (Markov Decision Process)**:
1. \(S\): –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Å–æ—Å—Ç–æ—è–Ω–∏–π.
2. \(A\): –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–µ–π—Å—Ç–≤–∏–π.
3. \(P(s_{t+1} | s_t, a_t)\): –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–µ—Ä–µ—Ö–æ–¥–∞.
4. \(R(s_t, a_t)\): –ù–∞–≥—Ä–∞–¥–∞.
5. \(\gamma\): –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.

–ó–∞–¥–∞—á–∞:
\[
\text{–ù–∞–π—Ç–∏: } \pi^* = \arg\max_{\pi} \mathbb{E} \left[ \sum_{t=0}^\infty \gamma^t R(s_t, a_t) \right]
\]
–≥–¥–µ \(\pi\) ‚Äî –ø–æ–ª–∏—Ç–∏–∫–∞ –∞–≥–µ–Ω—Ç–∞.

---

#### 1.4 **–ü–æ–ª—É–æ–±—É—á–µ–Ω–∏–µ (Semi-Supervised Learning)**

- **–¶–µ–ª—å**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ–±–æ–ª—å—à—É—é —á–∞—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö —Å –º–µ—Ç–∫–∞–º–∏ –∏ –±–æ–ª—å—à—É—é —á–∞—Å—Ç—å –±–µ–∑ –º–µ—Ç–æ–∫.
- **–ü—Ä–∏–º–µ—Ä –∑–∞–¥–∞—á–∏**:
  - –†–∞–∑–º–µ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤, –≥–¥–µ –±–æ–ª—å—à–∞—è —á–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–æ–≤ –Ω–µ –∏–º–µ–µ—Ç –º–µ—Ç–æ–∫.

---

#### 1.5 **–û–±—É—á–µ–Ω–∏–µ —Å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ–º (Self-Supervised Learning)**

- **–¶–µ–ª—å**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–¥–∞—á –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è.
- **–ü—Ä–∏–º–µ—Ä –∑–∞–¥–∞—á–∏**:
  - –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ GPT).

---

### 2. **–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º**

---

#### 2.1 **–ü—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á**

1. **–ò–≥—Ä—ã**:
   - –ü—Ä–∏–º–µ—Ä: AlphaGo, –æ–±—É—á–µ–Ω–∏–µ –∏–≥—Ä–µ –≤ —à–∞—Ö–º–∞—Ç—ã.
2. **–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–∞–º–∏**:
   - –ü—Ä–∏–º–µ—Ä: –û–±—É—á–µ–Ω–∏–µ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏.
3. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–∞—Ä—à—Ä—É—Ç–æ–≤**:
   - –ü—Ä–∏–º–µ—Ä: –í—ã–±–æ—Ä –º–∞—Ä—à—Ä—É—Ç–∞ –¥–ª—è –±–µ—Å–ø–∏–ª–æ—Ç–Ω–∏–∫–∞.
4. **–≠–∫–æ–Ω–æ–º–∏–∫–∞**:
   - –ü—Ä–∏–º–µ—Ä: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ—Ä—Ç—Ñ–µ–ª—è.

---

#### 2.2 **–ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º**

1. **–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è**:
   - –ê–≥–µ–Ω—Ç –Ω–∞—á–∏–Ω–∞–µ—Ç –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ \(s_0\).
2. **–¶–∏–∫–ª –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è**:
   - –ê–≥–µ–Ω—Ç –≤—ã–±–∏—Ä–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ \(a_t\) –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª–∏—Ç–∏–∫–∏ \(\pi(s_t)\).
   - –°—Ä–µ–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ–≤—É—é –Ω–∞–≥—Ä–∞–¥—É \(R(s_t, a_t)\) –∏ –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ \(s_{t+1}\).
3. **–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ**:
   - –ê–≥–µ–Ω—Ç –æ–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é \(\pi(s)\), —á—Ç–æ–±—ã —É–ª—É—á—à–∏—Ç—å –æ–∂–∏–¥–∞–µ–º—É—é –Ω–∞–≥—Ä–∞–¥—É.

---

#### 2.3 **–ú–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è**

1. **Q-Learning**:
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Q-—Ç–∞–±–ª–∏—Ü—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ "–∫–∞—á–µ—Å—Ç–≤–∞" –¥–µ–π—Å—Ç–≤–∏–π:
     \[
     Q(s_t, a_t) = R(s_t, a_t) + \gamma \max_a Q(s_{t+1}, a)
     \]

2. **Deep Q-Learning**:
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –¥–ª—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏ Q-—Ñ—É–Ω–∫—Ü–∏–∏.

3. **Policy Gradient**:
   - –ù–∞—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é \(\pi(a|s)\), –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É—è –Ω–∞–≥—Ä–∞–¥—É:
     \[
     J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \sum_t R(s_t, a_t) \right]
     \]

---

### –í—ã–≤–æ–¥—ã

1. **–í–∏–¥—ã –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è**:
   - –û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º, –±–µ–∑ —É—á–∏—Ç–µ–ª—è, —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –ø–æ–ª—É–æ–±—É—á–µ–Ω–∏–µ –∏ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ.
2. **–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º**:
   - –ê–≥–µ–Ω—Ç –æ–±—É—á–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å–æ —Å—Ä–µ–¥–æ–π, –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É—è —Å–æ–≤–æ–∫—É–ø–Ω—É—é –Ω–∞–≥—Ä–∞–¥—É.
3. **–ú–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è**:
   - Q-Learning, Policy Gradient, Deep Q-Learning.
4. PyTorch –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º.        
"""
    else:
        code = """
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# –ü—Ä–∏–º–µ—Ä Q-–æ–±—É—á–µ–Ω–∏—è —Å –ø—Ä–æ—Å—Ç–æ–π —Å—Ä–µ–¥–æ–π

# –°—Ä–µ–¥–∞: –¥–≤–∏–∂–µ–Ω–∏–µ –≤–¥–æ–ª—å –ª–∏–Ω–∏–∏
states = 10
actions = 2  # –í–ª–µ–≤–æ –∏–ª–∏ –≤–ø—Ä–∞–≤–æ
q_table = np.zeros((states, actions))

# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
learning_rate = 0.1
gamma = 0.99
epsilon = 0.1
episodes = 1000

# Q-–æ–±—É—á–µ–Ω–∏–µ
for episode in range(episodes):
    state = np.random.randint(0, states)
    done = False
    
    while not done:
        if np.random.rand() < epsilon:
            action = np.random.randint(0, actions)  # –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
        else:
            action = np.argmax(q_table[state])  # –≠–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è
        next_state = state + (1 if action == 1 else -1)
        next_state = max(0, min(next_state, states - 1))
        reward = 1 if next_state == states - 1 else -0.1
        done = next_state == states - 1

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Q-—Ç–∞–±–ª–∏—Ü—ã
        q_table[state, action] += learning_rate * (
            reward + gamma * np.max(q_table[next_state]) - q_table[state, action]
        )
        state = next_state

print("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. Q-—Ç–∞–±–ª–∏—Ü–∞:")
print(q_table)
"""
    pyperclip.copy(code)


def rl_policy(idx: int = 0):
    if idx == 0:
        code = """
### 20. –ü–æ–¥—Ö–æ–¥—ã –∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤ RL, –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ, –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ, Q- –∏ V-—Ñ—É–Ω–∫—Ü–∏–∏, —É—Ä–∞–≤–Ω–µ–Ω–∏–µ –ë–µ–ª–ª–º–∞–Ω–∞

---

### 1. **–ü–æ–¥—Ö–æ–¥—ã –∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤ RL**

–°—Ç—Ä–∞—Ç–µ–≥–∏—è (policy, \(\pi\)) –≤ Reinforcement Learning –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫–∞–∫–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –∞–≥–µ–Ω—Ç –≤—ã–±–∏—Ä–∞–µ—Ç –≤ –∫–∞–∂–¥–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏.

#### –í–∏–¥—ã —Å—Ç—Ä–∞—Ç–µ–≥–∏–π:
1. **–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è**:
   - –ê–≥–µ–Ω—Ç –≤—Å–µ–≥–¥–∞ –≤—ã–±–∏—Ä–∞–µ—Ç –æ–¥–Ω–æ –∏ —Ç–æ –∂–µ –¥–µ–π—Å—Ç–≤–∏–µ \(a\) –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ \(s\).
   - –§–æ—Ä–º–∞–ª—å–Ω–æ: \(\pi(s) = a\).

2. **–°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è**:
   - –ê–≥–µ–Ω—Ç –≤—ã–±–∏—Ä–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ —Å –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é.
   - –§–æ—Ä–º–∞–ª—å–Ω–æ: \(\pi(a|s) = P(a|s)\), –≥–¥–µ \(P(a|s)\) ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—ã–±–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏—è \(a\) –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ \(s\).

---

### 2. **–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏ –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è**

#### 2.1. –í–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ
–í–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ \(R(s, a)\) ‚Äî —á–∏—Å–ª–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∞—è, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ–π—Å—Ç–≤–∏—è \(a\) –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ \(s\).

- –ú–≥–Ω–æ–≤–µ–Ω–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ: \(R_t = R(s_t, a_t)\).
- –ü—Ä–∏–º–µ—Ä: –ï—Å–ª–∏ –∞–≥–µ–Ω—Ç –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ü–µ–ª–∏, –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ —Ä–∞–≤–Ω–æ \(+1\); –µ—Å–ª–∏ –æ—à–∏–±–∞–µ—Ç—Å—è, \(-1\).

#### 2.2. –î–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ
–ß—Ç–æ–±—ã —É—á–∏—Ç—ã–≤–∞—Ç—å –±—É–¥—É—â–∏–µ –Ω–∞–≥—Ä–∞–¥—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ:
\[
G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1}
\]
–≥–¥–µ:
- \(G_t\) ‚Äî —Å–æ–≤–æ–∫—É–ø–Ω–æ–µ –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ.
- \(\gamma \in [0, 1]\) ‚Äî –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –æ–ø—Ä–µ–¥–µ–ª—è—é—â–∏–π –≤–∞–∂–Ω–æ—Å—Ç—å –±—É–¥—É—â–∏—Ö –Ω–∞–≥—Ä–∞–¥.

**–ó–Ω–∞—á–µ–Ω–∏–µ \(\gamma\):**
- –ï—Å–ª–∏ \(\gamma \approx 1\), –∞–≥–µ–Ω—Ç —É—á–∏—Ç—ã–≤–∞–µ—Ç –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã.
- –ï—Å–ª–∏ \(\gamma \approx 0\), –∞–≥–µ–Ω—Ç —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã—Ö –Ω–∞–≥—Ä–∞–¥–∞—Ö.

---

### 3. **Q-—Ñ—É–Ω–∫—Ü–∏—è –∏ V-—Ñ—É–Ω–∫—Ü–∏—è**

#### 3.1. –§—É–Ω–∫—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è (State Value Function, \(V(s)\))
–û—Ü–µ–Ω–∏–≤–∞–µ—Ç "—Ö–æ—Ä–æ—à–µ—Å—Ç—å" —Å–æ—Å—Ç–æ—è–Ω–∏—è \(s\) –ø—Ä–∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ \(\pi\):
\[
V^\pi(s) = \mathbb{E}_\pi \left[ G_t | s_t = s \right]
\]
–≥–¥–µ \(G_t\) ‚Äî —Å–æ–≤–æ–∫—É–ø–Ω–æ–µ –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ.

#### 3.2. Q-—Ñ—É–Ω–∫—Ü–∏—è (State-Action Value Function, \(Q(s, a)\))
–û—Ü–µ–Ω–∏–≤–∞–µ—Ç "—Ö–æ—Ä–æ—à–µ—Å—Ç—å" –ø–∞—Ä—ã \(s, a\) –ø—Ä–∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ \(\pi\):
\[
Q^\pi(s, a) = \mathbb{E}_\pi \left[ G_t | s_t = s, a_t = a \right]
\]

#### –°–≤—è–∑—å –º–µ–∂–¥—É \(V(s)\) –∏ \(Q(s, a)\):
\[
V^\pi(s) = \mathbb{E}_\pi \left[ Q^\pi(s, a) \right]
\]
–∏–ª–∏:
\[
V^\pi(s) = \sum_a \pi(a|s) Q^\pi(s, a)
\]

---

### 4. **–£—Ä–∞–≤–Ω–µ–Ω–∏–µ –ë–µ–ª–ª–º–∞–Ω–∞**

–£—Ä–∞–≤–Ω–µ–Ω–∏–µ –ë–µ–ª–ª–º–∞–Ω–∞ –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—É—é —Å–≤—è–∑—å –º–µ–∂–¥—É –∑–Ω–∞—á–µ–Ω–∏–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏.

#### 4.1. –£—Ä–∞–≤–Ω–µ–Ω–∏–µ –ë–µ–ª–ª–º–∞–Ω–∞ –¥–ª—è \(V(s)\):
\[
V^\pi(s) = \sum_a \pi(a|s) \left[ R(s, a) + \gamma \sum_{s'} P(s'|s, a) V^\pi(s') \right]
\]
–≥–¥–µ:
- \(P(s'|s, a)\) ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–µ—Ä–µ—Ö–æ–¥–∞ –∏–∑ \(s\) –≤ \(s'\) –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –¥–µ–π—Å—Ç–≤–∏—è \(a\).

#### 4.2. –£—Ä–∞–≤–Ω–µ–Ω–∏–µ –ë–µ–ª–ª–º–∞–Ω–∞ –¥–ª—è \(Q(s, a)\):
\[
Q^\pi(s, a) = R(s, a) + \gamma \sum_{s'} P(s'|s, a) \sum_{a'} \pi(a'|s') Q^\pi(s', a')
\]

---

### –í—ã–≤–æ–¥—ã

1. **Q- –∏ V-—Ñ—É–Ω–∫—Ü–∏–∏**:
   - \(V(s)\) –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç "—Ö–æ—Ä–æ—à–µ—Å—Ç—å" —Å–æ—Å—Ç–æ—è–Ω–∏—è, \(Q(s, a)\) ‚Äî –ø–∞—Ä—ã —Å–æ—Å—Ç–æ—è–Ω–∏–µ-–¥–µ–π—Å—Ç–≤–∏–µ.
2. **–£—Ä–∞–≤–Ω–µ–Ω–∏–µ –ë–µ–ª–ª–º–∞–Ω–∞**:
   - –û–ø–∏—Å—ã–≤–∞–µ—Ç —Å–≤—è–∑—å –º–µ–∂–¥—É —Ç–µ–∫—É—â–µ–π –Ω–∞–≥—Ä–∞–¥–æ–π –∏ –±—É–¥—É—â–∏–º–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è–º–∏.
3. **Q-Learning**:
   - –ú–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏ —á–µ—Ä–µ–∑ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—é \(Q(s, a)\).
4. PyTorch –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫–∞–∫ —Ç–∞–±–ª–∏—á–Ω—ã–µ, —Ç–∞–∫ –∏ –≥–ª—É–±–æ–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è.
"""
    else:
        code = """
#### 5.1. –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Q-Learning:
import numpy as np

# –ü—Ä–∏–º–µ—Ä —Å—Ä–µ–¥—ã
states = 5
actions = 2
q_table = np.zeros((states, actions))  # Q-—Ç–∞–±–ª–∏—Ü–∞

# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
alpha = 0.1  # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
gamma = 0.9  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
epsilon = 0.1  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–ª—É—á–∞–π–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è
episodes = 1000

# –û–±—É—á–µ–Ω–∏–µ
for episode in range(episodes):
    state = np.random.randint(0, states)
    done = False

    while not done:
        # –í—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è
        if np.random.rand() < epsilon:
            action = np.random.randint(0, actions)  # –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
        else:
            action = np.argmax(q_table[state])  # –≠–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è

        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è
        next_state = (state + action) % states
        reward = 1 if next_state == states - 1 else -0.1
        done = next_state == states - 1

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Q-—Ç–∞–±–ª–∏—Ü—ã
        q_table[state, action] += alpha * (
            reward + gamma * np.max(q_table[next_state]) - q_table[state, action]
        )
        state = next_state

print("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. Q-—Ç–∞–±–ª–∏—Ü–∞:")
print(q_table)

#### 5.2. Deep Q-Learning –≤ PyTorch:
import torch
import torch.nn as nn
import torch.optim as optim

# –°–µ—Ç—å –¥–ª—è Q-—Ñ—É–Ω–∫—Ü–∏–∏
class QNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, action_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
state_dim = 4
action_dim = 2
q_network = QNetwork(state_dim, action_dim)
optimizer = optim.Adam(q_network.parameters(), lr=0.001)
loss_fn = nn.MSELoss()

# –û–±—É—á–µ–Ω–∏–µ
for episode in range(1000):
    state = torch.randn(1, state_dim)
    action = torch.randint(0, action_dim, (1,))
    reward = torch.tensor([1.0])
    next_state = torch.randn(1, state_dim)

    # Q-–∑–Ω–∞—á–µ–Ω–∏–µ
    q_value = q_network(state)[0, action]
    target = reward + gamma * q_network(next_state).max()
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
    loss = loss_fn(q_value, target.detach())
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
"""
    pyperclip.copy(code)


def policy_grad(idx: int = 0):
    if idx == 0:
        code = """
### 21. –ú–µ—Ç–æ–¥ Policy Gradient, —É–ª—É—á—à–µ–Ω–∏—è –∏ –º–æ–¥–µ–ª—å Actor-Critic

---

### 1. **–ú–µ—Ç–æ–¥ Policy Gradient**

–ú–µ—Ç–æ–¥ Policy Gradient (PG) –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –∫–ª–∞—Å—Å—É –º–µ—Ç–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—É—á–∞—é—Ç **—Å—Ç—Ä–∞—Ç–µ–≥–∏—é (policy)** –Ω–∞–ø—Ä—è–º—É—é, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—è –µ—ë –ø–∞—Ä–∞–º–µ—Ç—Ä—ã \(\theta\).

#### –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è:
- –í–º–µ—Å—Ç–æ –æ–±—É—á–µ–Ω–∏—è Q-—Ñ—É–Ω–∫—Ü–∏–∏ –∏–ª–∏ V-—Ñ—É–Ω–∫—Ü–∏–∏, –∫–∞–∫ –≤ Q-Learning, PG –æ–±—É—á–∞–µ—Ç **—Å—Ç—Ä–∞—Ç–µ–≥–∏—é \(\pi(a|s; \theta)\)**, –∫–æ—Ç–æ—Ä–∞—è –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—ã–±–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏—è \(a\) –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ \(s\).

#### –¶–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è (Expected Return):
–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è:
\[
J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \sum_{t=0}^\infty \gamma^t R(s_t, a_t) \right]
\]

#### –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç —Ü–µ–ª–µ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏:
\[
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a|s) G_t \right]
\]
–≥–¥–µ:
- \(\log \pi_\theta(a|s)\) ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è.
- \(G_t\) ‚Äî –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å—É–º–º–∞—Ä–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ.

---

### 2. **–£–ª—É—á—à–µ–Ω–∏—è –º–µ—Ç–æ–¥–∞ Policy Gradient**

#### 2.1. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Advantage –≤–º–µ—Å—Ç–æ \(G_t\)**

–ü—Ä–æ–±–ª–µ–º–∞:
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è \(G_t\) –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –≤—ã—Å–æ–∫–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –≤ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è—Ö.

–†–µ—à–µ–Ω–∏–µ:
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **Advantage Function \(A(s, a)\)**, –∫–æ—Ç–æ—Ä–∞—è –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –¥–µ–π—Å—Ç–≤–∏–µ –ª—É—á—à–µ —Å—Ä–µ–¥–Ω–µ–≥–æ:
\[
A(s, a) = Q(s, a) - V(s)
\]

–§–æ—Ä–º—É–ª–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è:
\[
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a|s) A(s, a) \right]
\]

#### 2.2. **REINFORCE with Baseline**

–ü—Ä–æ–±–ª–µ–º–∞:
- –í—ã—Å–æ–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞.

–†–µ—à–µ–Ω–∏–µ:
- –í—ã—á–µ—Å—Ç—å –∏–∑ \(G_t\) **–±–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å** \(b(s)\), –∫–æ—Ç–æ—Ä—ã–π –Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –¥–µ–π—Å—Ç–≤–∏—è:
\[
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a|s) (G_t - b(s)) \right]
\]

–û–±—ã—á–Ω–æ –≤ –∫–∞—á–µ—Å—Ç–≤–µ \(b(s)\) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏—è \(V(s)\).

---

### 3. **–ú–æ–¥–µ–ª—å Actor-Critic**

–ú–æ–¥–µ–ª—å Actor-Critic –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –¥–≤–∞ –ø–æ–¥—Ö–æ–¥–∞:
1. **Actor**:
   - –†–µ–∞–ª–∏–∑—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é \(\pi(a|s; \theta_{\text{actor}})\).
   - –û–±–Ω–æ–≤–ª—è–µ—Ç—Å—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–æ–ª–∏—Ç–∏–∫–∏:
     \[
     \nabla_{\theta_{\text{actor}}} J = \mathbb{E} \left[ \nabla_{\theta_{\text{actor}}} \log \pi(a|s) A(s, a) \right]
     \]

2. **Critic**:
   - –û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é –∑–Ω–∞—á–µ–Ω–∏—è \(V(s; \theta_{\text{critic}})\) –∏–ª–∏ \(Q(s, a; \theta_{\text{critic}})\).
   - –û–±—É—á–∞–µ—Ç—Å—è –∫–∞–∫ –∑–∞–¥–∞—á–∞ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏:
     \[
     \mathcal{L}_{\text{critic}} = \left( R + \gamma V(s') - V(s) \right)^2
     \]

#### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Actor-Critic:
- **Actor** –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ —É–ª—É—á—à–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏.
- **Critic** —Å–Ω–∏–∂–∞–µ—Ç –¥–∏—Å–ø–µ—Ä—Å–∏—é, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è —Å—Ç–∞–±–∏–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É Advantage.

---

### 5. **–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã**

1. **Policy Gradient**:
   - –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –Ω–∞–ø—Ä—è–º—É—é, –æ–±—É—á–∞—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã \(\pi(a|s; \theta)\).
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∏–ª–∏ Advantage –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è.

2. **Actor-Critic**:
   - Actor –æ–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é.
   - Critic –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ, —Å–Ω–∏–∂–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—é –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞.

3. **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Actor-Critic**:
   - –ë—ã—Å—Ç—Ä–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å.
   - –£–º–µ–Ω—å—à–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —á–∏—Å—Ç—ã–º Policy Gradient.
"""
    else:
        code = """
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# –°–µ—Ç—å –¥–ª—è Actor
class Actor(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(Actor, self).__init__()
        self.fc = nn.Linear(state_dim, 128)
        self.output = nn.Linear(128, action_dim)

    def forward(self, x):
        x = torch.relu(self.fc(x))
        return torch.softmax(self.output(x), dim=-1)

# –°–µ—Ç—å –¥–ª—è Critic
class Critic(nn.Module):
    def __init__(self, state_dim):
        super(Critic, self).__init__()
        self.fc = nn.Linear(state_dim, 128)
        self.output = nn.Linear(128, 1)

    def forward(self, x):
        x = torch.relu(self.fc(x))
        return self.output(x)

# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
state_dim = 4
action_dim = 2
gamma = 0.99
lr = 0.001

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Actor –∏ Critic
actor = Actor(state_dim, action_dim)
critic = Critic(state_dim)
actor_optimizer = optim.Adam(actor.parameters(), lr=lr)
critic_optimizer = optim.Adam(critic.parameters(), lr=lr)

# –ü—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è
for episode in range(1000):
    state = torch.randn(1, state_dim)  # –°–ª—É—á–∞–π–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    probs = actor(state)
    action = torch.multinomial(probs, 1).item()  # –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è
    reward = torch.tensor([1.0])  # –ü—Ä–∏–º–µ—Ä –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è
    next_state = torch.randn(1, state_dim)  # –ù–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    
    # –í—ã—á–∏—Å–ª—è–µ–º Advantage
    value = critic(state)
    next_value = critic(next_state)
    advantage = reward + gamma * next_value - value

    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Actor
    actor_loss = -torch.log(probs[0, action]) * advantage.detach()
    actor_optimizer.zero_grad()
    actor_loss.backward()
    actor_optimizer.step()

    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Critic
    critic_loss = advantage.pow(2)
    critic_optimizer.zero_grad()
    critic_loss.backward()
    critic_optimizer.step()

    if episode % 100 == 0:
        print(f"Episode {episode}, Actor Loss: {actor_loss.item():.4f}, Critic Loss: {critic_loss.item():.4f}")
"""
    pyperclip.copy(code)


def q_learning(idx: int = 0):
    if idx == 0:
        code = """
### 1. **–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞ Q-Learning**

**Q-Learning** ‚Äî —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ (\(\pi^*\)) –≤ —Å—Ä–µ–¥–µ. –¶–µ–ª—å –º–µ—Ç–æ–¥–∞ ‚Äî –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ \(Q(s, a)\), –∫–æ—Ç–æ—Ä–∞—è –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç "–∫–∞—á–µ—Å—Ç–≤–æ" –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏—è \(a\) –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ \(s\).

#### –§—É–Ω–∫—Ü–∏—è \(Q(s, a)\):
- \(Q(s, a)\) ‚Äî –æ–∂–∏–¥–∞–µ–º–∞—è —Å–æ–≤–æ–∫—É–ø–Ω–∞—è –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –¥–µ–π—Å—Ç–≤–∏—è \(a\) –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ \(s\) –∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:
\[
Q(s, a) = \mathbb{E} \left[ \sum_{t=0}^\infty \gamma^t R(s_t, a_t) \, \Big| \, s_0 = s, a_0 = a \right]
\]

#### –¶–µ–ª—å –º–µ—Ç–æ–¥–∞:
- –ú–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–æ–≤–æ–∫—É–ø–Ω–æ–µ –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ:
\[
\pi^*(s) = \arg\max_a Q(s, a)
\]

---

### 2. **–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Q-—Ñ—É–Ω–∫—Ü–∏–∏**

Q-Learning –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É—Ä–∞–≤–Ω–µ–Ω–∏–µ –ë–µ–ª–ª–º–∞–Ω–∞ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è Q-–∑–Ω–∞—á–µ–Ω–∏–π. –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ \(Q^*(s, a)\) —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è–µ—Ç —É—Ä–∞–≤–Ω–µ–Ω–∏—é:
\[
Q^*(s, a) = R(s, a) + \gamma \max_{a'} Q^*(s', a')
\]
–≥–¥–µ:
- \(R(s, a)\) ‚Äî –º–≥–Ω–æ–≤–µ–Ω–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ.
- \(\gamma \in [0, 1]\) ‚Äî –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —É–º–µ–Ω—å—à–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ –±—É–¥—É—â–∏—Ö –Ω–∞–≥—Ä–∞–¥.
- \(s'\) ‚Äî –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏—è \(a\).

#### –ê–ª–≥–æ—Ä–∏—Ç–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è:
1. –î–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è \(s\) –∏ –¥–µ–π—Å—Ç–≤–∏—è \(a\), –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ, –ø–æ–ª—É—á–∏—Ç–µ \(s'\) –∏ \(R(s, a)\).
2. –û–±–Ω–æ–≤–∏—Ç–µ Q-–∑–Ω–∞—á–µ–Ω–∏–µ:
\[
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ R(s, a) + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
\]
–≥–¥–µ \(\alpha \in [0, 1]\) ‚Äî —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è.

---

### 3. **–ê–ª–≥–æ—Ä–∏—Ç–º Q-Learning**

#### –ü—Å–µ–≤–¥–æ–∫–æ–¥:
1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ Q-—Ç–∞–±–ª–∏—Ü—É: \(Q(s, a) = 0 \, \forall s, a\).
2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞:
   - –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ \(s\).
   - –ü–æ–≤—Ç–æ—Ä—è–π—Ç–µ, –ø–æ–∫–∞ —ç–ø–∏–∑–æ–¥ –Ω–µ –∑–∞–≤–µ—Ä—à–∏—Ç—Å—è:
     1. –í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ \(a\) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º \(\epsilon\)-–∂–∞–¥–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ (–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è).
     2. –í—ã–ø–æ–ª–Ω–∏—Ç–µ \(a\), –ø–æ–ª—É—á–∏—Ç–µ –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ \(s'\) –∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ \(R(s, a)\).
     3. –û–±–Ω–æ–≤–∏—Ç–µ Q-–∑–Ω–∞—á–µ–Ω–∏–µ:
        \[
        Q(s, a) \leftarrow Q(s, a) + \alpha \left[ R(s, a) + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
        \]
     4. –û–±–Ω–æ–≤–∏—Ç–µ —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ: \(s \leftarrow s'\).

3. –ü–æ–≤—Ç–æ—Ä—è–π—Ç–µ, –ø–æ–∫–∞ Q-—Ç–∞–±–ª–∏—Ü–∞ –Ω–µ —Å–æ–π–¥—ë—Ç—Å—è.

---

### 4. **–ü—Ä–∏–º–µ—Ä —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ Q-Learning –≤ Python**

#### –ü—Ä–∏–º–µ—Ä: –ü—Ä–æ—Å—Ç–∞—è —Å—Ä–µ–¥–∞ —Å –¥–≤–∏–∂–µ–Ω–∏–µ–º –≤–¥–æ–ª—å –ª–∏–Ω–∏–∏
```python
import numpy as np

# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
alpha = 0.1  # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
gamma = 0.9  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
epsilon = 0.1  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–ª—É—á–∞–π–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è
episodes = 1000
states = 5
actions = 2  # –í–ª–µ–≤–æ –∏–ª–∏ –≤–ø—Ä–∞–≤–æ

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Q-—Ç–∞–±–ª–∏—Ü—ã
q_table = np.zeros((states, actions))

# Q-Learning
for episode in range(episodes):
    state = np.random.randint(0, states)  # –°–ª—É—á–∞–π–Ω–æ–µ –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    done = False

    while not done:
        # –í—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è (\epsilon-–∂–∞–¥–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è)
        if np.random.rand() < epsilon:
            action = np.random.randint(0, actions)  # –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
        else:
            action = np.argmax(q_table[state])  # –≠–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è

        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è
        next_state = state + (1 if action == 1 else -1)
        next_state = max(0, min(next_state, states - 1))  # –ì—Ä–∞–Ω–∏—Ü—ã —Å—Ä–µ–¥—ã
        reward = 1 if next_state == states - 1 else -0.1  # –ù–∞–≥—Ä–∞–¥–∞
        done = next_state == states - 1

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Q-—Ç–∞–±–ª–∏—Ü—ã
        q_table[state, action] += alpha * (
            reward + gamma * np.max(q_table[next_state]) - q_table[state, action]
        )
        state = next_state

# –í—ã–≤–æ–¥ Q-—Ç–∞–±–ª–∏—Ü—ã
print("Q-—Ç–∞–±–ª–∏—Ü–∞ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è:")
print(q_table)
```

---

### 5. **–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–∞ Q-Learning**

#### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
1. **–ú–æ–¥–µ–ª—å-–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å**:
   - –ù–µ —Ç—Ä–µ–±—É–µ—Ç –∑–Ω–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å—Ä–µ–¥—ã (\(P(s'|s, a)\)) ‚Äî —Ç–æ–ª—å–∫–æ –Ω–∞–≥—Ä–∞–¥—ã –∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è.
2. **–ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å**:
   - –ü—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö (\(\alpha\), \(\gamma\)) Q-Learning —Å—Ö–æ–¥–∏—Ç—Å—è –∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏.

#### –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:
1. **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ —Ä–∞–∑–º–µ—Ä—É —Å–æ—Å—Ç–æ—è–Ω–∏—è**:
   - –ü—Ä–∏ –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Å–æ—Å—Ç–æ—è–Ω–∏–π Q-—Ç–∞–±–ª–∏—Ü–∞ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π (–ø—Ä–æ–±–ª–µ–º–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏).
2. **–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è**:
   - –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ–º (\(exploration\)) –∏ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–µ–π (\(exploitation\)) —Ç—Ä–µ–±—É–µ—Ç —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏.

---

### 6. **Deep Q-Learning (DQN)**

–î–ª—è —Å—Ä–µ–¥—ã —Å –±–æ–ª—å—à–∏–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—é—Ç **–≥–ª—É–±–æ–∫–æ–µ Q-–æ–±—É—á–µ–Ω–∏–µ**, –≥–¥–µ Q-—Ñ—É–Ω–∫—Ü–∏—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä—É–µ—Ç—Å—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç—å—é.

#### –û—Å–Ω–æ–≤–Ω—ã–µ –∏–¥–µ–∏:
1. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å \(Q(s, a; \theta)\) –≤–º–µ—Å—Ç–æ Q-—Ç–∞–±–ª–∏—Ü—ã.
2. –ü–æ—Ç–µ—Ä–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞:
   \[
   \mathcal{L}(\theta) = \left( R + \gamma \max_{a'} Q(s', a'; \theta^-) - Q(s, a; \theta) \right)^2
   \]
   –≥–¥–µ \(\theta^-\) ‚Äî –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω–æ–π —Ü–µ–ª–µ–≤–æ–π —Å–µ—Ç–∏.

---

### –í—ã–≤–æ–¥—ã

1. **Q-Learning** ‚Äî –º–æ—â–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç Q-—Ñ—É–Ω–∫—Ü–∏—é, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∞–≥–µ–Ω—Ç–∞.
2. **–£—Ä–∞–≤–Ω–µ–Ω–∏–µ –ë–µ–ª–ª–º–∞–Ω–∞** –ª–µ–∂–∏—Ç –≤ –æ—Å–Ω–æ–≤–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è Q-–∑–Ω–∞—á–µ–Ω–∏–π.
3. **Deep Q-Learning (DQN)** –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å Q-Learning –∫ —Å–ª–æ–∂–Ω—ã–º –∑–∞–¥–∞—á–∞–º —Å –ø–æ–º–æ—â—å—é –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π.
"""
    else:
        code = """
import torch
import torch.nn as nn
import torch.optim as optim

# –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è Q-—Ñ—É–Ω–∫—Ü–∏–∏
class QNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)
        self.fc2 = nn.Linear(128, action_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
state_dim = 4
action_dim = 2
q_network = QNetwork(state_dim, action_dim)
optimizer = optim.Adam(q_network.parameters(), lr=0.001)
loss_fn = nn.MSELoss()

# –ü—Ä–∏–º–µ—Ä –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
state = torch.tensor([[0.1, 0.2, 0.3, 0.4]], dtype=torch.float32)
action = 1
reward = 1.0
next_state = torch.tensor([[0.2, 0.3, 0.4, 0.5]], dtype=torch.float32)

# Q-–∑–Ω–∞—á–µ–Ω–∏–µ
q_value = q_network(state)[0, action]
target = reward + gamma * torch.max(q_network(next_state)).detach()

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
loss = loss_fn(q_value, target)
optimizer.zero_grad()
loss.backward()
optimizer.step()
"""
    pyperclip.copy(code)


def vgg16(idx: int = 0):
    if idx == 0:
        code = """
### 23. –¢–∏–ø—ã –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ VGG16: –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏

---

### 1. **–¢–∏–ø—ã –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, —Ä–µ—à–∞–µ–º—ã–µ —Å –ø–æ–º–æ—â—å—é –≥–ª—É–±–æ–∫–∏—Ö –º–æ–¥–µ–ª–µ–π**

–ì–ª—É–±–æ–∫–∏–µ –º–æ–¥–µ–ª–∏, –æ—Å–æ–±–µ–Ω–Ω–æ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (CNN), –∞–∫—Ç–∏–≤–Ω–æ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è.

#### –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–∏–ø—ã –∑–∞–¥–∞—á:

1. **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π**:
   - –ó–∞–¥–∞—á–∞: –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∫ –∫–∞–∫–æ–º—É –∫–ª–∞—Å—Å—É –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.
   - –ü—Ä–∏–º–µ—Ä: —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ (–∫–æ—à–∫–∞, —Å–æ–±–∞–∫–∞, –∞–≤—Ç–æ–º–æ–±–∏–ª—å).

2. **–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π**:
   - –ó–∞–¥–∞—á–∞: –≤—ã–¥–µ–ª–∏—Ç—å –æ–±–ª–∞—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –æ—Ç–Ω–æ—Å—è—â–∏–µ—Å—è –∫ —Ä–∞–∑–Ω—ã–º –∫–ª–∞—Å—Å–∞–º.
   - –ü—Ä–∏–º–µ—Ä: –≤—ã–¥–µ–ª–µ–Ω–∏–µ –¥–æ—Ä–æ–≥ –Ω–∞ —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã—Ö —Å–Ω–∏–º–∫–∞—Ö.

3. **–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ (Object Detection)**:
   - –ó–∞–¥–∞—á–∞: –Ω–∞–π—Ç–∏ –æ–±—ä–µ–∫—Ç—ã –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏—Ö –≥—Ä–∞–Ω–∏—Ü—ã.
   - –ü—Ä–∏–º–µ—Ä: –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –ø–µ—à–µ—Ö–æ–¥–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏.

4. **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π**:
   - –ó–∞–¥–∞—á–∞: —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ GAN.
   - –ü—Ä–∏–º–µ—Ä: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ª–∏—Ü –∏–ª–∏ —Å—Ç–∏–ª–∏–∑–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.

5. **–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π**:
   - –ó–∞–¥–∞—á–∞: –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–≤–∏–¥–µ–æ) –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏–π.
   - –ü—Ä–∏–º–µ—Ä: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ.

6. **–ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (Image Retrieval)**:
   - –ó–∞–¥–∞—á–∞: –Ω–∞–π—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ –¥–∞–Ω–Ω–æ–µ.
   - –ü—Ä–∏–º–µ—Ä: –ø–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–æ–≤ –ø–æ —Ñ–æ—Ç–æ.

---

### 2. **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ VGG16**

VGG16 ‚Äî —ç—Ç–æ –æ–¥–Ω–∞ –∏–∑ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –≤ 2014 –≥–æ–¥—É –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è–º–∏ –∏–∑ Visual Geometry Group (VGG). –û–Ω–∞ –±—ã–ª–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤ —Ä–∞–±–æ—Ç–µ "Very Deep Convolutional Networks for Large-Scale Image Recognition".

#### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ VGG16:
1. **–û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è**:
   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–µ–±–æ–ª—å—à–∏—Ö —Å–≤—ë—Ä—Ç–æ—á–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤ (\(3 \times 3\)) —Å —à–∞–≥–æ–º 1 –∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º padding, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
   - –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –≥–ª—É–±–∏–Ω—ã —Å–µ—Ç–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

2. **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**:
   - –°–æ—Å—Ç–æ–∏—Ç –∏–∑ 16 —Å–ª–æ—ë–≤: 13 —Å–≤—ë—Ä—Ç–æ—á–Ω—ã—Ö —Å–ª–æ—ë–≤ –∏ 3 –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã—Ö —Å–ª–æ—è.
   - –ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞ —Å–≤—ë—Ä—Ç–∫–∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è MaxPooling –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.
   - –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–ª–æ–∏ ‚Äî –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ, –∑–∞–∫–∞–Ω—á–∏–≤–∞—é—â–∏–µ—Å—è Softmax –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.

| –ë–ª–æ–∫ | –°–ª–æ–∏ –≤ –±–ª–æ–∫–µ | –†–∞–∑–º–µ—Ä –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ |
|------|--------------|-------------------------|
| Conv1 | \(3 \times 3\), \(3 \to 64\), \(3 \times 3\), \(64 \to 64\) | \(224 \times 224 \to 112 \times 112\) |
| Conv2 | \(3 \times 3\), \(64 \to 128\), \(3 \times 3\), \(128 \to 128\) | \(112 \times 112 \to 56 \times 56\) |
| Conv3 | \(3 \times 3\), \(128 \to 256\), 3 —Å–ª–æ—è | \(56 \times 56 \to 28 \times 28\) |
| Conv4 | \(3 \times 3\), \(256 \to 512\), 3 —Å–ª–æ—è | \(28 \times 28 \to 14 \times 14\) |
| Conv5 | \(3 \times 3\), \(512 \to 512\), 3 —Å–ª–æ—è | \(14 \times 14 \to 7 \times 7\) |
| FC    | –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏: \(4096 \to 4096 \to 1000\) | \(1 \times 1\) |

---

### 3. **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ VGG16**

1. **–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å**:
   - VGG16 —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –±–∞–∑–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –º–Ω–æ–≥–∏—Ö –∑–∞–¥–∞—á –≤ –º–∞—à–∏–Ω–Ω–æ–º –∑—Ä–µ–Ω–∏–∏.

2. **–ü—Ä–æ—Å—Ç–æ—Ç–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã**:
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ \(3 \times 3\) —Ñ–∏–ª—å—Ç—Ä—ã, —á—Ç–æ —É–ø—Ä–æ—â–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é.

3. **–°–ª–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤**:
   - –ì–ª—É–±–æ–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–≤–ª–µ–∫–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏.

4. **–ü–µ—Ä–µ–Ω–æ—Å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (Transfer Learning)**:
   - VGG16, –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ ImageNet, –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è.

---

### 4. **–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ VGG16**

1. **–í—ã—Å–æ–∫–∞—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å**:
   - –ì–ª—É–±–æ–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ç—Ä–µ–±—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–≤—Ä–µ–º—è, –ø–∞–º—è—Ç—å).

2. **–ë–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤**:
   - –û–∫–æ–ª–æ 138 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –º–æ–¥–µ–ª—å –≥—Ä–æ–º–æ–∑–¥–∫–æ–π –∏ —Å–∫–ª–æ–Ω–Ω–æ–π –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é.

3. **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å**:
   - –ù–æ–≤—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ ResNet, MobileNet, –∏ EfficientNet, –¥–æ—Å—Ç–∏–≥–∞—é—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –º–µ–Ω—å—à–∏–º–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏.

---

### 6. **–í—ã–≤–æ–¥—ã**

1. **–¢–∏–ø—ã –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è**:
   - –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è, –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
2. **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ VGG16**:
   - –ì–ª—É–±–æ–∫–∞—è —Å–µ—Ç—å —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º–∏ \(3 \times 3\) —Å–≤—ë—Ä—Ç–∫–∞–º–∏ –∏ –ø—É–ª–ª–∏–Ω–≥–æ–º.
   - –•–æ—Ä–æ—à–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞ –æ–±—É—á–µ–Ω–∏—è.
3. **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏**:
   - –ü—Ä–æ—Å—Ç–æ—Ç–∞ –∏ –≤—ã—Å–æ–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ —Å–≤–æ—ë–º –≤—Ä–µ–º–µ–Ω–∏, –Ω–æ –±–æ–ª—å—à–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏ —É—Å—Ç–∞—Ä–µ–≤–∞–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏.
"""
    else:
        code = """
#### –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ VGG16:
import torch
from torchvision import models

# –ó–∞–≥—Ä—É–∑–∫–∞ VGG16, –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –Ω–∞ ImageNet
vgg16 = models.vgg16(pretrained=True)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏
print(vgg16)

# –ó–∞–º–µ–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ 10 –∫–ª–∞—Å—Å–æ–≤
vgg16.classifier[6] = torch.nn.Linear(4096, 10)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
print(vgg16)

#### –ü—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è:
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms

# –î–∞—Ç–∞—Å–µ—Ç
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
])

train_data = datasets.FakeData(transform=transform)
train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)

# –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(vgg16.parameters(), lr=0.001)

# –û–±—É—á–µ–Ω–∏–µ
for epoch in range(3):
    for images, labels in train_loader:
        optimizer.zero_grad()
        outputs = vgg16(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")
"""
    pyperclip.copy(code)


def google_net(idx: int = 0):
    if idx == 0:
        code = """
### 24. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ GoogLeNet, –æ–ø–∏—Å–∞–Ω–∏–µ Inception module, —Å–≤–µ—Ä—Ç–∫–∞ \(1 \times 1\), DepthConcat –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏

---

### 1. **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ GoogLeNet**

GoogLeNet ‚Äî —ç—Ç–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –≥–ª—É–±–æ–∫–∏—Ö —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω–∞—è –≤ 2014 –≥–æ–¥—É –∫–æ–º–∞–Ω–¥–æ–π Google Research –≤ —Ä–∞–±–æ—Ç–µ *"Going Deeper with Convolutions"*. –û–Ω–∞ –∑–∞–≤–æ–µ–≤–∞–ª–∞ –ø–µ—Ä–≤–æ–µ –º–µ—Å—Ç–æ –Ω–∞ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–∏ ImageNet Large Scale Visual Recognition Challenge (ILSVRC-2014).

#### –ö–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
1. **–ì–ª—É–±–æ–∫–∞—è —Å–µ—Ç—å**:
   - GoogLeNet —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 22 —É—Ä–æ–≤–Ω–µ–π (–≥–ª—É–±–∏–Ω–∞ —Å–µ—Ç–∏).
   - –í–∫–ª—é—á–∞–µ—Ç 9 Inception-–º–æ–¥—É–ª–µ–π.

2. **Inception-–º–æ–¥—É–ª—å**:
   - –ö–ª—é—á–µ–≤–∞—è –∏–Ω–Ω–æ–≤–∞—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ä–∞–∑–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞.

3. **–°–Ω–∏–∂–µ–Ω–∏–µ —á–∏—Å–ª–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤**:
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–≤–µ—Ä—Ç–∫–∞ \(1 \times 1\) –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏ –ø–æ–≤—ã—à–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.
   - –ò—Ç–æ–≥–æ: 5 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å ~60 –º–ª–Ω —É AlexNet).

4. **Auxiliary Classifiers**:
   - –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Å–ª–æ—è—Ö –¥–ª—è –±–æ—Ä—å–±—ã —Å –∑–∞—Ç—É—Ö–∞–Ω–∏–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.

---

### 2. **Inception Module**

#### –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è:
–í–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã –≤—ã–±–∏—Ä–∞—Ç—å —Ä–∞–∑–º–µ—Ä —Ñ–∏–ª—å—Ç—Ä–∞ –¥–ª—è —Å–≤–µ—Ä—Ç–∫–∏ (\(1 \times 1\), \(3 \times 3\), \(5 \times 5\)), Inception module –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏—Ö –∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª—è–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ä–∞–∑–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞.

#### –°—Ç—Ä—É–∫—Ç—É—Ä–∞:
Inception-–º–æ–¥—É–ª—å –≤–∫–ª—é—á–∞–µ—Ç:
1. –°–≤–µ—Ä—Ç–∫–∏ \(1 \times 1\):
   - –î–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏).
2. –°–≤–µ—Ä—Ç–∫–∏ \(3 \times 3\) –∏ \(5 \times 5\):
   - –î–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ä–∞–∑–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞.
3. MaxPooling:
   - –î–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏ –≤—ã–¥–µ–ª–µ–Ω–∏—è –æ–±–æ–±—â–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
4. **DepthConcat**:
   - –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –ø–æ –≥–ª—É–±–∏–Ω–µ (–∫–∞–Ω–∞–ª–∞–º).

#### –ü—Ä–∏–º–µ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã:
```
        Input
          ‚Üì
   [1x1] [1x1 ‚Üí 3x3] [1x1 ‚Üí 5x5] [Pooling ‚Üí 1x1]
          ‚Üì
        Concatenate (–ø–æ –≥–ª—É–±–∏–Ω–µ)
```

---

### 3. **–°–≤–µ—Ä—Ç–∫–∞ \(1 \times 1\)**

#### –û—Å–Ω–æ–≤–Ω–æ–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ:
1. **–°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏**:
   - –£–º–µ–Ω—å—à–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤, —á—Ç–æ —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã.
   - –ù–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ \(256 \to 64\) –ø–µ—Ä–µ–¥ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π (\(3 \times 3\), \(5 \times 5\)).

2. **–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç–∏**:
   - –ü–æ—Å–ª–µ —Å–≤–µ—Ä—Ç–∫–∏ \(1 \times 1\) –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è ReLU, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–µ—Ç–∏ –∏–∑—É—á–∞—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏.

#### –ü—Ä–∏–º–µ—Ä:
–°–≤–µ—Ä—Ç–∫–∞ \(1 \times 1\) —Å 256 –≤—Ö–æ–¥–Ω—ã–º–∏ –∫–∞–Ω–∞–ª–∞–º–∏ –∏ 64 –≤—ã—Ö–æ–¥–Ω—ã–º–∏:
\[
\text{–ß–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤} = 1 \cdot 1 \cdot 256 \cdot 64 = 16,384
\]
–î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, —Å–≤–µ—Ä—Ç–∫–∞ \(3 \times 3\) –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø–æ—Ç—Ä–µ–±–æ–≤–∞–ª–∞ –±—ã:
\[
\text{–ß–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤} = 3 \cdot 3 \cdot 256 \cdot 64 = 147,456
\]

---

### 4. **DepthConcat**

DepthConcat ‚Äî —ç—Ç–æ –æ–ø–µ—Ä–∞—Ü–∏—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –æ—Å–∏ –∫–∞–Ω–∞–ª–æ–≤ –ø–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –≤ Inception-–º–æ–¥—É–ª–µ.

#### –õ–æ–≥–∏–∫–∞:
- –í—ã—Ö–æ–¥—ã –∫–∞–∂–¥–æ–≥–æ –ø—É—Ç–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–≤–µ—Ä—Ç–∫–∏ \(1 \times 1\), \(3 \times 3\), \(5 \times 5\)) –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∫–∞–Ω–∞–ª—ã.
- –ù–∞–ø—Ä–∏–º–µ—Ä:
  - –í—Ö–æ–¥: \(32 \times 32 \times 64\).
  - –ü–æ—Å–ª–µ \(1 \times 1\): \(32 \times 32 \times 32\).
  - –ü–æ—Å–ª–µ \(3 \times 3\): \(32 \times 32 \times 64\).
  - –ü–æ—Å–ª–µ \(5 \times 5\): \(32 \times 32 \times 32\).
  - –ü–æ—Å–ª–µ DepthConcat: \(32 \times 32 \times 192\).

---

### 5. **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ GoogLeNet —Å –¥—Ä—É–≥–∏–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏**

| **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞** | **–ß–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤** | **–ì–ª—É–±–∏–Ω–∞** | **–ö–ª—é—á–µ–≤—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏** | **–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏** |
|------------------|-----------------------|-------------|-------------------------|-----------------|
| **AlexNet**     | 60 –º–ª–Ω               | 8           | MaxPooling, Dropout    | –ë–æ–ª—å—à–æ–µ —á–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ |
| **VGG16**       | 138 –º–ª–Ω              | 16          | –ü—Ä–æ—Å—Ç–æ—Ç–∞, \(3 \times 3\) —Å–≤–µ—Ä—Ç–∫–∏ | –û–≥—Ä–æ–º–Ω—ã–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã |
| **GoogLeNet**   | 5 –º–ª–Ω                | 22          | Inception module, Auxiliary Classifiers | –°–ª–æ–∂–Ω–æ—Å—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã |
| **ResNet**      | 25 –º–ª–Ω (ResNet-50)   | 50          | Residual Connections   | –°–ª–æ–∂–Ω–æ—Å—Ç—å –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è |

---

### 7. **–í—ã–≤–æ–¥—ã**

1. **GoogLeNet**:
   - –í–ø–µ—Ä–≤—ã–µ –≤–≤–µ–ª–∞ Inception-–º–æ–¥—É–ª—å, –ø–æ–∑–≤–æ–ª–∏–≤ –∏–∑–≤–ª–µ–∫–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ä–∞–∑–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞.
   - –û—Ç–ª–∏—á–∞–µ—Ç—Å—è –Ω–∏–∑–∫–∏–º —á–∏—Å–ª–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–≤—Å–µ–≥–æ 5 –º–ª–Ω).
2. **Inception Module**:
   - –°–æ—á–µ—Ç–∞–µ—Ç —Ñ–∏–ª—å—Ç—Ä—ã —Ä–∞–∑–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (\(1 \times 1\), \(3 \times 3\), \(5 \times 5\)) –∏ MaxPooling.
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç DepthConcat –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≤—ã—Ö–æ–¥–æ–≤.
3. **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ**:
   - GoogLeNet –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –ø–æ —á–∏—Å–ª—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –Ω–æ —É—Å—Ç—É–ø–∞–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, ResNet) –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏.
"""
    else:
        code = """
import torch
import torch.nn as nn

# –†–µ–∞–ª–∏–∑–∞—Ü–∏—è Inception Module
class InceptionModule(nn.Module):
    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, pool_proj):
        super(InceptionModule, self).__init__()
        self.branch1 = nn.Conv2d(in_channels, out_1x1, kernel_size=1)

        self.branch2 = nn.Sequential(
            nn.Conv2d(in_channels, red_3x3, kernel_size=1),
            nn.Conv2d(red_3x3, out_3x3, kernel_size=3, padding=1)
        )

        self.branch3 = nn.Sequential(
            nn.Conv2d(in_channels, red_5x5, kernel_size=1),
            nn.Conv2d(red_5x5, out_5x5, kernel_size=5, padding=2)
        )

        self.branch4 = nn.Sequential(
            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),
            nn.Conv2d(in_channels, pool_proj, kernel_size=1)
        )

    def forward(self, x):
        branch1 = self.branch1(x)
        branch2 = self.branch2(x)
        branch3 = self.branch3(x)
        branch4 = self.branch4(x)
        return torch.cat([branch1, branch2, branch3, branch4], dim=1)

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
x = torch.randn(1, 192, 32, 32)  # –ü—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
inception = InceptionModule(192, 64, 96, 128, 16, 32, 32)
output = inception(x)
print("Output shape:", output.shape)  # –†–µ–∑—É–ª—å—Ç–∞—Ç: [1, 256, 32, 32]
"""
    pyperclip.copy(code)


def resnet(idx: int = 0):
    if idx == 0:
        code = """
### 25. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å residual connections, –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã, –Ω–∞—Ä—É—à–µ–Ω–∏–µ —Å–∏–º–º–µ—Ç—Ä–∏–∏, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏

---

### 1. **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å residual connections**

**Residual connections** (–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è) –≤–ø–µ—Ä–≤—ã–µ –±—ã–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ **ResNet** (Residual Network), –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–π –≤ 2015 –≥–æ–¥—É –≤ —Å—Ç–∞—Ç—å–µ *"Deep Residual Learning for Image Recognition"*.

#### –¶–µ–ª—å:
- –†–µ—à–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—ã –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π:
  - **–ó–∞—Ç—É—Ö–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤**: –≤ –æ—á–µ–Ω—å –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç—è—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —É–º–µ–Ω—å—à–∞—é—Ç—Å—è –¥–æ –Ω—É–ª—è.
  - **–î–µ–≥—Ä–∞–¥–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è**: –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –≥–ª—É–±–∏–Ω—ã –º–æ–¥–µ–ª–∏ –µ—ë –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–∂–µ—Ç —É—Ö—É–¥—à–∞—Ç—å—Å—è.

#### –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è:
- –î–æ–±–∞–≤–∏—Ç—å –ø—Ä—è–º—ã–µ –ø—Ä–æ–ø—É—Å–∫–∏ (skip connections), –ø–æ–∑–≤–æ–ª—è—è —Å–ª–æ—è–º –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ **–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏**:
  \[
  \mathcal{F}(x) = H(x) - x \quad \implies \quad H(x) = \mathcal{F}(x) + x
  \]
  –≥–¥–µ:
  - \(H(x)\) ‚Äî —Ü–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä—É—é –¥–æ–ª–∂–µ–Ω –∏–∑—É—á–∏—Ç—å —Å–ª–æ–π.
  - \(\mathcal{F}(x)\) ‚Äî –æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä—É—é –∏–∑—É—á–∞–µ—Ç —Å–ª–æ–π, –Ω–∞—á–∏–Ω–∞—è —Å \(\mathcal{F}(x) = 0\).

---

### 2. **–û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã residual connections**

1. **–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä—è–º–æ–≥–æ –ø—É—Ç–∏ (skip connection)**:
   - –í—ã—Ö–æ–¥ –∏–∑ –æ–¥–Ω–æ–≥–æ —Å–ª–æ—è –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∫ –≤—ã—Ö–æ–¥—É –¥—Ä—É–≥–æ–≥–æ —Å–ª–æ—è.
   - –ù–∞–ø—Ä–∏–º–µ—Ä:
     \[
     y = \mathcal{F}(x, W) + x
     \]
     –≥–¥–µ \(\mathcal{F}(x, W)\) ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–≤–µ—Ä—Ç–∫–∞), –∞ \(x\) ‚Äî –≤—Ö–æ–¥.

2. **–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è**:
   - –ü—Ä—è–º–æ–π –ø—É—Ç—å \(x\) –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–ª–æ—ë–≤, —Å–Ω–∏–∂–∞—è –∑–∞—Ç—É—Ö–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.

3. **–£—Å–∫–æ—Ä–µ–Ω–∏–µ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏**:
   - –õ–µ–≥—á–µ –æ–±—É—á–∞—Ç—å –æ—Å—Ç–∞—Ç–æ—á–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é (\(\mathcal{F}(x)\)).

4. **–ì–∏–±–∫–æ—Å—Ç—å**:
   - –ú–æ–∂–Ω–æ –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª–æ—ë–≤ (deep residual block).

---

### 3. **–ù–∞—Ä—É—à–µ–Ω–∏–µ —Å–∏–º–º–µ—Ç—Ä–∏–∏ –≥–ª—É–±–æ–∫–∏—Ö –ò–ù–° —Å –ø–æ–º–æ—â—å—é residual connections**

#### –ü—Ä–æ–±–ª–µ–º–∞ —Å–∏–º–º–µ—Ç—Ä–∏–∏:
–í —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç—è—Ö –±–µ–∑ –æ—Å—Ç–∞—Ç–æ—á–Ω—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –º–æ–∂–µ—Ç –≤–æ–∑–Ω–∏–∫–Ω—É—Ç—å **—Å–∏–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É —Å–æ—Å–µ–¥–Ω–∏–º–∏ —Å–ª–æ—è–º–∏**, —Ç–∞–∫ –∫–∞–∫ –∫–∞–∂–¥—ã–π —Å–ª–æ–π –Ω–∞–ø—Ä—è–º—É—é –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≤—ã—Ö–æ–¥–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ. –≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫:
- –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é.
- –ó–∞—Ç—É—Ö–∞–Ω–∏—é –∏–ª–∏ –≤–∑—Ä—ã–≤—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.

#### –†–µ—à–µ–Ω–∏–µ —á–µ—Ä–µ–∑ residual connections:
- **–ù–∞—Ä—É—à–µ–Ω–∏–µ —Å–∏–º–º–µ—Ç—Ä–∏–∏**:
  - Skip connections –¥–æ–±–∞–≤–ª—è—é—Ç **–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø—É—Ç—å –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤**, –ø–æ–∑–≤–æ–ª—è—è –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª–æ—ë–≤.
- **–£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è**:
  - –û—Å—Ç–∞—Ç–æ—á–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (\(\mathcal{F}(x)\)) –æ–±—É—á–∞—é—Ç—Å—è –±—ã—Å—Ç—Ä–µ–µ, —Ç–∞–∫ –∫–∞–∫ –Ω–∞—á–∏–Ω–∞—é—Ç —Å –º–∞–ª—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (–±–ª–∏–∂–µ –∫ –Ω—É–ª—é).

---

### 4. **–ü—Ä–∏–º–µ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã residual block**

#### –û–±—ã—á–Ω—ã–π residual block:
1. –î–≤–∞ —Å–ª–æ—è —Å–≤—ë—Ä—Ç–æ–∫ —Å –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π (ReLU) –∏ BatchNorm.
2. –ü—Ä—è–º–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ (skip connection):
   \[
   y = \text{ReLU}(\mathcal{F}(x, W) + x)
   \]

---

### 5. **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ ResNet —Å –¥—Ä—É–≥–∏–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏**

| **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞** | **–ß–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤** | **–ì–ª—É–±–∏–Ω–∞** | **–ö–ª—é—á–µ–≤—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏**        | **–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏**                    |
|------------------|-----------------------|-------------|--------------------------------|------------------------------------|
| **AlexNet**     | ~60 –º–ª–Ω              | 8           | MaxPooling, Dropout           | –û–≥—Ä–æ–º–Ω–æ–µ —á–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤         |
| **VGG16**       | ~138 –º–ª–Ω             | 16          | –ü—Ä–æ—Å—Ç–æ—Ç–∞, –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ \(3 \times 3\) | –í—ã—Å–æ–∫–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã    |
| **GoogLeNet**   | ~5 –º–ª–Ω               | 22          | Inception-–º–æ–¥—É–ª—å              | –°–ª–æ–∂–Ω–æ—Å—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã             |
| **ResNet**      | ~25 –º–ª–Ω (ResNet-50)  | 50+         | Residual Connections          | –°–ª–æ–∂–Ω–æ—Å—Ç—å –≤ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏            |

---

### 6. **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ ResNet**

1. **–û–±—É—á–µ–Ω–∏–µ –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π**:
   - –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å –æ—á–µ–Ω—å –≥–ª—É–±–æ–∫–∏–µ —Å–µ—Ç–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ResNet-152).

2. **–°–Ω–∏–∂–µ–Ω–∏–µ –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤**:
   - –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ skip connections, —á—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –∏—Ö –∑–∞—Ç—É—Ö–∞–Ω–∏–µ.

3. **–ì–∏–±–∫–æ—Å—Ç—å**:
   - –õ–µ–≥–∫–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è –¥–ª—è –∑–∞–¥–∞—á —Ä–∞–∑–ª–∏—á–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏.

4. **–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å**:
   - –ë—ã—Å—Ç—Ä–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –±–ª–∞–≥–æ–¥–∞—Ä—è –æ–±—É—á–µ–Ω–∏—é –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏.

---

### 7. **–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ ResNet**

1. **–°–ª–æ–∂–Ω–æ—Å—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã**:
   - –£—Å–ª–æ–∂–Ω–µ–Ω–∏–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ —Ç—Ä—É–¥–Ω–æ—Å—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –∑–∞–¥–∞—á.

2. **–ò–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å**:
   - –í –æ—á–µ–Ω—å –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç—è—Ö –º–æ–≥—É—Ç –≤–æ–∑–Ω–∏–∫–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º –∏–∑-–∑–∞ –ª–∏—à–Ω–∏—Ö —Å–ª–æ—ë–≤.

---

### 9. **–í—ã–≤–æ–¥—ã**

1. **Residual connections** –ø–æ–∑–≤–æ–ª—è—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—É—á–∞—Ç—å –≥–ª—É–±–æ–∫–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏, —Ä–µ—à–∞—è –ø—Ä–æ–±–ª–µ–º—ã –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∏ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏.
2. ResNet –∏ –µ–≥–æ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ (ResNet-50, ResNet-152) —è–≤–ª—è—é—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º –¥–ª—è –º–Ω–æ–≥–∏—Ö –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è.
3. –í —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å –¥—Ä—É–≥–∏–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏ ResNet –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –æ—Ç–ª–∏—á–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –≥–ª—É–±–∏–Ω—É –ø—Ä–∏ —Ä–∞–∑—É–º–Ω–æ–º —á–∏—Å–ª–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
"""
    else:
        code = """


### 8. **–ü—Ä–∏–º–µ—Ä ResNet-50 –≤ PyTorch**

from torchvision.models import resnet50

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ ResNet-50
model = resnet50(pretrained=True)

# –ò–∑–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ 10 –∫–ª–∞—Å—Å–æ–≤
model.fc = nn.Linear(2048, 10)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
print(model)

import torch
import torch.nn as nn

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)

        # –ü—Ä—è–º–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ (–¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è —á–∏—Å–ª–∞ –∫–∞–Ω–∞–ª–æ–≤ –∏–ª–∏ —à–∞–≥–∞)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = torch.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        return torch.relu(out)

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–ª–æ–∫–∞
x = torch.randn(1, 64, 32, 32)  # 1 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, 64 –∫–∞–Ω–∞–ª–∞, —Ä–∞–∑–º–µ—Ä 32x32
block = ResidualBlock(64, 64)
output = block(x)
print("Output shape:", output.shape)  # [1, 64, 32, 32]
"""
    pyperclip.copy(code)


def unet(idx: int = 0):
    if idx == 0:
        code = """
### 26. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ U-Net: –æ–±—â–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã, –æ–ø–∏—Å–∞–Ω–∏–µ, residual connections –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ –∑–∞–¥–∞—á–∞—Ö —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏

---

### 1. **–û–±—â–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã U-Net**

**U-Net** ‚Äî —ç—Ç–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –≤ 2015 –≥–æ–¥—É –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω–∞ –±—ã–ª–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –Ω–æ —Å–µ–π—á–∞—Å —à–∏—Ä–æ–∫–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤ –¥—Ä—É–≥–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö.

#### –û—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–ª—å:
- –ò–∑—É—á–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–ª–∞—Å—Å–∞ –∫–∞–∂–¥–æ–≥–æ –ø–∏–∫—Å–µ–ª—è (–∑–∞–¥–∞—á–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏).

#### –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
1. **–°–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞—è U-–æ–±—Ä–∞–∑–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞**:
   - –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö —á–∞—Å—Ç–µ–π:
     - **–ö–æ–Ω—Ç—Ä–∞–∫—Ç–∏–≤–Ω–∞—è —á–∞—Å—Ç—å (Encoder)**: –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏, —É–º–µ–Ω—å—à–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
     - **–≠–∫—Å–ø–∞–Ω—Å–∏–≤–Ω–∞—è —á–∞—Å—Ç—å (Decoder)**: –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏.
   - –ù–∞–∑–≤–∞–Ω–∏–µ "U-Net" —Å–≤—è–∑–∞–Ω–æ —Å —Ñ–æ—Ä–º–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã.

2. **–°–∫–∏–ø-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è (skip connections)**:
   - –ü—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ Encoder –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –≤ Decoder, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.

3. **–ü–æ–ª–Ω–∞—è —Å–≤–µ—Ä—Ç–∫–∞ (Fully Convolutional Network)**:
   - U-Net –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã—Ö —Å–ª–æ–µ–≤, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ—ë –ø—Ä–∏–º–µ–Ω–∏–º–æ–π –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ª—é–±–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.

---

### 2. **–û–ø–∏—Å–∞–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã U-Net**

#### –û–±—â–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:
1. **–ö–æ–Ω—Ç—Ä–∞–∫—Ç–∏–≤–Ω–∞—è —á–∞—Å—Ç—å (Encoder)**:
   - –°–æ—Å—Ç–æ–∏—Ç –∏–∑ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –±–ª–æ–∫–æ–≤:
     - –î–≤–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Å–≤—ë—Ä—Ç–∫–∏ \(3 \times 3\) (Conv ‚Üí ReLU).
     - MaxPooling \(2 \times 2\) –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
   - –ù–∞ –∫–∞–∂–¥–æ–º —É—Ä–æ–≤–Ω–µ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤, —á—Ç–æ–±—ã –∑–∞—Ö–≤–∞—Ç–∏—Ç—å –±–æ–ª—å—à–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

2. **–ë–æ—Ç—Ç–ª–Ω–µ–∫ (Bottleneck)**:
   - –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π —É–∑–µ–ª —Å–µ—Ç–∏, –≥–¥–µ —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π, –∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ.
   - –ü–æ–∑–≤–æ–ª—è–µ—Ç –æ–±–æ–±—â–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∞ –≤—ã—Å–æ–∫–æ–º —É—Ä–æ–≤–Ω–µ.

3. **–≠–∫—Å–ø–∞–Ω—Å–∏–≤–Ω–∞—è —á–∞—Å—Ç—å (Decoder)**:
   - –°–æ—Å—Ç–æ–∏—Ç –∏–∑:
     - –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–≤—ë—Ä—Ç–æ–∫ (Transposed Convolution) –∏–ª–∏ UpSampling –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è.
     - –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏–∏ —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏–∑ Encoder —á–µ—Ä–µ–∑ skip connections.
     - –î–≤—É—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–≤—ë—Ä—Ç–æ–∫ \(3 \times 3\) (Conv ‚Üí ReLU).

4. **–í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π**:
   - –ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π ‚Äî —Å–≤—ë—Ä—Ç–∫–∞ \(1 \times 1\), –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –º–Ω–æ–≥–æ–∫–∞–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –≤—ã—Ö–æ–¥–Ω–æ–π –∫–∞–Ω–∞–ª (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–∞—Ä—Ç—É —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏).

#### –ì—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ:
```
Input ‚Üí [Encoder] ‚Üí Bottleneck ‚Üí [Decoder + Skip Connections] ‚Üí Output
```

---

### 3. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ residual connections –≤ U-Net**

#### –ó–∞—á–µ–º –Ω—É–∂–Ω—ã skip connections?
1. **–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏**:
   - –í –ø—Ä–æ—Ü–µ—Å—Å–µ —Å–≤—ë—Ä—Ç–∫–∏ –∏ –ø—É–ª–ª–∏–Ω–≥–∞ –¥–µ—Ç–∞–ª–∏ —Ç–µ—Ä—è—é—Ç—Å—è. Skip connections –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç —ç—Ç–∏ –¥–µ—Ç–∞–ª–∏, —Å–æ–µ–¥–∏–Ω—è—è –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ Encoder —Å –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –≤ Decoder.

2. **–£–ª—É—á—à–µ–Ω–∏–µ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏**:
   - –ü—Ä—è–º—ã–µ –ø—É—Ç–∏ –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É—é—Ç –æ–±—É—á–µ–Ω–∏–µ.

#### –ü—Ä–∏–º–µ—Ä —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ residual connections:
–í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–∞—Ö U-Net (–Ω–∞–ø—Ä–∏–º–µ—Ä, ResUNet) –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è **residual connections** –¥–ª—è –∫–∞–∂–¥–æ–π —Å–≤—ë—Ä—Ç–∫–∏:
\[
y = \text{ReLU}(\mathcal{F}(x) + x)
\]
–≥–¥–µ \(\mathcal{F}(x)\) ‚Äî –≤—ã—Ö–æ–¥ –∏–∑ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–≥–æ –±–ª–æ–∫–∞, –∞ \(x\) ‚Äî –≤—Ö–æ–¥–Ω–æ–π —Å–∏–≥–Ω–∞–ª.

---

### 5. **–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ U-Net –≤ –∑–∞–¥–∞—á–µ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏**

#### –ü—Ä–∏–º–µ—Ä –∑–∞–¥–∞—á–∏:
–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≥–¥–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –≤—ã–¥–µ–ª–∏—Ç—å –æ—Ä–≥–∞–Ω—ã –∏–ª–∏ –∞–Ω–æ–º–∞–ª–∏–∏ –Ω–∞ —Å–Ω–∏–º–∫–∞—Ö.

#### –ü—Ä–æ—Ü–µ—Å—Å:
1. **–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ**:
   - –ß—ë—Ä–Ω–æ-–±–µ–ª—ã–µ (1 –∫–∞–Ω–∞–ª) –∏–ª–∏ —Ü–≤–µ—Ç–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (3 –∫–∞–Ω–∞–ª–∞).
2. **–í—ã—Ö–æ–¥**:
   - –ö–∞—Ä—Ç–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, –≥–¥–µ –∫–∞–∂–¥—ã–π –ø–∏–∫—Å–µ–ª—å –æ—Ç–Ω–µ—Å—ë–Ω –∫ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–º—É –∫–ª–∞—Å—Å—É.
3. **–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å**:
   - –î–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: Binary Cross-Entropy (BCE).
   - –î–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: Cross-Entropy Loss.
   - –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Dice Loss –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —É—á–µ—Ç–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π.

---

### 6. **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ U-Net —Å –¥—Ä—É–≥–∏–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏**

| **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞** | **–¶–µ–ª—å**                    | **–ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏**          | **–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏**                         |
|------------------|-----------------------------|------------------------------------|----------------------------------------|
| **U-Net**       | –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π     | Skip connections, —Å–∏–º–º–µ—Ç—Ä–∏—è       | –¢—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –±–æ–ª—å—à–∏–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏     |
| **SegNet**      | –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π     | –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—É–ª–ª–∏–Ω–≥ –∏–Ω–¥–µ–∫—Å—ã        | –í—ã—Å–æ–∫–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã         |
| **DeepLab**     | –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è     | Dilated convolutions              | –ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞              |
| **PSPNet**      | –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è      | Pyramid Pooling Module            | –¢—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏                  |

---

### 7. **–í—ã–≤–æ–¥—ã**

1. **U-Net**:
   - –û—Ç–ª–∏—á–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∑–∞–¥–∞—á —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –±–ª–∞–≥–æ–¥–∞—Ä—è U-–æ–±—Ä–∞–∑–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –∏ skip connections.
2. **Residual connections**:
   - –í–∞—Ä–∏–∞–Ω—Ç—ã U-Net —Å residual connections (–Ω–∞–ø—Ä–∏–º–µ—Ä, ResUNet) —É–ª—É—á—à–∞—é—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è.
3. **–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ**:
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö, —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã—Ö –∏ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
4. **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ**:
   - –ü—Ä–æ—Å—Ç–æ—Ç–∞ U-Net –¥–µ–ª–∞–µ—Ç –µ—ë –±–∞–∑–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π –¥–ª—è –∑–∞–¥–∞—á —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, —Ö–æ—Ç—è –µ—Å—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é.
"""
    else:
        code = """
import torch
import torch.nn as nn

# U-Net –±–ª–æ–∫
class UNetBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(UNetBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        return x

# U-Net –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
class UNet(nn.Module):
    def __init__(self):
        super(UNet, self).__init__()
        # Encoder
        self.enc1 = UNetBlock(1, 64)
        self.enc2 = UNetBlock(64, 128)
        self.enc3 = UNetBlock(128, 256)
        self.enc4 = UNetBlock(256, 512)

        # Bottleneck
        self.bottleneck = UNetBlock(512, 1024)

        # Decoder
        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)
        self.dec4 = UNetBlock(1024, 512)

        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)
        self.dec3 = UNetBlock(512, 256)

        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
        self.dec2 = UNetBlock(256, 128)

        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.dec1 = UNetBlock(128, 64)

        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        self.out = nn.Conv2d(64, 1, kernel_size=1)

    def forward(self, x):
        # Encoder
        enc1 = self.enc1(x)
        enc2 = self.enc2(nn.MaxPool2d(2)(enc1))
        enc3 = self.enc3(nn.MaxPool2d(2)(enc2))
        enc4 = self.enc4(nn.MaxPool2d(2)(enc3))

        # Bottleneck
        bottleneck = self.bottleneck(nn.MaxPool2d(2)(enc4))

        # Decoder
        dec4 = self.dec4(torch.cat([self.up4(bottleneck), enc4], dim=1))
        dec3 = self.dec3(torch.cat([self.up3(dec4), enc3], dim=1))
        dec2 = self.dec2(torch.cat([self.up2(dec3), enc2], dim=1))
        dec1 = self.dec1(torch.cat([self.up1(dec2), enc1], dim=1))

        # –í—ã—Ö–æ–¥
        return self.out(dec1)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏
model = UNet()
x = torch.randn(1, 1, 256, 256)  # –ü—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
output = model(x)
print("Output shape:", output.shape)  # –†–µ–∑—É–ª—å—Ç–∞—Ç: [1, 1, 256, 256]
"""
    pyperclip.copy(code)


def gradcam(idx: int = 0):
    if idx == 0:
        code = """
### 27. –ú–µ—Ö–∞–Ω–∏–∑–º—ã –≤–Ω–∏–º–∞–Ω–∏—è –≤ –º–∞—à–∏–Ω–Ω–æ–º –∑—Ä–µ–Ω–∏–∏ –∏ –º–µ—Ç–æ–¥ Grad-CAM

---

### 1. **–ü–æ–¥—Ö–æ–¥—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –≥–ª—É–±–æ–∫–∏—Ö –º–æ–¥–µ–ª—è—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è**

–ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è (Attention Mechanism) –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –Ω–∞–∏–±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã—Ö —á–∞—Å—Ç—è—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–ª–∏ –¥–∞–Ω–Ω—ã—Ö, —É–ª—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –∑–∞–¥–∞—á–∞—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –¥—Ä—É–≥–∏—Ö.

#### –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤–Ω–∏–º–∞–Ω–∏—è:

1. **Channel Attention**:
   - –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–∞–ª–∞ –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
   - –ü—Ä–∏–º–µ—Ä: Squeeze-and-Excitation Networks (SE-Net), –≥–¥–µ –≤–µ—Å–∞ –∫–∞–Ω–∞–ª–æ–≤ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏.

2. **Spatial Attention**:
   - –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫–∞–∫–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤–∞–∂–Ω—ã –¥–ª—è —Ç–µ–∫—É—â–µ–π –∑–∞–¥–∞—á–∏.
   - –ü—Ä–∏–º–µ—Ä: –≤—ã–¥–µ–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏.

3. **Self-Attention**:
   - –ö–∞–∂–¥–∞—è –ø–æ–∑–∏—Ü–∏—è –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç —Å –¥—Ä—É–≥–∏–º–∏ –ø–æ–∑–∏—Ü–∏—è–º–∏, —á—Ç–æ–±—ã –≤—ã—è–≤–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏.
   - –ü—Ä–∏–º–µ—Ä: Vision Transformers (ViT).

4. **Combined Attention**:
   - –°–æ—á–µ—Ç–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–æ–≤–æ–≥–æ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, CBAM: Convolutional Block Attention Module).

5. **Cross-Attention**:
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö).

---

### 2. **–ú–µ—Ö–∞–Ω–∏–∑–º –∏ –ª–æ–≥–∏–∫–∞ Grad-CAM**

Grad-CAM (Gradient-weighted Class Activation Mapping) ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ—à–µ–Ω–∏—è –≥–ª—É–±–æ–∫–∏—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ —á–∞—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ–∫–∞–∑–∞–ª–∏ –Ω–∞–∏–±–æ–ª—å—à–µ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏.

#### –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è:
- Grad-CAM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ –∞–∫—Ç–∏–≤–∞—Ü–∏—è–º –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ —Å–ª–æ—è, —á—Ç–æ–±—ã –≤—ã—á–∏—Å–ª–∏—Ç—å "–≤–∞–∂–Ω–æ—Å—Ç—å" –∫–∞–∂–¥–æ–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏.

---

#### 2.1 **–ú–µ—Ö–∞–Ω–∏–∑–º Grad-CAM**

1. **–í—ã–±–æ—Ä —Å–ª–æ—è**:
   - –û–±—ã—á–Ω–æ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π —Å–ª–æ–π, —Ç–∞–∫ –∫–∞–∫ –æ–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π.

2. **–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è**:
   - –í—ã—á–∏—Å–ª—è—é—Ç—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤—ã—Ö–æ–¥–∞ –º–æ–¥–µ–ª–∏ (\(y_c\)) –ø–æ –∞–∫—Ç–∏–≤–∞—Ü–∏—è–º \(A_k\) –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Å–ª–æ—è:
     \[
     \frac{\partial y_c}{\partial A_k}
     \]
     –≥–¥–µ \(y_c\) ‚Äî –≤—ã—Ö–æ–¥–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞ \(c\).

3. **–°—Ä–µ–¥–Ω–µ–µ –ø–æ –∫–∞–Ω–∞–ª–∞–º**:
   - –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã —É—Å—Ä–µ–¥–Ω—è—é—Ç—Å—è –ø–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–∑–º–µ—Ä–µ–Ω–∏—è–º, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–∞–ª–∞:
     \[
     \alpha_k^c = \frac{1}{Z} \sum_i \sum_j \frac{\partial y_c}{\partial A_{k, ij}}
     \]
     –≥–¥–µ \(Z\) ‚Äî –æ–±—â–µ–µ —á–∏—Å–ª–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –∫–∞—Ä—Ç–µ \(A_k\).

4. **–í–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–π**:
   - –ê–∫—Ç–∏–≤–∞—Ü–∏–∏ –∫–∞—Ä—Ç—ã \(A_k\) –≤–∑–≤–µ—à–∏–≤–∞—é—Ç—Å—è –ø–æ –∏—Ö –≤–∞–∂–Ω–æ—Å—Ç–∏:
     \[
     L^c_{\text{Grad-CAM}} = \text{ReLU} \left( \sum_k \alpha_k^c A_k \right)
     \]
     –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ ReLU –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ —Ç–æ–ª—å–∫–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –≤–ª–∏—è–Ω–∏—è —É—á–∏—Ç—ã–≤–∞—é—Ç—Å—è.

5. **–ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –∏ –Ω–∞–ª–æ–∂–µ–Ω–∏–µ**:
   - –ö–∞—Ä—Ç–∞ \(L^c_{\text{Grad-CAM}}\) –∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä—É–µ—Ç—Å—è –¥–æ —Ä–∞–∑–º–µ—Ä–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –Ω–∞–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –Ω–µ–≥–æ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.

---

#### 2.2 **–ê–ª–≥–æ—Ä–∏—Ç–º Grad-CAM**

1. –ü—Ä–æ–ø—É—Å—Ç–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å –∏ –ø–æ–ª—É—á–∏—Ç–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ.
2. –í—ã—á–∏—Å–ª–∏—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ –ø–æ –∞–∫—Ç–∏–≤–∞—Ü–∏—è–º —Å–ª–æ—è.
3. –£—Å—Ä–µ–¥–Ω–∏—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–∑–º–µ—Ä–µ–Ω–∏—è–º.
4. –°–ª–æ–∂–∏—Ç–µ –≤–∑–≤–µ—à–µ–Ω–Ω—ã–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∏ –ø—Ä–∏–º–µ–Ω–∏—Ç–µ ReLU.
5. –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–π—Ç–µ –∫–∞—Ä—Ç—É –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–æ —Ä–∞–∑–º–µ—Ä–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
6. –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –∫–∞—Ä—Ç—É –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø–æ–≤–µ—Ä—Ö –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.

---

### 4. **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Grad-CAM**

1. **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å**:
   - –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫–∏–µ —á–∞—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤–ª–∏—è—é—Ç –Ω–∞ —Ä–µ—à–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏.

2. **–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å**:
   - –°–æ–≤–º–µ—Å—Ç–∏–º —Å –ª—é–±–æ–π CNN.

3. **–ü—Ä–æ—Å—Ç–æ—Ç–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏**:
   - –¢—Ä–µ–±—É–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –º–æ–¥–µ–ª–∏.

---

### 5. **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ Grad-CAM —Å –¥—Ä—É–≥–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏**

| **–ú–µ—Ç–æ–¥**        | **–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ**                          | **–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏**                                |
|-------------------|-----------------------------------------|------------------------------------------------|
| **CAM**          | –õ–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤                   | –¢—Ä–µ–±—É–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏           |
| **Grad-CAM**     | –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–Ω–∏–º–∞–Ω–∏—è     | –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –ª—é–±–æ–π CNN                         |
| **Smooth Grad-CAM** | –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –∫–∞—Ä—Ç—ã –≤–Ω–∏–º–∞–Ω–∏—è             | –°–Ω–∏–∂–∞–µ—Ç —à—É–º, —É–≤–µ–ª–∏—á–∏–≤–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å          |
| **Attention Maps** | –ö–∞—Ä—Ç—ã –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è Transformer –º–æ–¥–µ–ª–µ–π | –û—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏        |

---

### 6. **–í—ã–≤–æ–¥—ã**

1. **–ú–µ—Ö–∞–Ω–∏–∑–º—ã –≤–Ω–∏–º–∞–Ω–∏—è**:
   - –£—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤—É—é—Ç –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –≤–∞–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö.
2. **Grad-CAM**:
   - –£–¥–æ–±–Ω—ã–π –∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –≥–ª—É–±–æ–∫–∏—Ö –º–æ–¥–µ–ª–µ–π.
3. **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ**:
   - Grad-CAM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –æ—à–∏–±–æ–∫ –º–æ–¥–µ–ª–µ–π, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π.
"""
    else:
        code = """
### 3. **–†–µ–∞–ª–∏–∑–∞—Ü–∏—è Grad-CAM –≤ PyTorch**
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
model = models.resnet50(pretrained=True)
model.eval()

# –§—É–Ω–∫—Ü–∏—è Grad-CAM
class GradCAM:
    def __init__(self, model, target_layer):
        self.model = model
        self.target_layer = target_layer
        self.gradients = None

        # –•—É–∫ –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        self.target_layer.register_backward_hook(self.save_gradients)

    def save_gradients(self, module, grad_in, grad_out):
        self.gradients = grad_out[0]

    def __call__(self, x, class_idx):
        # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
        activations = None
        for name, module in self.model.named_children():
            x = module(x)
            if name == self.target_layer:
                activations = x

        # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
        self.model.zero_grad()
        class_score = x[:, class_idx].squeeze()
        class_score.backward()

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Grad-CAM
        pooled_gradients = torch.mean(self.gradients, dim=[0, 2, 3])
        activations = activations[0]  # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–π

        for i in range(activations.shape[0]):
            activations[i, :, :] *= pooled_gradients[i]

        heatmap = torch.mean(activations, dim=0).detach().numpy()
        heatmap = np.maximum(heatmap, 0)
        heatmap /= np.max(heatmap)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        return heatmap

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Grad-CAM
image_path = 'example.jpg'
image = Image.open(image_path).convert('RGB')
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

input_tensor = transform(image).unsqueeze(0)

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Grad-CAM
grad_cam = GradCAM(model, target_layer='layer4')
heatmap = grad_cam(input_tensor, class_idx=243)  # –ù–∞–ø—Ä–∏–º–µ—Ä, –∫–ª–∞—Å—Å 243 (—Å–æ–±–∞–∫–∞)

# –ù–∞–ª–æ–∂–µ–Ω–∏–µ heatmap –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
heatmap = np.uint8(255 * heatmap)
heatmap = Image.fromarray(heatmap).resize(image.size, Image.BILINEAR)
heatmap = np.array(heatmap)
superimposed_img = np.array(image) * 0.5 + heatmap[:, :, np.newaxis] * 0.5

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.imshow(superimposed_img.astype('uint8'))
plt.axis('off')
plt.show()
"""
    pyperclip.copy(code)


def vit_swin(idx: int = 0):
    if idx == 0:
        code = """
### 28. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Transformer –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Swin Transformer

---

### 1. **–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥—É–ª—è Transformer –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏**

Transformer ‚Äî –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–µ–∫—Å—Ç–∞), –±–ª–∞–≥–æ–¥–∞—Ä—è –º–µ—Ö–∞–Ω–∏–∑–º—É **Self-Attention**. –í –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏ (CV) Transformer –±—ã–ª –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∑–∞–º–µ–Ω—è—è —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏.

#### –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è:
1. **–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å**:
   - –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–µ–ª–∏—Ç—Å—è –Ω–∞ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ç—á–∏ (\(16 \times 16\)).
   - –ö–∞–∂–¥—ã–π –ø–∞—Ç—á –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è –≤ –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–µ—Ä–µ–∑ –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π).

2. **Self-Attention**:
   - –ü–æ–∑–≤–æ–ª—è–µ—Ç –∫–∞–∂–¥–æ–º—É –ø–∞—Ç—á—É –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å –¥—Ä—É–≥–∏–º–∏, –≤—ã—è–≤–ª—è—è –∫–∞–∫ –ª–æ–∫–∞–ª—å–Ω—ã–µ, —Ç–∞–∫ –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏.

3. **Embedding –∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è**:
   - –î–ª—è —É—á–µ—Ç–∞ –ø–æ—Ä—è–¥–∫–∞ –ø–∞—Ç—á–µ–π –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏.

4. **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–ª–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è**:
   - –ù–∞ –≤—ã—Ö–æ–¥–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–∞—è –∏–ª–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω–∞—è –≥–æ–ª–æ–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, MLP-—Å–ª–æ–π).

#### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Transformers –≤ CV:
- **–ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç**: Self-Attention –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏ –º–µ–∂–¥—É –ª—é–±—ã–º–∏ —á–∞—Å—Ç—è–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
- **–ì–∏–±–∫–æ—Å—Ç—å**: –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤.
- **–ü—Ä–æ—Å—Ç–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**: –ò–∑–±–∞–≤–ª—è–µ—Ç—Å—è –æ—Ç –∂–µ—Å—Ç–∫–∏—Ö –ª–æ–∫–∞–ª—å–Ω—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–ª–æ–µ–≤.

#### –ü—Ä–∏–º–µ—Ä—ã –º–æ–¥–µ–ª–µ–π:
- **Vision Transformer (ViT)**: –ü–µ—Ä–≤—ã–π Transformer, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è CV.
- **DeiT (Data-Efficient Transformer)**: –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è ViT —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.
- **Swin Transformer**: –ü—Ä–∏–º–µ–Ω—è–µ—Ç –æ–∫–æ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —É–ª—É—á—à–∞—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å.

---

### 2. **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Swin Transformer**

Swin Transformer (Shifted Window Transformer) –±—ã–ª –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –≤ —Ä–∞–±–æ—Ç–µ *"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"*. –≠—Ç–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç ViT —É–ª—É—á—à–µ–Ω–∏–µ–º –ª–æ–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏.

#### –û—Å–Ω–æ–≤–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ Swin Transformer:
1. **–û–∫–æ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ (Window Attention)**:
   - –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–µ–ª–∏—Ç—Å—è –Ω–∞ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ–∫–Ω–∞ (\(7 \times 7\)), –∏ Self-Attention –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ –æ–∫–Ω–∞.
   - –°–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Å \(O(N^2)\) –¥–æ \(O((M^2) \cdot N / M)\), –≥–¥–µ \(M\) ‚Äî —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞, \(N\) ‚Äî –æ–±—â–µ–µ —á–∏—Å–ª–æ –ø–∞—Ç—á–µ–π.

2. **–°–¥–≤–∏–≥ –æ–∫–æ–Ω (Shifted Windows)**:
   - –ß—Ç–æ–±—ã —É—á–µ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –æ–∫–Ω–∞, –æ–∫–Ω–∞ —Å–¥–≤–∏–≥–∞—é—Ç—Å—è –Ω–∞ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Å–ª–æ—è–º–∏.

3. **–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞**:
   - –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç ViT, Swin Transformer —Å—Ç—Ä–æ–∏—Ç –∏–µ—Ä–∞—Ä—Ö–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–∫–∞–∫ –≤ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–µ—Ç—è—Ö), —É–º–µ–Ω—å—à–∞—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –∫–∞–∂–¥–æ–º —É—Ä–æ–≤–Ω–µ –∏ —É–≤–µ–ª–∏—á–∏–≤–∞—è —á–∏—Å–ª–æ –∫–∞–Ω–∞–ª–æ–≤.

4. **–°–∫–∞–ª–∏—Ä—É–µ–º–æ—Å—Ç—å**:
   - –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è, –¥–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤).

---

#### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ Swin Transformer:

1. **Patch Partition**:
   - –î–µ–ª–∏—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ –ø–∞—Ç—á–∏ (\(4 \times 4\)) –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏—Ö –∫–∞–∫ –≤–µ–∫—Ç–æ—Ä—ã.
   - –í—ã—Ö–æ–¥: \(H/4 \times W/4 \times C\), –≥–¥–µ \(H\) –∏ \(W\) ‚Äî —Ä–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, \(C\) ‚Äî —á–∏—Å–ª–æ –∫–∞–Ω–∞–ª–æ–≤.

2. **Patch Merging**:
   - –£–º–µ–Ω—å—à–∞–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ, —É–≤–µ–ª–∏—á–∏–≤–∞—è —á–∏—Å–ª–æ –∫–∞–Ω–∞–ª–æ–≤.
   - –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –æ–ø–µ—Ä–∞—Ü–∏—è–º –ø—É–ª–ª–∏–Ω–≥–∞ –≤ CNN.

3. **Shifted Window Attention**:
   - –ö–∞–∂–¥—ã–π –±–ª–æ–∫ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö —ç—Ç–∞–ø–æ–≤:
     - **–û–∫–æ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ (Window Attention)**: Self-Attention –≤–Ω—É—Ç—Ä–∏ –æ–∫–æ–Ω.
     - **–°–¥–≤–∏–≥ –æ–∫–æ–Ω (Shifted Window Attention)**: –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —Å–æ—Å–µ–¥–Ω–∏–º–∏ –æ–∫–Ω–∞–º–∏.

4. **MLP**:
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ.

5. **–í—ã—Ö–æ–¥–Ω–∞—è –≥–æ–ª–æ–≤–∞**:
   - –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–ª–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏).

---

#### –ü—Ä–∏–º–µ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Swin Transformer –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:

| –≠—Ç–∞–ø                     | –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ | –ß–∏—Å–ª–æ –æ–∫–æ–Ω | –û–ø–µ—Ä–∞—Ü–∏–∏                     |
|---------------------------|----------------------------|------------|------------------------------|
| Patch Partition           | \(224 \times 224 \to 56 \times 56\) | ‚Äî          | –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø–∞—Ç—á–∏          |
| Stage 1                   | \(56 \times 56\)          | \(8 \times 8\) | Window Attention             |
| Stage 2                   | \(28 \times 28\)          | \(4 \times 4\) | Shifted Window Attention     |
| Stage 3                   | \(14 \times 14\)          | \(2 \times 2\) | Shifted Window Attention     |
| Stage 4                   | \(7 \times 7\)            | \(1 \times 1\) | Global Attention (–ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç) |

---

### 4. **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ Swin Transformer —Å –¥—Ä—É–≥–∏–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏**

| **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**       | **–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏**                            | **–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞**                        | **–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏**                         |
|-----------------------|--------------------------------------------|-----------------------------------------|----------------------------------------|
| **CNN (ResNet)**      | –õ–æ–∫–∞–ª—å–Ω—ã–µ —Å–≤—ë—Ä—Ç–∫–∏                         | –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å                          | –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç       |
| **ViT**               | –ì–ª–æ–±–∞–ª—å–Ω–æ–µ Self-Attention                 | –ú–æ—â–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤             | –í—ã—Å–æ–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö   |
| **Swin Transformer**  | –õ–æ–∫–∞–ª—å–Ω–æ–µ –æ–∫–æ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ + –∏–µ—Ä–∞—Ä—Ö–∏—è     | –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å         | –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏                   |

---

### 5. **–í—ã–≤–æ–¥—ã**

1. **Transformers –≤ CV**:
   - –û–Ω–∏ –∑–∞–º–µ–Ω—è—é—Ç CNN –≤ –∑–∞–¥–∞—á–∞—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –±–ª–∞–≥–æ–¥–∞—Ä—è –º–µ—Ö–∞–Ω–∏–∑–º—É –≤–Ω–∏–º–∞–Ω–∏—è.
2. **Swin Transformer**:
   - –ü—Ä–∏–º–µ–Ω—è–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–µ –æ–∫–æ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∏ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
3. **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ**:
   - Swin Transformer –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —à–∏—Ä–æ–∫–æ–≥–æ —Å–ø–µ–∫—Ç—Ä–∞ –∑–∞–¥–∞—á –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, —Å–æ—á–µ—Ç–∞—è –≥–∏–±–∫–æ—Å—Ç—å ViT –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å CNN.
"""
    else:
        code = """
### 3. **–†–µ–∞–ª–∏–∑–∞—Ü–∏—è Swin Transformer –≤ PyTorch**
import torch
import torch.nn as nn

class WindowAttention(nn.Module):
    def __init__(self, dim, num_heads, window_size):
        super(WindowAttention, self).__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.window_size = window_size
        self.scale = (dim // num_heads) ** -0.5
        self.softmax = nn.Softmax(dim=-1)

        self.qkv = nn.Linear(dim, dim * 3, bias=True)
        self.proj = nn.Linear(dim, dim)

    def forward(self, x):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = self.softmax(attn)
        out = (attn @ v).transpose(1, 2).reshape(B, N, C)
        return self.proj(out)

class SwinBlock(nn.Module):
    def __init__(self, dim, input_resolution, num_heads, window_size, shift_size):
        super(SwinBlock, self).__init__()
        self.window_size = window_size
        self.shift_size = shift_size
        self.attn = WindowAttention(dim, num_heads, window_size)

    def forward(self, x):
        # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –æ–∫–æ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –∏ —Å–¥–≤–∏–≥–∞
        B, H, W, C = x.shape
        x = x.view(B, H * W, C)
        x = self.attn(x)
        return x.view(B, H, W, C)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏
B, H, W, C = 1, 56, 56, 96
x = torch.randn(B, H, W, C)
block = SwinBlock(dim=96, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0)
output = block(x)
print("Output shape:", output.shape)
"""
    pyperclip.copy(code)


def decode(idx: int = 0):
    if idx == 0:
        code = """
### 29. –í–µ–∫—Ç–æ—Ä–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –≤–µ—Å–æ–≤ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–≥–æ –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞ –∏ –∑–∞–¥–∞—á–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞ –ø–æ—Å–ª–µ —Å–∂–∞—Ç–∏—è

---

### 1. **–í–µ–∫—Ç–æ—Ä–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –≤–µ—Å–æ–≤ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–≥–æ –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–∞**

–í –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–º –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω–µ (MLP) –≤–µ—Å–∞ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è –º–æ–∂–Ω–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ **–≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ** –≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

#### –û—Å–Ω–æ–≤–Ω—ã–µ –∏–¥–µ–∏:

1. **–ü—Ä–æ–µ–∫—Ü–∏–∏ –Ω–∞ –Ω–æ–≤–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ**:
   - –ö–∞–∂–¥—ã–π –Ω–µ–π—Ä–æ–Ω —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤—Ö–æ–¥–Ω–æ–π –≤–µ–∫—Ç–æ—Ä –≤ –Ω–æ–≤–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ, –≤—ã–ø–æ–ª–Ω—è—è –ª–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ:
     \[
     z_j = \sum_{i} w_{ij} x_i + b_j
     \]
     –≥–¥–µ \(x_i\) ‚Äî –≤—Ö–æ–¥–Ω–æ–π —Å–∏–≥–Ω–∞–ª, \(w_{ij}\) ‚Äî –≤–µ—Å–∞, \(b_j\) ‚Äî —Å–º–µ—â–µ–Ω–∏–µ.

   - –í–µ—Å–∞ \(w_{ij}\) –º–æ–∂–Ω–æ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –±–∞–∑–∏—Å–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –≤ –Ω–æ–≤–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, –∞ –ª–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∫–∞–∫ –ø—Ä–æ–µ–∫—Ü–∏—é –≤—Ö–æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –Ω–∞ —ç—Ç–æ—Ç –±–∞–∑–∏—Å.

2. **–í—ã—è–≤–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π**:
   - –ù–µ–π—Ä–æ–Ω—ã —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è "—É—á–∞—Ç" –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, –æ—Ç—Ä–∞–∂–∞—é—â–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö.

3. **–ò–∑–º–µ—Ä–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤**:
   - –í–µ—Å–∞ –Ω–µ–π—Ä–æ–Ω–∞ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, –∫–∞–∫–∏–µ –≤—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã \(x_i\)) –∏–º–µ—é—Ç –Ω–∞–∏–±–æ–ª—å—à–µ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –µ–≥–æ –∞–∫—Ç–∏–≤–∞—Ü–∏—é.

#### –ü—Ä–∏–º–µ—Ä –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏:
- –î–ª—è –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤–µ—Å–∞ –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è –º–æ–≥—É—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å —à–∞–±–ª–æ–Ω—ã –∏–ª–∏ "–∫—Ä–∞—è", –∞ –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ —Å–ª–æ–∏ ‚Äî –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ —Ñ–æ—Ä–º—ã.

---

### 2. **–ó–∞–¥–∞—á–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞ –ø–æ—Å–ª–µ —Å–∂–∞—Ç–∏—è**

–≠—Ç–∞ –∑–∞–¥–∞—á–∞ —Å–≤—è–∑–∞–Ω–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è (—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏) –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏—Ö —Å–∂–∞—Ç–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è. –û–Ω–∞ –ª–µ–∂–∏—Ç –≤ –æ—Å–Ω–æ–≤–µ **–∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤**.

#### –ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏:
1. **–°–∂–∞—Ç–∏–µ** (Encoding):
   - –í—Ö–æ–¥–Ω–æ–π —Å–∏–≥–Ω–∞–ª \(x \in \mathbb{R}^n\) –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è –≤ —Å–∫—Ä—ã—Ç–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ \(h \in \mathbb{R}^m\), –≥–¥–µ \(m < n\):
     \[
     h = f_{\text{encode}}(x) = \sigma(W_{\text{encode}} x + b_{\text{encode}})
     \]
     –≥–¥–µ \(W_{\text{encode}}\) ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ –≤–µ—Å–æ–≤, \(b_{\text{encode}}\) ‚Äî —Å–º–µ—â–µ–Ω–∏–µ.

2. **–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ** (Decoding):
   - –°–∫—Ä—ã—Ç–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ \(h\) –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è –æ–±—Ä–∞—Ç–Ω–æ –≤ –∏—Å—Ö–æ–¥–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ:
     \[
     \hat{x} = f_{\text{decode}}(h) = \sigma(W_{\text{decode}} h + b_{\text{decode}})
     \]

3. **–¶–µ–ª—å**:
   - –ú–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –≤—Ö–æ–¥–æ–º \(x\) –∏ –µ–≥–æ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π \(\hat{x}\) —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å:
     \[
     \mathcal{L}(x, \hat{x}) = \|x - \hat{x}\|^2
     \]

---

### 3. **–ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã (Autoencoders)**

–ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã ‚Äî —ç—Ç–æ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏, –æ–±—É—á–∞–µ–º—ã–µ –¥–ª—è —Å–∂–∞—Ç–∏—è –¥–∞–Ω–Ω—ã—Ö (–≤ Encoder) –∏ –∏—Ö –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è (–≤ Decoder).

#### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
1. **Encoder**:
   - –°–∂–∏–º–∞–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –º–µ–Ω—å—à–µ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏.
   - –ù–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è \(28 \times 28\) (784 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–∂–∞—Ç–æ –¥–æ 32 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.

2. **Latent Space**:
   - –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π, –≤ –∫–æ—Ç–æ—Ä–æ–º –¥–∞–Ω–Ω—ã–µ –∏–º–µ—é—Ç –∫–æ–º–ø–∞–∫—Ç–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ.

3. **Decoder**:
   - –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å–∫—Ä—ã—Ç–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è.

#### –ü—Ä–∏–º–µ—Ä:
- –í—Ö–æ–¥: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.
- –°–∫—Ä—ã—Ç–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ: –∫–æ–º–ø–∞–∫—Ç–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏.
- –í—ã—Ö–æ–¥: –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.

---

### 5. **–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤**

1. **–°–∂–∞—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö**:
   - –ü—Ä–∏–º–µ—Ä: —É–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–ª–∏ –∑–≤—É–∫–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤.
2. **–£–¥–∞–ª–µ–Ω–∏–µ —à—É–º–∞ (Denoising Autoencoder)**:
   - –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å —à—É–º–æ–º, –≤—ã—Ö–æ–¥ ‚Äî –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.
3. **–ê–Ω–æ–º–∞–ª–∏—è –¥–µ—Ç–µ–∫—Ü–∏–∏**:
   - –ù–∏–∑–∫–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
4. **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö**:
   - –ù–∞ –æ—Å–Ω–æ–≤–µ –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤ (VAE).

---

### 6. **–í—ã–≤–æ–¥—ã**

1. **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –≤–µ—Å–æ–≤ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è**:
   - –í–µ—Å–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π –±–∞–∑–∏—Å–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ, –æ–ø–∏—Å—ã–≤–∞—é—â–µ–µ –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–∞–Ω–Ω—ã—Ö.
2. **–†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö**:
   - –ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã –æ–±—É—á–∞—é—Ç—Å—è –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –≤—Ö–æ–¥—ã –∏–∑ –∏—Ö —Å–∂–∞—Ç–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è.
3. **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ**:
   - –ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏, —É–¥–∞–ª–µ–Ω–∏—è —à—É–º–∞, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π.
"""
    else:
        code = """
import torch
import torch.nn as nn
import torch.optim as optim

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞
class Autoencoder(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(Autoencoder, self).__init__()
        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU()
        )
        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim, input_dim),
            nn.Sigmoid()  # –î–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded

# –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è
input_dim = 784  # –î–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π 28x28
hidden_dim = 32

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
model = Autoencoder(input_dim, hidden_dim)

# –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑ MNIST)
data = torch.randn(64, input_dim)  # –ú–∏–Ω–∏-–±–∞—Ç—á –∏–∑ 64 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π 28x28

# –û–±—É—á–µ–Ω–∏–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞
for epoch in range(100):
    optimizer.zero_grad()
    output = model(data)
    loss = criterion(output, data)  # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–∞ —Å –≤—Ö–æ–¥–æ–º
    loss.backward()
    optimizer.step()
    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch+1}/100], Loss: {loss.item():.4f}")
"""
    pyperclip.copy(code)


def autoencoder(idx: int = 0):
    if idx == 0:
        code = """
### 30. –ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã: –ª–∏–Ω–µ–π–Ω—ã–µ, –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ, –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ, –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –∏ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è

---

### 1. **–ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã**

–ê–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã (Autoencoders) ‚Äî —ç—Ç–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–∞—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫–æ–º–ø–∞–∫—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö, –Ω–∞–∑—ã–≤–∞–µ–º–æ–≥–æ **–ª–∞—Ç–µ–Ω—Ç–Ω—ã–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ–º (latent space)**. –û–Ω–∏ —Å–æ—Å—Ç–æ—è—Ç –∏–∑ –¥–≤—É—Ö —á–∞—Å—Ç–µ–π:
- **Encoder**: –°–∂–∏–º–∞–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ \(x\) –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ \(h\).
- **Decoder**: –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ \(\hat{x}\) –∏–∑ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è \(h\).

#### –¶–µ–ª—å:
–ú–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ—à–∏–±–∫—É —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –º–µ–∂–¥—É –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ \(x\) –∏ –∏—Ö –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º \(\hat{x}\):
\[
\mathcal{L}(x, \hat{x}) = \|x - \hat{x}\|^2
\]

---

### 2. **–õ–∏–Ω–µ–π–Ω—ã–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä**

#### –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:
–õ–∏–Ω–µ–π–Ω—ã–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –ª–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è:
\[
\text{Encoder: } h = Wx + b, \quad \text{Decoder: } \hat{x} = W'h + b'
\]
–≥–¥–µ \(W\), \(W'\) ‚Äî –º–∞—Ç—Ä–∏—Ü—ã –≤–µ—Å–æ–≤, \(b\), \(b'\) ‚Äî —Å–º–µ—â–µ–Ω–∏—è.

#### –°–≤—è–∑—å —Å PCA:
–õ–∏–Ω–µ–π–Ω—ã–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–µ–Ω **–∞–Ω–∞–ª–∏–∑—É –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (PCA)**, –µ—Å–ª–∏:
1. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (\(L_2\)-–Ω–æ—Ä–º–∞).
2. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ä–∞–≤–Ω–æ —á–∏—Å–ª—É –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç.

**–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ:**
- –õ–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ª–∏–Ω–µ–π–Ω–æ–≥–æ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –ø–æ–¥–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ–º, –Ω–∞–π–¥–µ–Ω–Ω—ã–º –º–µ—Ç–æ–¥–æ–º PCA.
- –í–µ—Å–∞ \(W\) –ª–∏–Ω–µ–π–Ω–æ–≥–æ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ –ø—Ä–∏–±–ª–∏–∂–∞—é—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –º–∞—Ç—Ä–∏—Ü—ã –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö.

#### –ü—Ä–∏–º–µ—Ä:
–ï—Å–ª–∏ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ \(x \in \mathbb{R}^{n}\), –∞ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏–º–µ–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å \(k\), —Ç–æ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä –±—É–¥–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å –ø–µ—Ä–≤—ã–µ \(k\) –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç.

---

### 3. **–ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∏ –≥–ª—É–±–æ–∫–∏–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã**

#### –ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã:
- –ò—Å–ø–æ–ª—å–∑—É—é—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ReLU, Sigmoid).
- –°–ø–æ—Å–æ–±–Ω—ã –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤ –¥–∞–Ω–Ω—ã—Ö.

#### –ì–ª—É–±–æ–∫–∏–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã:
- –°–æ—Å—Ç–æ—è—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤ –∫–∞–∫ –≤ Encoder, —Ç–∞–∫ –∏ –≤ Decoder.
- –ö–∞–∂–¥—ã–π —Å–ª–æ–π –∏–∑—É—á–∞–µ—Ç –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö.
- –ü—Ä–∏–º–µ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã:
  - Encoder: \(784 \to 512 \to 256 \to 64\).
  - Decoder: \(64 \to 256 \to 512 \to 784\).

#### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
- –ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –ª–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–∞–Ω–Ω—ã–º–∏, –ª–µ–∂–∞—â–∏–º–∏ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏—è—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Ç–µ–∫—Å—Ç, –∑–≤—É–∫).

---

### 4. **–û–±–ª–∞—Å—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏–π –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞**

#### –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:
–õ–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ ‚Äî —ç—Ç–æ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –Ω–æ–≤–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –û–Ω–æ –¥–æ–ª–∂–Ω–æ:
1. –ó–∞—Ö–≤–∞—Ç—ã–≤–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–∞–Ω–Ω—ã—Ö.
2. –û–±–µ—Å–ø–µ—á–∏–≤–∞—Ç—å —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –≤—Ö–æ–¥–æ–≤.

#### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞:
- **–õ–æ–∫–∞–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞**:
  - –ë–ª–∏–∑–∫–∏–µ —Ç–æ—á–∫–∏ –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Å—Ö–æ–∂–∏–º –¥–∞–Ω–Ω—ã–º.
- **–ì–ª–æ–±–∞–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞**:
  - –õ–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –æ—Ç—Ä–∞–∂–∞–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö.

#### –ü—Ä–æ–±–ª–µ–º—ã:
1. –í –ª–∏–Ω–µ–π–Ω—ã—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞—Ö –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏–º–µ–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—É—é –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.
2. –ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã —á–∞—Å—Ç–æ —Å–æ–∑–¥–∞—é—Ç –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –±–µ–∑ —è–≤–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, —á—Ç–æ –∑–∞—Ç—Ä—É–¥–Ω—è–µ—Ç –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—é.

---

### 5. **–ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –∏ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ**

#### –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è:
- –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—ã–µ —Ç–æ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö, –Ω–∞—Ö–æ–¥—è—â–∏–µ—Å—è –º–µ–∂–¥—É –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏.
- –ü—Ä–∏–º–µ—Ä: –õ–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–æ—á–∫–∞–º–∏ \(h_1\) –∏ \(h_2\) –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ:
  \[
  h_{\text{interp}} = \alpha h_1 + (1 - \alpha) h_2, \quad \alpha \in [0, 1]
  \]

#### –≠–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è:
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –æ–±–ª–∞—Å—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏–π –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞.
- –ü—Ä–æ–±–ª–µ–º–∞: –ú–æ–∂–µ—Ç –ø—Ä–∏–≤–æ–¥–∏—Ç—å –∫ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º –∏–ª–∏ –Ω–µ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º, —Ç–∞–∫ –∫–∞–∫ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã –Ω–µ –æ–±—É—á–µ–Ω—ã –Ω–∞ —Ç–∞–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö.

---

### 7. **–í—ã–≤–æ–¥—ã**

1. **–õ–∏–Ω–µ–π–Ω—ã–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä**:
   - –≠–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–µ–Ω PCA –∏ –ø–æ–ª–µ–∑–µ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª–∏–Ω–µ–π–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π.
2. **–ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∏ –≥–ª—É–±–æ–∫–∏–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã**:
   - –°–ø–æ—Å–æ–±–Ω—ã –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö.
3. **–õ–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ**:
   - –ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∂–∞—Ç—É—é —Ñ–æ—Ä–º—É –¥–∞–Ω–Ω—ã—Ö, —Å–æ—Ö—Ä–∞–Ω—è—è –∏—Ö –∫–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏.
4. **–ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –∏ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è**:
   - –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –≤–Ω—É—Ç—Ä–∏ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, —Ç–æ–≥–¥–∞ –∫–∞–∫ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è —á–∞—Å—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–µ–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º.
"""
    else:
        code = """
import torch
import torch.nn as nn
import numpy as np

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–≤—É—Ö –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö —Ç–æ—á–µ–∫
h1 = torch.tensor([0.1, 0.2, 0.3])
h2 = torch.tensor([0.5, 0.6, 0.7])

# –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è
alpha = 0.5
h_interp = alpha * h1 + (1 - alpha) * h2
print("Interpolated point:", h_interp.numpy())

# –≠–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è
alpha_ext = 1.5
h_extrap = alpha_ext * h1 + (1 - alpha_ext) * h2
print("Extrapolated point:", h_extrap.numpy())

#### MNIST (—Ä—É–∫–æ–ø–∏—Å–Ω—ã–µ —Ü–∏—Ñ—Ä—ã):
1. –ü–æ—Å—Ç—Ä–æ–π—Ç–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä.
2. –ü—Ä–µ–æ–±—Ä–∞–∑—É–π—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ.
3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ T-SNE –∏–ª–∏ PCA –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π.

#### –ü—Ä–∏–º–µ—Ä –∫–æ–¥–∞:
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ latent_space —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–∫—Ä—ã—Ç—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
latent_space = torch.randn(1000, 2).numpy()  # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ
labels = np.random.randint(0, 10, 1000)     # –ú–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ü–∏—Ñ—Ä—ã)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å T-SNE
tsne = TSNE(n_components=2, random_state=42)
latent_2d = tsne.fit_transform(latent_space)

plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=labels, cmap='tab10', s=5)
plt.colorbar()
plt.title("Latent Space Visualization")
plt.show()
"""
    pyperclip.copy(code)


def discr_models(idx: int = 0):
    if idx == 0:
        code = """
        ### 31. –î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω—ã–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏: –∑–∞–¥–∞—á–∏, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ

---

### 1. **–î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω—ã–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏**

#### –î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏:
- **–¶–µ–ª—å**: –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –º–µ—Ç–∫–∏ \(y\) –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö \(x\).
- **–ü—Ä–∏–º–µ—Ä –∑–∞–¥–∞—á–∏**:
  - –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: \(P(y|x)\), –≥–¥–µ \(y\) ‚Äî –º–µ—Ç–∫–∞, –∞ \(x\) ‚Äî –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.
- **–ü–æ–¥—Ö–æ–¥**:
  - –ò–∑—É—á–∞—é—Ç –≥—Ä–∞–Ω–∏—Ü—É –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏.
- **–ü—Ä–∏–º–µ—Ä—ã**:
  - –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è, SVM, –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ResNet).

#### –ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏:
- **–¶–µ–ª—å**: –ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö \(P(x)\) –∏–ª–∏ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ \(P(x, y)\).
- **–ü—Ä–∏–º–µ—Ä –∑–∞–¥–∞—á–∏**:
  - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ø–æ—Ö–æ–∂–∏—Ö –Ω–∞ –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É.
- **–ü–æ–¥—Ö–æ–¥**:
  - –ò–∑—É—á–∞—é—Ç, –∫–∞–∫ –¥–∞–Ω–Ω—ã–µ \(x\) —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ.
- **–ü—Ä–∏–º–µ—Ä—ã**:
  - Variational Autoencoders (VAE), Generative Adversarial Networks (GAN), –Ω–æ—Ä–º–∞–ª–∏–∑—É—é—â–∏–µ –ø–æ—Ç–æ–∫–∏.

---

### 2. **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω—ã—Ö –∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π**

| **–ê—Å–ø–µ–∫—Ç**               | **–î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏**           | **–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏**             |
|---------------------------|---------------------------------------|-------------------------------------|
| **–¶–µ–ª—å**                 | –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–µ—Ç–æ–∫ \(y\)             | –ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö \(x\)         |
| **–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö** | –ù–µ –º–æ–¥–µ–ª–∏—Ä—É—é—Ç \(P(x)\)               | –ò–∑—É—á–∞—é—Ç \(P(x)\) –∏–ª–∏ \(P(x, y)\)   |
| **–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ**           | –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, —Ä–µ–≥—Ä–µ—Å—Å–∏—è             | –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö, –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è |
| **–ü—Ä–∏–º–µ—Ä—ã**              | SVM, –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è, ResNet | VAE, GAN, Normalizing Flows        |
| **–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å**          | –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏       | –£–º–µ—é—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ    |

---

### 3. **–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π**

–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–µ–ª—è—Ç—Å—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–æ –ø—Ä–∏–Ω—Ü–∏–ø—É —Ä–∞–±–æ—Ç—ã:

#### 3.1. **–õ–∞—Ç–µ–Ω—Ç–Ω–æ-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏**
- –ú–æ–¥–µ–ª–∏—Ä—É—é—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö \(P(x)\) —á–µ—Ä–µ–∑ —Å–∫—Ä—ã—Ç–æ–µ (–ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ) –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ \(z\).
- –ü—Ä–∏–º–µ—Ä—ã:
  - **Variational Autoencoders (VAE)**:
    - –ò—Å–ø–æ–ª—å–∑—É—é—Ç –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—é –±–∞–π–µ—Å–æ–≤—Å–∫–æ–≥–æ –≤—ã–≤–æ–¥–∞ –¥–ª—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞.
    - –ü–æ–∑–≤–æ–ª—è—é—Ç –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —á–µ—Ä–µ–∑ \(z\).
  - **Generative Adversarial Networks (GAN)**:
    - –ò—Å–ø–æ–ª—å–∑—É—é—Ç —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –º–µ–∂–¥—É –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–º –∏ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–æ–º.
    - –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ, –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∏—Ö –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å.

#### 3.2. **–ù–æ—Ä–º–∞–ª–∏–∑—É—é—â–∏–µ –º–æ–¥–µ–ª–∏**
- –ú–æ–¥–µ–ª–∏—Ä—É—é—Ç –ø–ª–æ—Ç–Ω–æ—Å—Ç—å \(P(x)\) –Ω–∞–ø—Ä—è–º—É—é, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è \(P(x)\) —Ç–æ—á–Ω–æ.
- –ü—Ä–∏–º–µ—Ä—ã:
  - **Normalizing Flows**:
    - –ü—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç –¥–∞–Ω–Ω—ã–µ –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —á–µ—Ä–µ–∑ –æ–±—Ä–∞—Ç–∏–º—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è.
    - –ü—Ä–∏–º–µ—Ä: RealNVP, Glow.

#### 3.3. **–≠–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏**
- –ú–æ–¥–µ–ª–∏—Ä—É—é—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ \(P(x)\) —á–µ—Ä–µ–∑ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫—É—é —Ñ—É–Ω–∫—Ü–∏—é \(E(x)\):
  \[
  P(x) \propto \exp(-E(x))
  \]
- –ü—Ä–∏–º–µ—Ä—ã:
  - Restricted Boltzmann Machines (RBM).
  - Energy-Based Models (EBM).

#### 3.4. **–ê–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏**
- –†–∞–∑–±–∏–≤–∞—é—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ \(P(x)\) –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —É—Å–ª–æ–≤–Ω—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π:
  \[
  P(x) = \prod_{i=1}^n P(x_i | x_1, x_2, \ldots, x_{i-1})
  \]
- –ü—Ä–∏–º–µ—Ä—ã:
  - PixelCNN, PixelRNN.

---

### 4. **–ü—Ä–∏–∫–ª–∞–¥–Ω—ã–µ –∑–∞–¥–∞—á–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏**

–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ —à–∏—Ä–æ–∫–æ–º —Å–ø–µ–∫—Ç—Ä–µ –ø—Ä–∏–∫–ª–∞–¥–Ω—ã—Ö –∑–∞–¥–∞—á:

#### 4.1. **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π**
- –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø–æ—Ö–æ–∂–∏—Ö –Ω–∞ –¥–∞–Ω–Ω—ã–µ –∏–∑ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏.
- –ü—Ä–∏–º–µ—Ä: GAN –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏.

#### 4.2. **–°—Ç–∏–ª–∏–∑–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π**
- –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Å—Ç–∏–ª—å.
- –ü—Ä–∏–º–µ—Ä: CycleGAN, DeepArt.

#### 4.3. **–°—É–ø–µ—Ä—Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ (Super-Resolution)**
- –£–ª—É—á—à–µ–Ω–∏–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
- –ü—Ä–∏–º–µ—Ä: SRGAN.

#### 4.4. **–£–¥–∞–ª–µ–Ω–∏–µ —à—É–º–∞ (Denoising)**
- –£–¥–∞–ª–µ–Ω–∏–µ —à—É–º–∞ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
- –ü—Ä–∏–º–µ—Ä: Variational Autoencoders (VAE).

#### 4.5. **–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö (Inpainting)**
- –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö —á–∞—Å—Ç–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
- –ü—Ä–∏–º–µ—Ä: Contextual Attention GAN.

#### 4.6. **–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π**
- –í—ã—è–≤–ª–µ–Ω–∏–µ –Ω–µ–æ–±—ã—á–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, –Ω–µ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—â–∏—Ö –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ.
- –ü—Ä–∏–º–µ—Ä: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö.

#### 4.7. **–°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–¥–µ–æ**
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –≤–∏–¥–µ–æ.
- –ü—Ä–∏–º–µ—Ä: VideoGAN.

#### 4.8. **–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è**
- –°–æ–∑–¥–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞.
- –ü—Ä–∏–º–µ—Ä: DALL-E, CLIP-GAN.

---

### 6. **–í—ã–≤–æ–¥—ã**

1. **–î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏**:
   - –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—Ç –º–µ—Ç–∫–∏, –Ω–æ –Ω–µ –º–æ–≥—É—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ.
2. **–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏**:
   - –ú–æ–¥–µ–ª–∏—Ä—É—é—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏ –∞–Ω–æ–º–∞–ª–∏–π.
3. **–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ**:
   - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å—É–ø–µ—Ä—Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ, –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π, —Å—Ç–∏–ª–∏–∑–∞—Ü–∏—è –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è.
4. **–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è**:
   - GAN, VAE –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É—é—â–∏–µ –ø–æ—Ç–æ–∫–∏ –∞–∫—Ç–∏–≤–Ω–æ —Ä–∞–∑–≤–∏–≤–∞—é—Ç—Å—è –∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –≤ —à–∏—Ä–æ–∫–æ–º —Å–ø–µ–∫—Ç—Ä–µ –∑–∞–¥–∞—á.
"""
    else:
        code = """
#### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Å—Ç–æ–≥–æ GAN –≤ PyTorch:
import torch
import torch.nn as nn
import torch.optim as optim

# –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä
class Generator(nn.Module):
    def __init__(self, noise_dim, img_dim):
        super(Generator, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(noise_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, img_dim),
            nn.Tanh()
        )

    def forward(self, x):
        return self.net(x)

# –î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä
class Discriminator(nn.Module):
    def __init__(self, img_dim):
        super(Discriminator, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(img_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.net(x)

# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
noise_dim = 100
img_dim = 28 * 28  # –î–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π 28x28 (–Ω–∞–ø—Ä–∏–º–µ—Ä, MNIST)
lr = 0.0002

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
generator = Generator(noise_dim, img_dim)
discriminator = Discriminator(img_dim)

# –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã
optimizer_G = optim.Adam(generator.parameters(), lr=lr)
optimizer_D = optim.Adam(discriminator.parameters(), lr=lr)

# –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
criterion = nn.BCELoss()

# –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ GAN
for epoch in range(10000):
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ —à—É–º–∞
    noise = torch.randn(64, noise_dim)
    fake_images = generator(noise)
    real_labels = torch.ones(64, 1)
    fake_labels = torch.zeros(64, 1)

    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–∞
    real_images = torch.randn(64, img_dim)  # –ü—Ä–∏–º–µ—Ä —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    outputs_real = discriminator(real_images)
    outputs_fake = discriminator(fake_images.detach())
    loss_D = criterion(outputs_real, real_labels) + criterion(outputs_fake, fake_labels)
    optimizer_D.zero_grad()
    loss_D.backward()
    optimizer_D.step()

    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
    outputs_fake = discriminator(fake_images)
    loss_G = criterion(outputs_fake, real_labels)
    optimizer_G.zero_grad()
    loss_G.backward()
    optimizer_G.step()

    if (epoch + 1) % 1000 == 0:
        print(f"Epoch {epoch+1}, Loss_D: {loss_D.item():.4f}, Loss_G: {loss_G.item():.4f}")
"""

    pyperclip.copy(code)


def gan(idx: int = 0):
    if idx == 0:
        code = """
### 32. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ GAN: –æ–±—â–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –æ–±—É—á–µ–Ω–∏–µ –∏ –ø—Ä–∏–º–µ—Ä —Å –æ–¥–Ω–æ–º–µ—Ä–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è

---

### 1. **–û–±—â–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ GAN**

**Generative Adversarial Networks (GAN)** ‚Äî —ç—Ç–æ –∫–ª–∞—Å—Å –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω–æ–º –ø–æ–¥—Ö–æ–¥–µ –º–µ–∂–¥—É –¥–≤—É–º—è –Ω–µ–π—Ä–æ–Ω–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏:
- **–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä (Generator, \(G\))**:
  - –°–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ \(x\), –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.
  - –ü—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ —Å–ª—É—á–∞–π–Ω—ã–π —à—É–º \(z\) –∏–∑ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞.
- **–î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä (Discriminator, \(D\))**:
  - –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –≤—Ö–æ–¥–Ω–æ–π —Å–∏–≥–Ω–∞–ª —Ä–µ–∞–ª—å–Ω—ã–º (–∏–∑ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞) –∏–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º.

#### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
1. **–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä**:
   - –í—Ö–æ–¥: —Å–ª—É—á–∞–π–Ω—ã–π —à—É–º \(z \sim p_z(z)\).
   - –í—ã—Ö–æ–¥: —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ \(\hat{x} = G(z)\).

2. **–î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä**:
   - –í—Ö–æ–¥: –ª–∏–±–æ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ \(x\), –ª–∏–±–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ \(\hat{x}\).
   - –í—ã—Ö–æ–¥: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ç–æ–≥–æ, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —Ä–µ–∞–ª—å–Ω—ã \(D(x) \in [0, 1]\).

---

### 2. **–û–±—É—á–µ–Ω–∏–µ GAN**

–û–±—É—á–µ–Ω–∏–µ GAN ‚Äî —ç—Ç–æ –∏–≥—Ä–∞ —Å –Ω—É–ª–µ–≤–æ–π —Å—É–º–º–æ–π, –≥–¥–µ:
1. –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø—ã—Ç–∞–µ—Ç—Å—è –æ–±–º–∞–Ω—É—Ç—å –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä, —Å–æ–∑–¥–∞–≤–∞—è –¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –æ–Ω —Å—á–∏—Ç–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–º–∏.
2. –î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä –æ–±—É—á–∞–µ—Ç—Å—è —Ä–∞–∑–ª–∏—á–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.

#### –¶–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è:
–û–±—É—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –º–∏–Ω–∏–º–∞–∫—Å–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏:
\[
\min_G \max_D \, \mathbb{E}_{x \sim p_{\text{data}}(x)} [\log D(x)] + \mathbb{E}_{z \sim p_z(z)} [\log (1 - D(G(z)))]
\]
–≥–¥–µ:
- \(p_{\text{data}}(x)\) ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
- \(p_z(z)\) ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —à—É–º–∞.

#### –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –Ω–∞ —ç—Ç–∞–ø—ã:
1. **–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–∞**:
   - –ú–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:
     \[
     \mathcal{L}_D = - \left( \mathbb{E}_{x \sim p_{\text{data}}(x)} [\log D(x)] + \mathbb{E}_{z \sim p_z(z)} [\log (1 - D(G(z)))] \right)
     \]

2. **–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞**:
   - –ú–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ç–æ–≥–æ, —á—Ç–æ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ "—Ñ–µ–π–∫":
     \[
     \mathcal{L}_G = - \mathbb{E}_{z \sim p_z(z)} [\log D(G(z))]
     \]

#### –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å:
- –ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ (–æ–±—ã—á–Ω–æ 1-2), —á—Ç–æ–±—ã –ª—É—á—à–µ —Ä–∞–∑–ª–∏—á–∞—Ç—å –¥–∞–Ω–Ω—ã–µ.
- –ó–∞—Ç–µ–º –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä.

---

### 3. **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –≥–ª—É–±–æ–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –≤ GAN**

1. **–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ GAN**:
   - –ò—Å–ø–æ–ª—å–∑—É—é—Ç –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –∏ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–∞.

2. **Deep Convolutional GAN (DCGAN)**:
   - –ü—Ä–∏–º–µ–Ω—è—é—Ç —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–ª–æ–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
   - –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–≤—ë—Ä—Ç–∫–∏ (\(ConvTranspose\)).
   - –î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Å–≤—ë—Ä—Ç–∫–∏ (\(Conv\)).

3. **Conditional GAN (cGAN)**:
   - –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä —É—Å–ª–æ–≤–Ω–æ –∑–∞–≤–∏—Å—è—Ç –æ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö \(y\) (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–ª–∞—Å—Å–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è):
     \[
     G(z, y), \quad D(x, y)
     \]

4. **Wasserstein GAN (WGAN)**:
   - –í–≤–æ–¥—è—Ç –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –í–∞—Å–µ—Ä—à—Ç–µ–π–Ω–∞, —á—Ç–æ–±—ã —É–ª—É—á—à–∏—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è.

5. **StyleGAN**:
   - –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ä–∞–∑–¥–µ–ª—è–µ—Ç —Å—Ç–∏–ª—å –∏ –∫–æ–Ω—Ç–µ–Ω—Ç, –ø–æ–∑–≤–æ–ª—è—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

---

### 4. **–ü—Ä–∏–Ω—Ü–∏–ø –æ–±—É—á–µ–Ω–∏—è GAN –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ –æ–¥–Ω–æ–º–µ—Ä–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è**

#### –ó–∞–¥–∞—á–∞:
- –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä–æ–µ –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç—Å—è –∫ —Ä–µ–∞–ª—å–Ω–æ–º—É –æ–¥–Ω–æ–º–µ—Ä–Ω–æ–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, \(p_{\text{data}}(x) \sim \mathcal{N}(0, 1)\)).

#### –®–∞–≥–∏:

1. **–†–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ**:
   - \(p_{\text{data}}(x)\): –ò—Å—Ç–∏–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ).

2. **–®—É–º**:
   - \(z \sim \mathcal{U}(-1, 1)\): –°–ª—É—á–∞–π–Ω—ã–π —à—É–º –∏–∑ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è.

3. **–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä**:
   - –°–æ–∑–¥–∞—ë—Ç —Å—ç–º–ø–ª—ã \(G(z)\), –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ª–∂–Ω—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å \(p_{\text{data}}(x)\).

4. **–î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä**:
   - –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–æ—á–∫–∞ —Ä–µ–∞–ª—å–Ω–æ–π (–∏–∑ \(p_{\text{data}}\)) –∏–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π (\(G(z)\)).

---

### 5. **–í—ã–≤–æ–¥—ã**

1. **GAN**:
   - –°–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –≤ –∫–æ—Ç–æ—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–æ–∑–¥–∞—ë—Ç –¥–∞–Ω–Ω—ã–µ, –∞ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä —Ä–∞–∑–ª–∏—á–∞–µ—Ç –∏—Ö –æ—Ç —Ä–µ–∞–ª—å–Ω—ã—Ö.
2. **–û–±—É—á–µ–Ω–∏–µ**:
   - –ú–∏–Ω–∏–º–∞–∫—Å–Ω–∞—è –∑–∞–¥–∞—á–∞, –≥–¥–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å—Ç—Ä–µ–º–∏—Ç—Å—è –æ–±–º–∞–Ω—É—Ç—å –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä.
3. **–ü—Ä–∏–º–µ—Ä**:
   - GAN —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç –æ–¥–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.
4. **–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ**:
   - GAN –Ω–∞—Ö–æ–¥—è—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≤–∏–¥–µ–æ, –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á–∞—Ö.
"""
    else:
        code = """
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

# –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(1, 16),
            nn.ReLU(),
            nn.Linear(16, 1)
        )

    def forward(self, z):
        return self.net(z)

# –î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(1, 16),
            nn.ReLU(),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.net(x)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
generator = Generator()
discriminator = Discriminator()

# –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã
optimizer_G = optim.Adam(generator.parameters(), lr=0.001)
optimizer_D = optim.Adam(discriminator.parameters(), lr=0.001)

# –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
criterion = nn.BCELoss()

# –û–±—É—á–µ–Ω–∏–µ GAN
num_epochs = 1000
real_data = lambda n: torch.randn(n, 1)  # –ò—Å—Ç–∏–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ N(0, 1)
batch_size = 64

for epoch in range(num_epochs):
    # 1. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–∞
    z = torch.rand(batch_size, 1) * 2 - 1  # –®—É–º –∏–∑ U(-1, 1)
    fake_data = generator(z)
    real_samples = real_data(batch_size)

    real_labels = torch.ones(batch_size, 1)
    fake_labels = torch.zeros(batch_size, 1)

    loss_D = criterion(discriminator(real_samples), real_labels) + \
             criterion(discriminator(fake_data.detach()), fake_labels)
    optimizer_D.zero_grad()
    loss_D.backward()
    optimizer_D.step()

    # 2. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
    loss_G = criterion(discriminator(fake_data), real_labels)
    optimizer_G.zero_grad()
    loss_G.backward()
    optimizer_G.step()

    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    if (epoch + 1) % 100 == 0:
        print(f"Epoch {epoch+1}, Loss_D: {loss_D.item():.4f}, Loss_G: {loss_G.item():.4f}")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
z = torch.rand(1000, 1) * 2 - 1
generated_data = generator(z).detach().numpy()

plt.hist(generated_data, bins=50, alpha=0.5, label='Generated')
plt.hist(real_data(1000).numpy(), bins=50, alpha=0.5, label='Real')
plt.legend()
plt.title("Distribution Comparison")
plt.show()
"""
    pyperclip.copy(code)


def vae(idx: int = 0):
    if idx == 0:
        code = """
### 33. –í–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä (VAE): —Ü–µ–ª–∏, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –±–∞–π–µ—Å–æ–≤—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å

---

### 1. **–û–±—â–∏–µ –∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Ü–µ–ª–∏ –º–æ–¥–µ–ª–∏ VAE**

#### –û–±—â–∏–µ —Ü–µ–ª–∏:
- –í–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä (VAE) ‚Äî —ç—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, —Å–ø–æ—Å–æ–±–Ω–∞—è:
  1. –°–∂–∏–º–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ (–ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ).
  2. –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.

#### –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Ü–µ–ª–∏:
1. **–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ**:
   - –õ–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏–º–µ–µ—Ç –æ—Å–º—ã—Å–ª–µ–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä–æ–≤–∞—Ç—å, —ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä–æ–≤–∞—Ç—å –∏ —É–ø—Ä–∞–≤–ª—è—Ç—å —Å–≤–æ–π—Å—Ç–≤–∞–º–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
2. **–ë–∞–π–µ—Å–æ–≤—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ**:
   - VAE –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä—É–µ—Ç –∏—Å—Ç–∏–Ω–Ω–æ–µ –∞–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö \(P(z|x)\) —á–µ—Ä–µ–∑ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ \(q(z|x)\).
3. **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è**:
   - VAE —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Ç–∞–∫, —á—Ç–æ–±—ã –æ–Ω–æ —Å–ª–µ–¥–æ–≤–∞–ª–æ –∑–∞—Ä–∞–Ω–µ–µ –∑–∞–¥–∞–Ω–Ω–æ–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é (–æ–±—ã—á–Ω–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º—É \(\mathcal{N}(0, 1)\)).

---

### 2. **–û–±—â–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ VAE**

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ VAE —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö —á–∞—Å—Ç–µ–π, —Å—Ö–æ–∂–∏—Ö —Å –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–º –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–º:

1. **Encoder**:
   - –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ \(x\) –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ \(z\), —Ç–∞–∫–∏–µ –∫–∞–∫ —Å—Ä–µ–¥–Ω–µ–µ \(\mu(x)\) –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ \(\sigma(x)\).
   - –í—ã—Ö–æ–¥: –ø–∞—Ä–∞–º–µ—Ç—Ä—ã \(\mu, \sigma\), –∫–æ—Ç–æ—Ä—ã–µ –æ–ø–∏—Å—ã–≤–∞—é—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ \(q(z|x)\).

2. **Decoder**:
   - –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Å—ç–º–ø–ª \(z\), –ø–æ–ª—É—á–µ–Ω–Ω—ã–π –∏–∑ \(q(z|x)\), –∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ \(\hat{x}\).
   - –í—ã—Ö–æ–¥: —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö \(\hat{x} \sim P(x|z)\).

---

### 3. **–ë–∞–π–µ—Å–æ–≤—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ VAE**

#### –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è:
VAE –æ—Å–Ω–æ–≤—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –±–∞–π–µ—Å–æ–≤—Å–∫–æ–º –ø–æ–¥—Ö–æ–¥–µ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö. –¶–µ–ª—å ‚Äî –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ –¥–∞–Ω–Ω—ã—Ö:
\[
\log P(x) = \int P(x|z) P(z) dz
\]
–≥–¥–µ:
- \(P(z)\) ‚Äî –∞–ø—Ä–∏–æ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö (–æ–±—ã—á–Ω–æ \(\mathcal{N}(0, 1)\)).
- \(P(x|z)\) ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –Ω–∞–±–ª—é–¥–∞—Ç—å \(x\) –ø—Ä–∏ –¥–∞–Ω–Ω–æ–º \(z\).

#### –ü—Ä–æ–±–ª–µ–º–∞:
–ò–Ω—Ç–µ–≥—Ä–∞–ª –≤—ã—á–∏—Å–ª—è—Ç—å –Ω–∞–ø—Ä—è–º—É—é —Å–ª–æ–∂–Ω–æ –∏–∑-–∑–∞ –≤—ã—Å–æ–∫–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏.

#### –†–µ—à–µ–Ω–∏–µ:
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è **–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–∞—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è**:
- –ó–∞–º–µ–Ω—è–µ—Ç—Å—è –∏—Å—Ç–∏–Ω–Ω–æ–µ –∞–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ \(P(z|x)\) –ø—Ä–∏–±–ª–∏–∂—ë–Ω–Ω—ã–º \(q(z|x)\), –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–º –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é.
- –≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ **–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω–æ–º—É –Ω–∏–∂–Ω–µ–º—É –ø—Ä–µ–¥–µ–ª—É (ELBO)**:
  \[
  \log P(x) \geq \mathbb{E}_{z \sim q(z|x)} [\log P(x|z)] - D_{\text{KL}}(q(z|x) \| P(z))
  \]
  –≥–¥–µ:
  - –ü–µ—Ä–≤—ã–π —á–ª–µ–Ω ‚Äî –ª–æ–≥–∞—Ä–∏—Ñ–º –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ (–æ—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è).
  - –í—Ç–æ—Ä–æ–π —á–ª–µ–Ω ‚Äî –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è –ö—É–ª—å–±–∞–∫–∞-–õ–µ–π–±–ª–µ—Ä–∞ –º–µ–∂–¥—É \(q(z|x)\) –∏ \(P(z)\) (—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞).

---

### 4. **–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å VAE**

–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å VAE –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ ELBO:

1. **–û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è (\(\log P(x|z)\))**:
   - –û—Ü–µ–Ω–∏–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –¥–µ–∫–æ–¥–µ—Ä –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ.
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω–∞–ø—Ä–∏–º–µ—Ä, MSE –∏–ª–∏ –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏—è.

2. **–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (\(D_{\text{KL}}\))**:
   - –ó–∞—Å—Ç–∞–≤–ª—è–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ \(q(z|x)\) –±—ã—Ç—å –±–ª–∏–∑–∫–∏–º –∫ \(P(z)\) (–æ–±—ã—á–Ω–æ \(\mathcal{N}(0, 1)\)).
   - –§–æ—Ä–º—É–ª–∞ KL-–¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π:
     \[
     D_{\text{KL}}(\mathcal{N}(\mu, \sigma^2) \| \mathcal{N}(0, 1)) = -\frac{1}{2} \sum (1 + \log \sigma^2 - \mu^2 - \sigma^2)
     \]

---

### 5. **–°–ø–µ—Ü–∏—Ñ–∏–∫–∞ –ø–æ–ª—É—á–∞–µ–º—ã—Ö —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π**

#### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
1. **–°–º—ã—Å–ª–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞**:
   - –õ–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏–º–µ–µ—Ç —Ä–µ–≥—É–ª—è—Ä–Ω—É—é –∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—é –º–µ–∂–¥—É —Ç–æ—á–∫–∞–º–∏.

2. **–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ**:
   - –ö–∞–∂–¥–æ–µ –∏–∑–º–µ—Ä–µ–Ω–∏–µ –≤ \(z\) –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—É—é —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫—É –¥–∞–Ω–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, —è—Ä–∫–æ—Å—Ç—å, —É–≥–æ–ª –ø–æ–≤–æ—Ä–æ—Ç–∞).

3. **–û–±–æ–±—â–µ–Ω–∏–µ**:
   - –ë–ª–∞–≥–æ–¥–∞—Ä—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏, VAE –Ω–µ –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –º–æ–∂–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –Ω–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã.

---

### 6. **–ü—Ä–∏–º–µ—Ä –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏ –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ**

#### –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è:
–ü—É—Å—Ç—å \(z_1, z_2 \in \mathbb{R}^k\) ‚Äî –¥–≤–∞ –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–∞. –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –º–µ–∂–¥—É –Ω–∏–º–∏:
\[
z_\text{interp} = \alpha z_1 + (1 - \alpha) z_2, \quad \alpha \in [0, 1]
\]

#### –≠–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è:
–ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ, –Ω–æ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –≤—ã—Ö–æ–¥—è—Ç –∑–∞ –ø—Ä–µ–¥–µ–ª—ã \([0, 1]\).

---

### 8. **–í—ã–≤–æ–¥—ã**

1. **–¶–µ–ª–∏ VAE**:
   - –û–±—É—á–µ–Ω–∏–µ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–≥–æ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è.
   - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
2. **–ë–∞–π–µ—Å–æ–≤—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ**:
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.
3. **–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å**:
   - –°–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞.
4. **–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ**:
   - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ–±—Ä–∞–±–æ—Ç–∫–∞ —à—É–º–∞, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞.
"""
    else:
        code = """
### 7. **–†–µ–∞–ª–∏–∑–∞—Ü–∏—è VAE –≤ PyTorch**
import torch
import torch.nn as nn
import torch.optim as optim

# Encoder
class Encoder(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(Encoder, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc_mu = nn.Linear(128, latent_dim)
        self.fc_logvar = nn.Linear(128, latent_dim)

    def forward(self, x):
        h = torch.relu(self.fc1(x))
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar

# Decoder
class Decoder(nn.Module):
    def __init__(self, latent_dim, output_dim):
        super(Decoder, self).__init__()
        self.fc1 = nn.Linear(latent_dim, 128)
        self.fc_out = nn.Linear(128, output_dim)

    def forward(self, z):
        h = torch.relu(self.fc1(z))
        return torch.sigmoid(self.fc_out(h))

# VAE
class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super(VAE, self).__init__()
        self.encoder = Encoder(input_dim, latent_dim)
        self.decoder = Decoder(latent_dim, input_dim)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        mu, logvar = self.encoder(x)
        z = self.reparameterize(mu, logvar)
        return self.decoder(z), mu, logvar

# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
input_dim = 784  # –ù–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è MNIST
latent_dim = 2
vae = VAE(input_dim, latent_dim)
optimizer = optim.Adam(vae.parameters(), lr=0.001)
criterion = nn.MSELoss()

# –û–±—É—á–µ–Ω–∏–µ
for epoch in range(10):
    for x in train_loader:  # –î–∞–Ω–Ω—ã–µ –∏–∑ DataLoader
        x = x.view(-1, input_dim)
        optimizer.zero_grad()
        x_reconstructed, mu, logvar = vae(x)
        reconstruction_loss = criterion(x_reconstructed, x)
        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
        loss = reconstruction_loss + kl_loss
        loss.backward()
        optimizer.step()
"""
    pyperclip.copy(code)


def diffusion(idx: int = 0):
    if idx == 0:
        code = """
### 35. Denoising Diffusion Models: –ø—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã, –ø—Ä—è–º–æ–π –∏ –æ–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å—ã

---

### 1. **–û–±—â–∏–π –ø—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã Denoising Diffusion Models**

Denoising Diffusion Models (DDMs) ‚Äî —ç—Ç–æ –∫–ª–∞—Å—Å –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–∑–¥–∞—é—Ç –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—É—Ç–µ–º –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ "–æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞" –∏–∑ —à—É–º–∞, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–∏–Ω—Ü–∏–ø—ã —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏.

#### –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è:
1. **–ü—Ä—è–º–æ–π –ø—Ä–æ—Ü–µ—Å—Å**:
   - –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –ø–æ–¥–≤–µ—Ä–≥–∞—é—Ç—Å—è —à—É–º—É, –ø–æ–∫–∞ –æ–Ω–∏ –Ω–µ —Å—Ç–∞–Ω—É—Ç –ø–æ—Ö–æ–∂–∏–º–∏ –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã–π —à—É–º.
2. **–û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å**:
   - –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è —à—É–º–∞, –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—è –¥–∞–Ω–Ω—ã–µ –æ—Ç –ø–æ–ª–Ω–æ–≥–æ —à—É–º–∞ –∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º.

#### –ó–∞–¥–∞—á–∏:
- –í—ã—É—á–∏—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö \(P(x)\) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–≤—É—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤:
  1. –ü—Ä—è–º–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞: \(q(x_t|x_{t-1})\).
  2. –û–±—Ä–∞—Ç–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞: \(p_\theta(x_{t-1}|x_t)\).

---

### 2. **–ü—Ä—è–º–æ–π –ø—Ä–æ—Ü–µ—Å—Å**

#### –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:
–ü—Ä—è–º–æ–π –ø—Ä–æ—Ü–µ—Å—Å (\(q(x_t|x_{t-1})\)) –¥–æ–±–∞–≤–ª—è–µ—Ç –≥–∞—É—Å—Å–æ–≤—Å–∫–∏–π —à—É–º –∫ –¥–∞–Ω–Ω—ã–º \(x\) –∑–∞ \(T\) —à–∞–≥–æ–≤. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –¥–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –ø—Ä–µ–≤—Ä–∞—â–∞—é—Ç—Å—è –≤ —à—É–º.

1. **–ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞**:
   - –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –∫–∞–∫:
     \[
     q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I)
     \]
     –≥–¥–µ:
     - \(\beta_t \in (0, 1)\) ‚Äî –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —É—Ä–æ–≤–Ω—è —à—É–º–∞ –¥–ª—è —à–∞–≥–∞ \(t\).
     - \(\mathcal{N}\) ‚Äî –≥–∞—É—Å—Å–æ–≤—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ.

2. **–°–≤–µ–¥–µ–Ω–∏–µ –∫ —à—É–º—É**:
   - –ü–æ—Å–ª–µ \(T\) —à–∞–≥–æ–≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ \(x_T\) –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç—Å—è –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é:
     \[
     q(x_T|x_0) \approx \mathcal{N}(0, I)
     \]

3. **–ü—Ä—è–º–æ–π –ø—Ä–æ—Ü–µ—Å—Å –∑–∞ –æ–¥–∏–Ω —à–∞–≥**:
   - –£—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–ª—è –ª—é–±–æ–≥–æ \(t\):
     \[
     q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)
     \]
     –≥–¥–µ \(\bar{\alpha}_t = \prod_{s=1}^t (1 - \beta_s)\).

---

### 3. **–û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å**

#### –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:
–û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å (\(p_\theta(x_{t-1}|x_t)\)) —É—á–∏—Ç –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–±–∏—Ä–∞—Ç—å —à—É–º, –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏–≤ –¥–∞–Ω–Ω—ã–µ \(x_0\).

1. **–ú–æ–¥–µ–ª—å –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞**:
   - –ê–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä—É–µ—Ç—Å—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç—å—é:
     \[
     p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))
     \]
   - –ì–¥–µ \(\mu_\theta\) –∏ \(\Sigma_\theta\) ‚Äî –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º—ã–µ –º–æ–¥–µ–ª—å—é.

2. **–¶–µ–ª—å –æ–±—É—á–µ–Ω–∏—è**:
   - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã \(\theta\), —á—Ç–æ–±—ã –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ \(x_0\) —á–µ—Ä–µ–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —à–∞–≥–æ–≤:
     \[
     p_\theta(x_0) = \int \prod_{t=1}^T p_\theta(x_{t-1}|x_t) dx_t
     \]

3. **–£–ø—Ä–æ—â–µ–Ω–∏—è**:
   - –û–±—ã—á–Ω–æ \(\Sigma_\theta\) —Ñ–∏–∫—Å–∏—Ä—É–µ—Ç—Å—è, –∏ –∑–∞–¥–∞—á–∞ —Å–≤–æ–¥–∏—Ç—Å—è –∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—é \(\mu_\theta\).

---

### 4. **–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å**

#### –û—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞:
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è –¥–∞–Ω–Ω—ã—Ö:
  \[
  \mathcal{L}_{\text{VLB}} = \mathbb{E}_q \left[ \sum_{t=1}^T D_{\text{KL}}(q(x_{t-1}|x_t, x_0) \| p_\theta(x_{t-1}|x_t)) \right]
  \]

#### –£–ø—Ä–æ—â–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å:
- –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏—è –æ–±—É—á–∞—é—Ç —Å–µ—Ç—å \(\epsilon_\theta(x_t, t)\), –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–π —à—É–º:
  \[
  \mathcal{L} = \mathbb{E}_{t, x_0, \epsilon} \left[ \| \epsilon - \epsilon_\theta(x_t, t) \|^2 \right]
  \]
  –≥–¥–µ:
  - \(\epsilon\) ‚Äî —à—É–º, –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–π –Ω–∞ —à–∞–≥–µ \(t\).
  - \(\epsilon_\theta(x_t, t)\) ‚Äî —à—É–º, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –º–æ–¥–µ–ª—å—é.

---

### 5. **–ò–Ω—Ç—É–∏—Ü–∏—è —Ä–∞–±–æ—Ç—ã Denoising Diffusion Models**

#### –ü—Ä—è–º–æ–π –ø—Ä–æ—Ü–µ—Å—Å:
1. –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ \(x_0\) –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –±–æ–ª–µ–µ —à—É–º–Ω—ã–º.
2. –®–∞–≥–∏ \(x_1, x_2, \ldots, x_T\) –æ–ø–∏—Å—ã–≤–∞—é—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è.

#### –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å:
1. –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —É—á–∏—Ç —É–±–∏—Ä–∞—Ç—å —à—É–º —Å \(x_T\), —à–∞–≥ –∑–∞ —à–∞–≥–æ–º –ø—Ä–∏–±–ª–∏–∂–∞—è—Å—å –∫ \(x_0\).
2. –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ \(T\) —à–∞–≥–æ–≤.


---

### 7. **–í—ã–≤–æ–¥—ã**

1. **–ü—Ä—è–º–æ–π –ø—Ä–æ—Ü–µ—Å—Å**:
   - –î–æ–±–∞–≤–ª—è–µ—Ç —à—É–º –∫ –¥–∞–Ω–Ω—ã–º, –ø—Ä–∏–±–ª–∏–∂–∞—è –∏—Ö –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é.
2. **–û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å**:
   - –£—á–∏—Ç –ø–æ—à–∞–≥–æ–≤–æ —É–±–∏—Ä–∞—Ç—å —à—É–º, —á—Ç–æ–±—ã –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ.
3. **–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å**:
   - –û—Å–Ω–æ–≤—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ —à—É–º–∞, –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ.
4. **–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ**:
   - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Ç–µ–∫—Å—Ç—É—Ä, —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —É–¥–∞–ª–µ–Ω–∏–µ —à—É–º–∞.
"""
    else:
        code = """
### 6. **–ü—Ä–∏–º–µ—Ä –∫–æ–¥–∞ Denoising Diffusion Model –≤ PyTorch**
import torch
import torch.nn as nn
import torch.optim as optim

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —à—É–º–∞
T = 1000  # –ß–∏—Å–ª–æ —à–∞–≥–æ–≤
betas = torch.linspace(1e-4, 0.02, T)  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã —à—É–º–∞
alphas = 1.0 - betas
alpha_bar = torch.cumprod(alphas, dim=0)  # –ö—É–º—É–ª—è—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è

# –ü—Ä—è–º–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è
def forward_diffusion(x0, t):
    noise = torch.randn_like(x0)
    mean = torch.sqrt(alpha_bar[t]) * x0
    std = torch.sqrt(1 - alpha_bar[t])
    xt = mean + std * noise
    return xt, noise

# –ú–æ–¥–µ–ª—å
class SimpleDiffusionModel(nn.Module):
    def __init__(self, input_dim):
        super(SimpleDiffusionModel, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, input_dim)
        )

    def forward(self, x, t):
        return self.net(x)

# –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
def loss_fn(model, x0, t):
    xt, noise = forward_diffusion(x0, t)
    noise_pred = model(xt, t)
    return nn.MSELoss()(noise_pred, noise)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
model = SimpleDiffusionModel(input_dim=784)
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# –û–±—É—á–µ–Ω–∏–µ
for epoch in range(10):
    x0 = torch.randn(64, 784)  # –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
    t = torch.randint(0, T, (64,))  # –°–ª—É—á–∞–π–Ω—ã–µ —à–∞–≥–∏
    loss = loss_fn(model, x0, t)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch + 1}, Loss: {loss.item():.4f}")
"""
    pyperclip.copy(code)


def mlp_reg(idx: int = 0):
    if idx == 0:
        code = """
import pandas as pd
import numpy as np
import torch
from torch import nn
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
"""
    elif idx == 1:
        code = """
# 1. –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω–Ω—ã—Ö
data = pd.read_csv('international-airline-passengers.csv')
data.columns = ['Month', 'Passengers']

# –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–∞—Ç—ã
data['Month'] = pd.to_datetime(data['Month'])
data['Month_num'] = data['Month'].dt.year * 12 + data['Month'].dt.month

# –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è(–ú–∏–Ω–ú–∞–∫—Å)
scaler = MinMaxScaler()
data['Passengers'] = scaler.fit_transform(data[['Passengers']])
data['Month_num'] = scaler.fit_transform(data[['Month_num']])
"""
    elif idx == 2:
        code = """
# Create input-output pairs (using the previous month's passengers as input)
def create_sequences(data, seq_length=1):
    x, y = [], []
    for i in range(len(data) - seq_length):
        x.append(data[i:i+seq_length, 0])  # Passengers as input
        y.append(data[i+seq_length, 0])    # Next month's passengers as output
    return np.array(x), np.array(y)

seq_length = 1
x_data, y_data = create_sequences(data[['Passengers']].values, seq_length=seq_length)

# —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö 
x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

x_train = torch.tensor(x_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)
x_test = torch.tensor(x_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)
"""
    elif idx == 3:
        code = """
# 2. –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å
class RegressionModel(nn.Module):
    def __init__(self):
        super(RegressionModel, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(seq_length, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, x):
        return self.network(x)

# 3. —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
def train_model(optimizer_name, model, criterion, optimizer, epochs=100):
    train_losses = []
    val_losses = []

    for epoch in range(epochs):

        model.train()
        optimizer.zero_grad()
        y_pred = model(x_train)
        train_loss = criterion(y_pred, y_train)
        train_loss.backward()
        optimizer.step()

        model.eval()
        with torch.no_grad():
            y_val_pred = model(x_test)
            val_loss = criterion(y_val_pred, y_test)

        train_losses.append(train_loss.item())
        val_losses.append(val_loss.item())

    return train_losses, val_losses
"""
    elif idx == 4:
        code = """
# –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
optimizers = {
    'SGD': torch.optim.SGD,
    'Adam': torch.optim.Adam,
    'AdamW': torch.optim.AdamW
}

results = {}
for opt_name, opt_class in optimizers.items():
    model = RegressionModel()
    criterion = nn.MSELoss()
    optimizer = opt_class(model.parameters(), lr=0.01)

    train_losses, val_losses = train_model(opt_name, model, criterion, optimizer)


    model.eval()
    with torch.no_grad():
        y_test_pred = model(x_test)
        mse = mean_squared_error(y_test.numpy(), y_test_pred.numpy())
        mae = mean_absolute_error(y_test.numpy(), y_test_pred.numpy())
        r2 = r2_score(y_test.numpy(), y_test_pred.numpy())

    results[opt_name] = {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'mse': mse,
        'mae': mae,
        'r2': r2
    }


    plt.plot(train_losses, label=f'{opt_name} - Train')
    plt.plot(val_losses, label=f'{opt_name} - Val')

plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss Curves for Different Optimizers')
plt.legend()
plt.show()


for opt_name, metrics in results.items():
    print(f"Optimizer: {opt_name}")
    print(f"MSE: {metrics['mse']:.4f}, MAE: {metrics['mae']:.4f}, R2: {metrics['r2']:.4f}\n")
"""
    pyperclip.copy(code)


def mlp_reg_2(idx: int = 0):
    if idx == 0:
        code = """
import pandas as pd
import numpy as np

import torch
from torch import nn, optim
from torch.utils.data import DataLoader, TensorDataset

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

import matplotlib.pyplot as plt

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –ø—É—Ç—å –∫ –≤–∞—à–∏–º –¥–∞–Ω–Ω—ã–º)
data = pd.read_csv('data.csv')

# –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –∏ –ø—Ä–∏–∑–Ω–∞–∫–∏
X = data.drop(columns=['Gold_T+22'])
y = data['Gold_T+22']

# –í—ã–¥–µ–ª–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
numerical_cols = X.columns.tolist()

# –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
scaler = StandardScaler()
X_processed = scaler.fit_transform(X)

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)

# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–µ–Ω–∑–æ—Ä—ã
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏ DataLoader
train_dataset = TensorDataset(X_train, y_train)
test_dataset = TensorDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

define_model = lambda input_dim: nn.Sequential(
    nn.Linear(input_dim, 64),
    nn.ReLU(),
    nn.Linear(64, 64),
    nn.ReLU(),
    nn.Linear(64, 1)
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏, —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
input_dim = X_train.shape[1]
model = define_model(input_dim)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
num_epochs = 100
print_every = 10
train_losses = []
test_losses = []

for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        predictions = model(batch_X)
        loss = criterion(predictions, batch_y)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()

    train_losses.append(epoch_loss / len(train_loader))

    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
    model.eval()
    with torch.no_grad():
        test_predictions = model(X_test)
        test_loss = criterion(test_predictions, y_test).item()
        test_losses.append(test_loss)

    if (epoch + 1) % print_every == 0:
        print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_loss:.4f}")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è loss
plt.figure(figsize=(10, 6))
plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')
plt.plot(range(1, num_epochs + 1), test_losses, label='Test Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Training and Test Loss')
plt.show()       
"""
    pyperclip.copy(code)


def mlp_reg_3(idx: int = 0):
    if idx == 0:
        code = """
import kagglehub

# Download latest version
path = kagglehub.dataset_download("balakrishcodes/others")

print("Path to dataset files:", path)

import pandas as pd
import numpy as np
import torch
from torch import nn
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
"""
    elif idx == 1:
        code = """
# –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –Ω–µ —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
data = pd.read_csv('Movie_regression.csv')


categorical_columns = data.select_dtypes(include=['object']).columns

# One-hot encode 
if len(categorical_columns) > 0:
    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')
    encoded = encoder.fit_transform(data[categorical_columns])
    encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(categorical_columns))
    data = pd.concat([data.drop(columns=categorical_columns), encoded_df], axis=1)
"""
    elif idx == 2:
        code = """
print("–ï—Å—Ç—å –ª–∏ NaN –∑–Ω–∞—á–µ–Ω–∏—è?")
print(data.isnull().sum())  

print("–ï—Å—Ç—å –ª–∏ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è?")
print(np.isinf(data).any())

print("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –¥–∞–Ω–Ω—ã—Ö:", data.max(numeric_only=True))

#–∑–∞–º–µ–Ω–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ä–µ–¥–Ω–µ–µ

data.fillna(data.mean(), inplace=True)

# –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
numerical_columns = data.drop(columns=['Marketing expense']).columns
scaler = StandardScaler()
data[numerical_columns] = scaler.fit_transform(data[numerical_columns])

# —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
X = data.drop(columns=['Marketing expense']).values
y = data['Marketing expense'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)
"""
    elif idx == 3:
        code = """
# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
class SimpleModel(nn.Module):
    def __init__(self, input_dim):
        super(SimpleModel, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

    def forward(self, x):
        return self.network(x)

class ComplexModel(nn.Module):
    def __init__(self, input_dim):
        super(ComplexModel, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, x):
        return self.network(x)


def train_model(model, criterion, optimizer, epochs=100):
    train_losses = []
    val_losses = []

    for epoch in range(epochs):
        # Training phase
        model.train()
        optimizer.zero_grad()
        y_pred = model(X_train)
        train_loss = criterion(y_pred, y_train)
        train_loss.backward()
        optimizer.step()

        # Validation phase
        model.eval()
        with torch.no_grad():
            y_val_pred = model(X_test)
            val_loss = criterion(y_val_pred, y_test)

        train_losses.append(train_loss.item())
        val_losses.append(val_loss.item())

    return train_losses, val_losses    
"""
    elif idx == 4:
        code = """
# –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ
input_dim = X_train.shape[1]
models = {
    'Simple': SimpleModel(input_dim),
    'Complex': ComplexModel(input_dim)
}

results = {}

for model_name, model in models.items():
    print(f"Training {model_name} model...")
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

    train_losses, val_losses = train_model(model, criterion, optimizer)

    model.eval()
    with torch.no_grad():
        y_test_pred = model(X_test)
        mse = mean_squared_error(y_test.numpy(), y_test_pred.numpy())
        mae = mean_absolute_error(y_test.numpy(), y_test_pred.numpy())
        r2 = r2_score(y_test.numpy(), y_test_pred.numpy())

    results[model_name] = {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'mse': mse,
        'mae': mae,
        'r2': r2
    }

    # –≥—Ä–∞—Ñ–∏–∫ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
    plt.plot(train_losses, label=f'{model_name} - Train')
    plt.plot(val_losses, label=f'{model_name} - Val')

plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss Curves for Different Architectures')
plt.legend()
plt.show()


for model_name, metrics in results.items():
    print(f"Model: {model_name}")
    print(f"MSE: {metrics['mse']:.4f}, MAE: {metrics['mae']:.4f}, R2: {metrics['r2']:.4f}\n")
"""
    pyperclip.copy(code)


def mlp_class(idx: int = 0):
    if idx == 0:
        code = """
   import pandas as pd
import numpy as np

import torch
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader, TensorDataset

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import matplotlib.pyplot as plt
"""
    elif idx == 1:
        code = """
data = pd.read_csv("bank.csv")

X = data.drop(columns=["default"])
y = data["default"].map({"yes": 1, "no": 0})  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –≤ 0 –∏ 1

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∏ —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã
categorical_cols = ["job", "marital", "education", "housing", "loan", "contact", "month", "poutcome"]
numerical_cols = ["age", "balance", "day", "duration", "campaign", "pdays", "previous"]

# –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(), categorical_cols)
    ])

X_processed = preprocessor.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)
"""

    elif idx == 2:
        code = """
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)

train_dataset = TensorDataset(X_train, y_train)
test_dataset = TensorDataset(X_test, y_test)

batch_size = 64
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)    
"""

    elif idx == 3:
        code = """
class Model(nn.Module):
    def __init__(self, input_dim):
        super(Model, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.net(x)

model = Model(X_train.shape[1])
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

epochs = 100
print_every = 10
loss_history = []
"""

    elif idx == 4:
        code = """
for epoch in range(epochs):
    epoch_loss = 0
    model.train()
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()

    loss_history.append(epoch_loss / len(train_loader))

    if (epoch + 1) % print_every == 0:
        print(f'Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(train_loader):.4f}')

model.eval()
with torch.no_grad():
    y_pred = model(X_test)
    y_pred_class = (y_pred >= 0.5).float()
"""

    elif idx == 5:
        code = """
cm = confusion_matrix(y_test, y_pred_class)
print("Confusion Matrix:")
print(cm)

print("Classification Report:")
print(classification_report(y_test, y_pred_class))

fpr, tpr, _ = roc_curve(y_test, y_pred)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.show()

plt.figure()
plt.plot(range(1, epochs + 1), loss_history, label='Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.legend()
plt.show()
"""

    pyperclip.copy(code)


def cnn_class(idx: int = 0):
    if idx == 0:
        code = """
import torch
import torch.nn as nn
import torch.optim as optim

from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.datasets import ImageFolder
import matplotlib.pyplot as plt
"""
    elif idx == 1:
        code = """
data_dir = "chars"

def calculate_mean_std(dataset):
    loader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=2)
    mean = 0.0
    std = 0.0
    total_images = 0
    for images, _ in loader:
        batch_samples = images.size(0)
        images = images.view(batch_samples, images.size(1), -1)
        mean += images.mean(2).sum(0)
        std += images.std(2).sum(0)
        total_images += batch_samples

    mean /= total_images
    std /= total_images
    return mean, std

transform_for_stats = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor()
])      
"""
    elif idx == 2:
        code = """
stats_dataset = ImageFolder(data_dir, transform=transform_for_stats)
mean, std = calculate_mean_std(stats_dataset)

transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
    transforms.Normalize(mean=mean, std=std)
])

full_dataset = ImageFolder(data_dir, transform=transform)

train_size = int(0.7 * len(full_dataset))
test_size = len(full_dataset) - train_size
train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])
"""

    elif idx == 3:
        code = """
class CNN(nn.Module):
    def __init__(self, num_classes):
        super(CNN, self).__init__()
        self.conv_layers = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),

            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.fc_layers = nn.Sequential(
            nn.Linear(128 * 16 * 16, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        x = self.conv_layers(x)
        x = x.view(x.size(0), -1)
        x = self.fc_layers(x)
        return x      
"""

    elif idx == 4:
        code = """
batch_size = 64
num_epochs = 100
print_every = 10
num_classes = len(full_dataset.classes)

model = CNN(num_classes)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

loss_history = []

for epoch in range(num_epochs):
    epoch_loss = 0
    model.train()
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()

    loss_history.append(epoch_loss / len(train_loader))

    if (epoch + 1) % print_every == 0:
        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_loader):.4f}')      
"""

    elif idx == 5:
        code = """
model.eval()
with torch.no_grad():
    correct = 0
    total = 0
    for batch_X, batch_y in test_loader:
        outputs = model(batch_X)
        _, predicted = torch.max(outputs, 1)
        total += batch_y.size(0)
        correct += (predicted == batch_y).sum().item()

    print(f'Accuracy: {100 * correct / total:.2f}%')

import random

dataiter = iter(test_loader)
images, labels = next(dataiter)

model.eval()
with torch.no_grad():
    outputs = model(images)
    _, predictions = torch.max(outputs, 1)

fig, axes = plt.subplots(2, 2, figsize=(10, 10))
for i, ax in enumerate(axes.flat):
    img = images[i].permute(1, 2, 0) * torch.tensor(std).view(1, 1, -1) + torch.tensor(mean).view(1, 1, -1)
    img = img.clamp(0, 1)
    ax.imshow(img)
    ax.set_title(f'True: {full_dataset.classes[labels[i]]}\nPred: {full_dataset.classes[predictions[i]]}')
    ax.axis('off')
plt.tight_layout()
plt.show()

plt.figure()
plt.plot(range(1, num_epochs + 1), loss_history, label='Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.legend()
plt.show()
"""

    pyperclip.copy(code)


def cnn_class_2(idx: int = 0):
    if idx == 0:
        code = """
import torch
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms
from torchvision.datasets import ImageFolder
from torch.functional import F
import torch.nn as nn

dataset_path = 'sign_language'

transform = transforms.Compose([
    transforms.Resize((128, 128)), 
    transforms.ToTensor(),         
])

dataset = ImageFolder(root=dataset_path, transform=transform)

train_dataset, val_dataset = random_split(dataset, [0.8, 0.2])
"""
    elif idx == 1:
        code = """
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)


print(f"Total images: {len(dataset)}")
print(f"Training images: {len(train_dataset)}")
print(f"Validation images: {len(val_dataset)}")

# Iterate through the training DataLoader as an example
for images, labels in train_loader:
    print(f"Batch image shape: {images.shape}, Batch label shape: {labels.shape}")
    break
"""
    elif idx == 2:
        code = """
class CNN(nn.Module):
    def __init__(self, num_classes=10):
        super(CNN, self).__init__()

        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)  # Output: 16x128x128
        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1) # Output: 32x128x128
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Halves the dimensions
        
#         —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å —Å–∫–æ–ª—å–∫–æ –∑–¥–µ—Å—å –Ω–µ–π—Ä–æ–Ω–æ–≤
#         –∑–∞–ø—É—Å—Ç–∏–º –º–æ–¥–µ–ª—å –±–µ–∑ –ª–∏–Ω–µ–π–Ω—ã—Ö —Å–ª–æ–µ–≤ (—Ç–æ–ª—å–∫–æ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –∏ –ø—É–ª–ª–∏–Ω–≥–∏)
#         flatten —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏ –±—É–¥–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤
#         –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è 
        
        self.fc1 = nn.Linear(32 * 32 * 32, 128)  
        self.fc2 = nn.Linear(128, num_classes)   

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))  
        x = self.pool(F.relu(self.conv2(x)))  
        
        x = x.view(x.size(0), -1)  
        
        x = F.relu(self.fc1(x))
        x = self.fc2(x)  
        
        return x
    
# model = CNN()
# img = torch.unsqueeze(dataset[0][0], 0)
# output = model(img).flatten().shape

# print(output, 32**3)   
"""
    elif idx == 3:
        code = """
lr = 1e-3
epochs = 10

model = CNN()
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

for epoch in range(epochs):
    model.train()
    train_loss = 0.0
    correct = 0
    total = 0

    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * images.size(0)
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    train_accuracy = 100.0 * correct / total
    train_loss /= total
"""
    elif idx == 4:
        code = """
    # Validation loop
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            val_loss += loss.item() * images.size(0)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    val_accuracy = 100.0 * correct / total
    val_loss /= total

    print(f"Epoch {epoch+1}/{epochs}, "
          f"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, "
          f"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%")
"""
    pyperclip.copy(code)
