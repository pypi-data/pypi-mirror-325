from mistralai import Mistral
import pyperclip
import random

questions = {2.00: "\nfrom sklearn.metrics import r2_score\n\nX = data.drop(columns='cnt', axis=1)\ny = data['cnt'].values\nscaler_y = StandardScaler()\ny = scaler_y.fit_transform(y.reshape(-1, 1))\n\ncategorical_cols = X.select_dtypes(include=['object', 'category']).columns\ncategorical_cols\n\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\nnumerical_cols\n\nclass RegressionModel(nn.Module):\n    def __init__(self, input_size):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_size, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)",
             2.01: "\ndef train(model, train_loader, test_loader):\n    num_epochs = 5\n\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    train_losses = []\n    test_r2_scores = []\n\n    for epoch in range(num_epochs):\n        model.train()\n        epoch_loss = 0\n\n        for batch_X, batch_y in train_loader:\n            optimizer.zero_grad()\n            y_pred = model(batch_X)\n            loss = criterion(y_pred, batch_y)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n\n        avg_epoch_loss = epoch_loss/len(train_loader)\n        train_losses.append(avg_epoch_loss)\n\n        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}')\n\n        model.eval()\n        y_test_pred = []\n        y_test_true = []\n        with torch.no_grad():\n            for batch_X, batch_y in test_loader:\n                preds = model(batch_X)\n                y_test_pred.append(preds)\n                y_test_true.append(batch_y)\n\n        y_test_pred = torch.cat(y_test_pred).numpy()\n        y_test_true = torch.cat(y_test_true).numpy()\n\n        r2 = r2_score(y_test_true, y_test_pred)\n        test_r2_scores.append(r2)\n\n    return train_losses, test_r2_scores",
             2.02: "\nmodel = RegressionModel(X_train.shape[1])\ntrain_losses, test_r2_scores = train(model, train_loader, test_loader)\n\nplt.plot(train_losses)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()\n\nplt.plot(test_r2_scores)\nplt.xlabel('Epochs')\nplt.ylabel('R^2')\nplt.show()\n\ndef check(model, X_test, y_test):\n    model.eval()\n    with torch.no_grad():\n        preds = model(X_test)\n    return r2_score(y_test, preds)\n\nprint(f'R^2 без Dropout: {check(model, X_test, y_test):.4f}')",
             2.03: "\nclass RegressionModelDropout(nn.Module):\n    def __init__(self, input_size):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_size, 256),\n            nn.Dropout(p=0.3),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.Dropout(p=0.3),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\nmodel_dp = RegressionModelDropout(X_train.shape[1])\ntrain_losses_dp, test_r2_scores_dp = train(model_dp, train_loader, test_loader)\n\nplt.plot(train_losses_dp)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()\n\nplt.plot(test_r2_scores_dp)\nplt.xlabel('Epochs')\nplt.ylabel('R^2')\nplt.show()\n\nprint(f'R^2 c Dropout: {check(model_dp, X_test, y_test):.4f}')",
             2.04: "\nclass RegressionModelBatchNorm(nn.Module):\n    def __init__(self, input_size):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_size, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1)\n        )\n        \n    def forward(self, x):\n        return self.fc(x)\n    \nmodel_bn = RegressionModelBatchNorm(X_train.shape[1])\ntrain_losses_bn, test_r2_scores_bn = train(model_bn, train_loader, test_loader)\n\n# ---\n\nplt.plot(train_losses_bn)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()\n\n# ---\n\nplt.plot(test_r2_scores_bn)\nplt.xlabel('Epochs')\nplt.ylabel('R^2')\nplt.show()\n\n# ---\n\nprint(f'R^2 c BatchNorm: {check(model_bn, X_test, y_test):.4f}')",
             2.10: "\nfrom sklearn.metrics import r2_score\n\ndata.info()\n\ny_cols = ['Gold_T-7', 'Gold_T-14', 'Gold_T-22', 'Gold_T+22']\nX = data.drop(columns=y_cols)\ny = data[y_cols]\n\nX = torch.FloatTensor(X.values)\ny = torch.FloatTensor(y.values)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.float32)\n\ntrain_dataset = TensorDataset(X_train, y_train)\ntest_dataset = TensorDataset(X_test, y_test)\n\nclass RegressionModel(nn.Module):\n    def __init__(self, input_size):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_size, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 4)\n        )\n        \n    def forward(self, x):\n        return self.fc(x)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)",
             2.11: "\ndef train(model, opt, train_loader):\n    num_epochs = 5\n\n    criterion = nn.MSELoss()\n    optimizer = opt\n\n    train_losses = []\n\n    for epoch in range(num_epochs):\n        model.train()\n        epoch_loss = 0\n        \n        for batch_X, batch_y in train_loader:\n            optimizer.zero_grad()\n            y_pred = model(batch_X)\n            loss = criterion(y_pred, batch_y)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        \n        avg_epoch_loss = epoch_loss/len(train_loader)\n        train_losses.append(avg_epoch_loss)\n\n        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}')\n    \n    return train_losses\n\ndef check(model, X_test, y_test):\n    model.eval()\n    with torch.no_grad():\n        preds = model(X_test)\n    return r2_score(y_test, preds)",
             2.12: "\noptimizers = {\n    'Adam': lambda params: optim.Adam(params, lr=0.001),\n    'SGD': lambda params: optim.SGD(params, lr=0.05, momentum=0.7),\n    'RMSprop': lambda params: optim.RMSprop(params, lr=0.001)\n}\n\nfor name, optimizer_fn in optimizers.items():\n    print(f'\n\nОбучение с оптимизатором: {name}')\n    model = RegressionModel(X_train.shape[1])\n    optimizer = optimizer_fn(model.parameters())\n\n    train_losses = train(model, optimizer, train_loader)\n\n    plt.plot(train_losses)\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.show()\n\n    print(f'R^2 на тестовом множестве: {check(model, X_test, y_test):.4f}')",
             2.13: '\nconfigs = [\n    {\'model\': RegressionModel, \'epochs\': 5, \'lr\': 0.01, \'name\': \'модели c 5 эпохами, lr=0.001\'},\n    {\'model\': RegressionModel, \'epochs\': 10, \'lr\': 0.001, \'name\': \'модели c 10 эпохами, lr=0.05\'},\n]\n\noptimizers = {\n    \'Adam\': lambda params, lr: optim.Adam(params, lr=lr),\n    \'RMSprop\': lambda params, lr: optim.RMSprop(params, lr=lr)\n}\n\nfor config in configs:\n    for name, optimizer_fn in optimizers.items():\n        model = config[\'model\'](X_train.shape[1])\n        print(f\'\n\nОбучение {config["name"]}, opt={name}\')\n        optimizer = optimizer_fn(model.parameters(), lr=config[\'lr\'])\n\n        train_losses = train(model, optimizer, train_loader, num_epochs=config[\'epochs\'])\n\n        plt.plot(train_losses)\n        plt.xlabel(\'Epochs\')\n        plt.ylabel(\'Loss\')\n        plt.show()\n\n        print(f\'R^2 на тестовом множестве: {check(model, X_test, y_test):.4f}\') ',
             2.20: "\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\n\nX = data.drop(columns='deposit', axis=1)\ny = data['deposit']\ny = LabelEncoder().fit_transform(y)\n\ncategorical_cols = X.select_dtypes(include=['object', 'category']).columns\ncategorical_cols\n\nnumerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\nnumerical_cols\n\nclass Model(nn.Module):\n    def __init__(self, num_params):\n        super(Model, self).__init__()\n        self.network = nn.Sequential(\n             nn.Linear(num_params, 32),\n             nn.Sigmoid(),\n             nn.Linear(32, 16),\n             nn.Sigmoid(),\n             nn.Linear(16, 2)\n           )\n\n    def forward(self, x):\n        return self.network(x)",
             2.21: "\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\ndef train(model, opt, train_loader):\n    num_epochs = 5\n    criterion = nn.CrossEntropyLoss()\n    optimizer = opt\n\n    train_losses = []\n\n    for epoch in range(num_epochs):\n        model.train()\n        epoch_loss = 0\n        \n        for batch_X, batch_y in train_loader:\n            optimizer.zero_grad()\n            y_pred = model(batch_X)\n            loss = criterion(y_pred, batch_y)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        \n        avg_epoch_loss = epoch_loss/len(train_loader)\n        train_losses.append(avg_epoch_loss)\n\n        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}')\n    \n    return train_losses",
             2.22: '\noptimizers = {\n    "SGD": lambda params: optim.SGD(params, lr=0.0001, momentum = 0.9),\n    "Adam": lambda params: optim.Adam(params, lr= 0.001),\n    "AdamW": lambda params: optim.AdamW(params, lr= 0.001)\n}\n\nfor name, optimizer_fn in optimizers.items():\n    print(f\'\n\nОбучение с оптимизатором: {name}\')\n    model = Model(X_train.shape[1])\n    optimizer = optimizer_fn(model.parameters())\n\n    train_losses = train(model, optimizer, train_loader)\n\n    plt.plot(train_losses)\n    plt.xlabel(\'Epochs\')\n    plt.ylabel(\'Loss\')\n    plt.show()\n\n    model.eval()\n    with torch.no_grad():\n        y_pred_logits = model(X_test)\n        y_pred = torch.argmax(y_pred_logits, axis=1)\n\n    print(\'Classification Report:\')\n    print(classification_report(y_test, y_pred))\n\n    conf_matrix = confusion_matrix(y_test, y_pred)\n    print(\'Confusion Matrix:\')\n    print(conf_matrix)\n\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(conf_matrix, annot=True, fmt=\'d\', cmap=\'Blues\', xticklabels=[\'Class 0\', \'Class 1\'], yticklabels=[\'Class 0\', \'Class 1\'])\n    plt.xlabel(\'Predicted Labels\')\n    plt.ylabel(\'True Labels\')\n    plt.title(\'Confusion Matrix\')\n    plt.show()',
             2.23: "\nmodel = Model(X_train.shape[1])\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ntrain_losses = train(model, optimizer, train_loader)\n\nplt.plot(train_losses)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()\n\nmodel.eval()\nwith torch.no_grad():\n    y_pred_logits = model(X_test)\n    y_pred = torch.argmax(y_pred_logits, axis=1)\n\nprint('Classification Report:')\nprint(classification_report(y_test, y_pred))\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix:')\nprint(conf_matrix)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()",
             2.24: "\nclass Model_dp(nn.Module):\n    def __init__(self, num_params):\n        super(Model_dp, self).__init__()\n        self.network = nn.Sequential(\n             nn.Linear(num_params, 32),\n             nn.Sigmoid(),\n             nn.Dropout(p=0.3),\n             nn.Linear(32, 16),\n             nn.Sigmoid(),\n             nn.Dropout(p=0.3),\n             nn.Linear(16, 2)\n           )\n\n    def forward(self, x):\n        return self.network(x)\n\nmodel_dp = Model_dp(X_train.shape[1])\noptimizer = optim.Adam(model_dp.parameters(), lr=0.001)\n\ntrain_losses_dp = train(model_dp, optimizer, train_loader)\n\nplt.plot(train_losses_dp)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()\n\nmodel_dp.eval()\nwith torch.no_grad():\n    y_pred_logits = model_dp(X_test)\n    y_pred = torch.argmax(y_pred_logits, axis=1)\n\nprint('Classification Report:')\nprint(classification_report(y_test, y_pred))\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix:')\nprint(conf_matrix)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()",
             2.25: '\nmodel = Model(X_train.shape[1])\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ntrain_losses = train(model, optimizer, train_loader)\n\nplt.plot(train_losses)\nplt.xlabel(\'Epochs\')\nplt.ylabel(\'Loss\')\nplt.show()\n\nmodel.eval()\nwith torch.no_grad():\n    y_pred_logits = model(X_test)\n    y_pred = torch.argmax(y_pred_logits, axis=1)\n\nprint("Classification Report:")\nprint(classification_report(y_test, y_pred))\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint("Confusion Matrix:")\nprint(conf_matrix)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\'d\', cmap=\'Blues\', xticklabels=["Class 0", "Class 1"], yticklabels=["Class 0", "Class 1"])\nplt.xlabel(\'Predicted Labels\')\nplt.ylabel(\'True Labels\')\nplt.title(\'Confusion Matrix\')\nplt.show()',
             2.26: "\nclass_counts = torch.tensor([(y_train == 0).sum(), (y_train == 1).sum()], dtype=torch.float32)\npos_weight = class_counts[0] / class_counts[1]  \n\nnum_epochs = 5\nmodel_2 = Model(X_train.shape[1], num_classes=1)\ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\noptimizer = optim.Adam(model_2.parameters(), lr=0.001)\n\ntrain_losses = []\n\nfor epoch in range(num_epochs):\n    model_2.train()\n    epoch_loss = 0\n    \n    for batch_X, batch_y in train_loader:\n        optimizer.zero_grad()\n        y_pred = model_2(batch_X).squeeze(1) \n        batch_y = batch_y.float()\n        loss = criterion(y_pred, batch_y)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n    \n    avg_epoch_loss = epoch_loss/len(train_loader)\n    train_losses.append(avg_epoch_loss)\n\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}')\n\nplt.plot(train_losses)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()",
             3.00: '\nimport torch.nn.functional as F\nfrom sklearn.metrics import f1_score\nimport zipfile\n\nwith zipfile.ZipFile("chars.zip", "r") as zip_ref:\n    zip_ref.extractall("chars_data")\n\ndata_dir = \'chars_data/chars\'\n\n# Предобработка изображений\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)), \n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\nclass CNN(nn.Module):\n    def __init__(self, num_classes):\n        super(CNN, self).__init__()\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.fc_layers = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 64 * 64, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = self.fc_layers(x)\n        return x',
             3.01: "\ndef train(model, train_loader, test_loader):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)  \n    num_epochs = 10\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        \n        model.train()\n        for images, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n    model.eval()\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for images, labels in test_loader:\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            y_true.extend(labels.numpy())\n            y_pred.extend(preds.numpy())\n    \n    f1 = f1_score(y_true, y_pred, average='micro')\n    return f1\n\nnum_classes = len(full_dataset.classes)\nmodel = CNN(num_classes=num_classes)\nf1 = train(model, train_loader, test_loader)\nprint(f'\nF1: {f1:.4f}')",
             3.02: "\ntransform_transforms = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\nfull_dataset_2 = ImageFolder(data_dir, transform=transform_transforms)\n\ntrain_size_2 = int(0.7 * len(full_dataset_2))\ntest_size_2 = len(full_dataset_2) - train_size_2\ntrain_dataset_2, test_dataset_2 = torch.utils.data.random_split(full_dataset_2, [train_size_2, test_size_2])\n\ntrain_loader_2 = DataLoader(train_dataset_2, batch_size=64, shuffle=True)\ntest_loader_2 = DataLoader(test_dataset_2, batch_size=64)\n\nmodel_2 = CNN(num_classes=num_classes)\nf1_2 = train(model_2, train_loader_2, test_loader_2)\nprint(f'\nF1 на расширенном датасете: {f1_2:.4f}')",
             3.03: "\ndef evaluate_predictions(model, test_loader, threshold=0.7):\n    model.eval()\n    y_true, y_pred = [], []\n    uncertain_count = 0  \n    \n    with torch.no_grad():\n        for images, labels in test_loader:\n            outputs = model(images)\n            probabilities = F.softmax(outputs, dim=1)\n            max_probs, preds = torch.max(probabilities, 1)\n\n            uncertain_mask = max_probs < threshold\n\n            y_true.extend(labels.numpy())\n            y_pred.extend(preds.numpy())\n\n            uncertain_count += uncertain_mask.sum().item()\n\n    f1 = f1_score(y_true, y_pred, average='micro')\n    return f1, uncertain_count",
             3.04: "\ndef train(model, train_loader, test_loader, threshold=0.7):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)  \n    num_epochs = 5\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        \n        model.train()\n        for images, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n    f1, uncertain_count = evaluate_predictions(model, test_loader, threshold)\n    return f1, uncertain_count\n\nnum_classes = len(full_dataset.classes)\nmodel = CNN(num_classes=num_classes)\nfor threshold in [0.5, 0.7, 0.9]:\n    print(f'\n\nОбучение с порогом: {threshold}')\n    f1, uncertain_count = train(model, train_loader, test_loader, threshold)\n    print(f'\nF1: {f1:.4f}, Неопределенные: {uncertain_count}')",
             3.10: '\nfrom sklearn.metrics import f1_score\nimport zipfile\n\nwith zipfile.ZipFile("eng_handwritten.zip", "r") as zip_ref:\n    zip_ref.extractall("eng_handwritten_data")\n\ndata_dir = \'eng_handwritten_data/eng_handwritten\'\n\n# Предобработка изображений\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.CenterCrop(50),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5,), std=(0.5,))\n])\n\n# Загрузка данных\nfull_dataset = ImageFolder(data_dir, transform=transform)\n\ntrain_size = int(0.7 * len(full_dataset))\nval_size = int(0.15 * len(full_dataset))\ntest_size = len(full_dataset) - train_size - val_size\n\ntrain_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\nclass CNN(nn.Module):\n    def __init__(self, num_classes):\n        super(CNN, self).__init__()\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.fc_layers = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 12 * 12, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = self.fc_layers(x)\n        return x',
             3.11: "\ndef train(model, train_loader, val_loader, patience=5):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)  \n    num_epochs = 20\n    best_f1 = 0\n    \n    for epoch in range(num_epochs):\n        \n        model.train()\n        for images, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n        model.eval()\n        val_true, val_preds = [], []\n\n        with torch.no_grad():\n            for images, labels in val_loader:\n                outputs = model(images)\n                _, predicted = torch.max(outputs, 1)\n\n                val_true.extend(labels.numpy())\n                val_preds.extend(predicted.numpy())\n        \n        micro_f1 = f1_score(val_true, val_preds, average='micro')\n        print(f'Epoch {epoch+1}/{num_epochs}, micro-F1: {micro_f1:.4f}')\n\n        if micro_f1 > best_f1:\n            best_f1 = micro_f1\n            patience_counter = 0\n            torch.save(model.state_dict(), 'best_model.pth')\n        else:\n            patience_counter += 1\n        if patience_counter > patience:\n            print(f'Ранняя остановка, лучший micro-F1: {best_f1:.4f}')\n            break\n\nnum_classes = len(full_dataset.classes)\nmodel = CNN(num_classes=num_classes)\ntrain(model, train_loader, val_loader, patience=3)\n\nmodel.load_state_dict(torch.load('best_model.pth')) \nmodel.eval()\n\ntest_true, test_preds = [], []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        test_true.extend(labels.numpy())\n        test_preds.extend(predicted.numpy())\n\nmicro_f1_test = f1_score(test_true, test_preds, average='micro')\nprint(f'micro-F1 на тестовом множестве: {micro_f1_test:.4f}')",
             3.12: "\ndef train(model, train_loader, val_loader):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)  \n    num_epochs = 10\n    best_f1 = 0\n    \n    for epoch in range(num_epochs):\n        \n        model.train()\n        for images, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n        model.eval()\n        val_true, val_preds = [], []\n\n        with torch.no_grad():\n            for images, labels in val_loader:\n                outputs = model(images)\n                _, predicted = torch.max(outputs, 1)\n\n                val_true.extend(labels.numpy())\n                val_preds.extend(predicted.numpy())\n        \n        micro_f1 = f1_score(val_true, val_preds, average='micro')\n        print(f'Epoch {epoch+1}/{num_epochs}, micro-F1 val: {micro_f1:.4f}')\n\n# --\n\nnum_classes = len(full_dataset.classes)\nmodel = CNN(num_classes=num_classes)\ntrain(model, train_loader, val_loader)\n\n# --\n\nmodel.eval()\ntest_true, test_preds = [], []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        test_true.extend(labels.numpy())\n        test_preds.extend(predicted.numpy())\n\nmicro_f1_test = f1_score(test_true, test_preds, average='micro')\nprint(f'micro-F1 на тестовом множестве: {micro_f1_test:.4f}')",
             3.13: "\nimage, label = random.choice(test_dataset)\n\ntransformations = [\n    transforms.RandomRotation(30), \n    transforms.ColorJitter(brightness=0.5),  \n    transforms.GaussianBlur(kernel_size=5) \n]\n\nmodified_images = [transform(image) for transform in transformations]\nimage_batch = torch.stack([image])  \nfor img in modified_images:\n    image_batch = torch.cat([image_batch, img.unsqueeze(0)], dim=0)\n\n\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(image_batch)  \n    probabilities = torch.softmax(outputs, dim=1) \n    predicted_classes = torch.argmax(probabilities, dim=1)\n\nfig, axes = plt.subplots(1, 4, figsize=(12, 4))\ntitles = ['Оригинал', 'Поворот', 'Яркость', 'Размытие']\n\nfor i, (img, title, pred) in enumerate(zip([image] + modified_images, titles, predicted_classes)):\n    axes[i].imshow(img.permute(1, 2, 0)) \n    axes[i].set_title(f'{title}\nПрогноз: {pred.item()}')\n    axes[i].axis('off')\n\nplt.show()\n\n# --\n\nprint(label)",
             3.20: '\nfrom sklearn.metrics import f1_score\nimport zipfile\nimport random\nimport matplotlib.pyplot as plt\n\nwith zipfile.ZipFile("sign_language.zip", "r") as zip_ref:\n    zip_ref.extractall("sign_language_data")\n    \ndata_dir = \'sign_language_data/sign_language\'\n\n# Предобработка изображений\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5,), std=(0.5,))\n])\n\nclass CNN(nn.Module):\n    def __init__(self, num_classes):\n        super(CNN, self).__init__()\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.fc_layers = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 64 * 64, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = self.fc_layers(x)\n        return x',
             3.21: "\ndef train(model, train_loader):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)  \n    num_epochs = 5\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n\n        model.train()\n        for images, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n      \nnum_classes = len(full_dataset.classes)\nmodel = CNN(num_classes=num_classes)\ntrain(model, train_loader)\n\ndef check(model, test_loader):\n    model.eval()\n    test_true, test_preds = [], []\n\n    with torch.no_grad():\n        for images, labels in test_loader:\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            test_true.extend(labels.numpy())\n            test_preds.extend(predicted.numpy())\n\n    micro_f1_test = f1_score(test_true, test_preds, average='micro')\n    return micro_f1_test",
             3.22: "\nclass CNN_2(nn.Module):\n    def __init__(self, num_classes):\n        super(CNN_2, self).__init__()\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.fc_layers = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(128 * 128 * 32, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = self.fc_layers(x)\n        return x\n    \nmodel_2 = CNN_2(num_classes=num_classes)\ntrain(model_2, train_loader)\n\nclass CNN_3(nn.Module):\n    def __init__(self, num_classes):\n        super(CNN_3, self).__init__()\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.fc_layers = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(128 * 32 * 32, 256),\n            nn.ReLU(),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = self.fc_layers(x)\n        return x\n    \nmodel_3 = CNN_3(num_classes=num_classes)\ntrain(model_3, train_loader)\n\nf1_1 = check(model, test_loader)\nprint(f'micro-F1 модели №1 на тестовом множестве: {f1_1:.4f}')\nf1_2 = check(model_2, test_loader)\nprint(f'micro-F1 модели №2 на тестовом множестве: {f1_2:.4f}')\nf1_3 = check(model_3, test_loader)\nprint(f'micro-F1 модели №3 на тестовом множестве: {f1_3:.4f}')\n\nblocks = [1, 2, 3]\nf1_ = [f1_2, f1_1, f1_3]\n\nplt.plot(blocks, f1_)\nplt.xlabel('Количество блоков')\nplt.ylabel('micro-F1')\nplt.show()",
             3.23: "\ndef train(model, train_loader):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)  \n    num_epochs = 5\n    train_losses = []\n    \n    for epoch in range(num_epochs):\n        epoch_loss = 0\n        \n        model.train()\n        for images, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        \n        avg_epoch_loss = epoch_loss/len(train_loader)\n        train_losses.append(avg_epoch_loss)\n\n        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_epoch_loss:.4f}')\n        \n    return train_losses\n\nnum_classes = len(full_dataset.classes)\nmodel = CNN(num_classes=num_classes)\ntrain_losses = train(model, train_loader)\n\nplt.plot(train_losses)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()\n\nmodel.eval()\ny_true, y_pred = [], []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images, labels\n        outputs = model(images)\n        _, preds = torch.max(outputs, 1)\n        y_true.extend(labels.numpy())\n        y_pred.extend(preds.numpy())\n\ncm = confusion_matrix(y_true, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=full_dataset.classes, yticklabels=full_dataset.classes)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\nprint(classification_report(y_true, y_pred, target_names=full_dataset.classes))",
             3.24: "\nclass CNN_FeatureExtractor(CNN):\n    def __init__(self, num_classes):\n        super(CNN_FeatureExtractor, self).__init__(num_classes)\n    \n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = self.fc_layers[0](x)  \n        return x\n    \ndef extract_features(model, dataloader):\n    model.eval()\n    features = []\n    labels = []\n    \n    with torch.no_grad():\n        for images, lbls in dataloader:\n            feats = model(images)\n            features.append(feats.numpy())\n            labels.append(lbls.numpy())\n    \n    return np.vstack(features), np.hstack(labels)\n\ndef plot_pca(features, labels):\n    pca = PCA(n_components=2)\n    reduced_features = pca.fit_transform(features)\n\n    plt.figure(figsize=(8, 6))\n    scatter = plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=labels, cmap='jet', alpha=0.7)\n    plt.colorbar(scatter, label='Class')\n    plt.xlabel('Компонента 1')\n    plt.ylabel('Компонента 2')\n    plt.show()\n\nmodel = CNN_FeatureExtractor(num_classes=num_classes) \nfeatures, labels = extract_features(model, test_loader)\n\nplot_pca(features, labels)",
             3.25: "\ndef train(model, train_loader):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)  \n    num_epochs = 5\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n\n        model.train()\n        for images, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n          \nnum_classes = len(full_dataset.classes)\nmodel = CNN(num_classes=num_classes)\ntrain(model, train_loader)\n\nmodel.eval()\ny_true, y_pred = [], []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images, labels\n        outputs = model(images)\n        _, preds = torch.max(outputs, 1)\n        y_true.extend(labels.numpy())\n        y_pred.extend(preds.numpy())\n\ncm = confusion_matrix(y_true, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=full_dataset.classes, yticklabels=full_dataset.classes)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\nprint(classification_report(y_true, y_pred, target_names=full_dataset.classes))",
             3.26: "\nclass CNN_FeatureExtractor(CNN):\n    def __init__(self, num_classes):\n        super(CNN_FeatureExtractor, self).__init__(num_classes)\n    \n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = self.fc_layers[0](x)  \n        return x\n\ndef extract_features(model, dataloader):\n    model.eval()\n    features, labels, images = [], [], []\n\n    with torch.no_grad():\n        for imgs, lbls in dataloader:\n            feats = model(imgs)\n            features.append(feats.numpy())\n            labels.append(lbls.numpy())\n            images.append(imgs.numpy())  \n\n    return np.vstack(features), np.hstack(labels), np.vstack(images)\n\nmodel_feat = CNN_FeatureExtractor(num_classes)\nfeatures, labels, images = extract_features(model_feat, test_loader)\nwrong_indices = np.where(np.array(y_true) != np.array(y_pred))[0]\nwrong_idx = wrong_indices[0] \n\nquery_feat = features[wrong_idx]  \n\nsimilarities = cosine_similarity([query_feat], features)[0]\ntop_indices = np.argsort(similarities)[::-1][1:6]\n\nfig, axes = plt.subplots(1, 6, figsize=(12, 4))\naxes[0].imshow(np.transpose(images[wrong_idx], (1, 2, 0)))  \naxes[0].set_title('Ошибка (Оригинал)')\naxes[0].axis('off')\n\nfor i, idx in enumerate(top_indices):\n    axes[i + 1].imshow(np.transpose(images[idx], (1, 2, 0)))\n    axes[i + 1].set_title(f'Похожее {i+1}')\n    axes[i + 1].axis('off')\n\nplt.show()",
             3.30: '\nfrom sklearn.preprocessing import LabelEncoder\nimport zipfile\n\nwith zipfile.ZipFile("clothes_multi.zip", "r") as zip_ref:\n    zip_ref.extractall("clothes_multi_data")\n\ndata_dir = \'clothes_multi_data/clothes_multi\'\n\n# Предобработка изображений\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5,), std=(0.5,))\n])\n\n# Загрузка данных\nfull_dataset = ImageFolder(data_dir, transform=transform)\n\ncolors = set()\nclothes = set()\n\nfor folder_name, _ in full_dataset.class_to_idx.items():\n    color, clothing = folder_name.split(\'_\')  \n    colors.add(color)\n    clothes.add(clothing)\n\ncolor_encoder = LabelEncoder()\nclothing_encoder = LabelEncoder()\n\ncolor_encoder.fit(list(colors))\nclothing_encoder.fit(list(clothes))\n\ndef custom_target_transform(target_idx):\n    folder_name = list(full_dataset.class_to_idx.keys())[target_idx]\n    color, clothing = folder_name.split(\'_\')\n    return torch.tensor([color_encoder.transform([color])[0], clothing_encoder.transform([clothing])[0]])\n\ndataset = ImageFolder(root=data_dir, transform=transform, target_transform=custom_target_transform)',
             3.31: "\nclass CNN(nn.Module):\n    def __init__(self, num_colors, num_clothes):\n        super(CNN, self).__init__()\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.fc_layers = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 64 * 64, 128),\n            nn.ReLU()\n        )\n\n        self.fc_color = nn.Linear(128, num_colors)\n        self.fc_clothing = nn.Linear(128, num_clothes)\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = self.fc_layers(x)\n        color_out = self.fc_color(x)\n        clothing_out = self.fc_clothing(x)\n        return color_out, clothing_out\n\ndef train(model, train_loader):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)  \n    num_epochs = 5\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        \n        for images, targets in train_loader:\n            optimizer.zero_grad()\n            targets = targets.T.long()  \n            color_labels, clothing_labels = targets[0], targets[1]\n\n            color_out, clothing_out = model(images)\n\n            loss_color = criterion(color_out, color_labels)\n            loss_clothing = criterion(clothing_out, clothing_labels)\n            loss = loss_color + loss_clothing\n\n            loss.backward()\n            optimizer.step()",
             3.32: "\ndef check(model, data_loader):\n    model.eval()\n    y_true_color, y_pred_color = [], []\n    y_true_clothing, y_pred_clothing = [], []\n\n    with torch.no_grad():\n        for images, targets in data_loader:\n            targets = targets.T\n            color_labels, clothing_labels = targets[0], targets[1]\n\n            color_out, clothing_out = model(images)\n            _, preds_color = torch.max(color_out, 1)\n            _, preds_clothing = torch.max(clothing_out, 1)\n\n            y_true_color.extend(color_labels.numpy())\n            y_pred_color.extend(preds_color.numpy())\n            y_true_clothing.extend(clothing_labels.numpy())\n            y_pred_clothing.extend(preds_clothing.numpy())\n\n    f1_color = f1_score(y_true_color, y_pred_color, average='macro')\n    f1_clothing = f1_score(y_true_clothing, y_pred_clothing, average='macro')\n\n    return f1_color, f1_clothing\n\nnum_classes = len(full_dataset.classes)\nmodel = CNN(num_colors=len(colors), num_clothes=len(clothes))\ntrain(model, train_loader)\n\ntrain_f1_color, train_f1_clothing = check(model, train_loader)\nprint(f'Train F1 Цвет: {train_f1_color:.4f}, Train F1 Предмет: {train_f1_clothing:.4f}')\n\n\ntest_f1_color, test_f1_clothing = check(model, test_loader)\nprint(f'Test F1 Цвет: {test_f1_color:.4f}, Test F1 Предмет: {test_f1_clothing:.4f}')"
}

themes = '''
2.00 Общая часть в датасете BIKE_CNT
2.01 Функция обучения
2.02 Обучение
2.03 Dropout
2.04 BatchNorm
2.10 Общая часть в датасете GOLD
2.11 Функция обучения
2.12 Оптимизаторы
2.13 Гиперпараметры
2.20 Общая часть в датасете BANK
2.21 Функция обучения
2.22 Оптимизаторы
2.23 Dropout часть 1
2.24 Dropout часть 2
2.25 Модификация функции потерь часть 1
2.26 Модификация функции потерь часть 2
3.00 Общая часть в датасете CHARS
3.01 F1
3.02 F1 с преобразованиями
3.03 Логика постобработки
3.04 Обучение с логикой
3.10 Общая часть в датасете ENG_HANDWRITTEN
3.11 Ранняя остановка
3.12 Случайный выбор изображения часть 1
3.13 Случайный выбор изображения часть 2
3.20 Общая часть в датасете SIGN_LANGUAGE
3.21 От количества сверточных слоев часть 1
3.22 От количества сверточных слоев часть 2
3.23 PCA часть 1
3.24 PCA часть 2
3.25 Поиск похожих изображений часть 1
3.26 Поиск похожих изображений часть 2
3.30 Предобработка CLOTHES_MULTI
3.31 Модель и обучение
3.32 Результаты
'''

api_keys = ['uQwjntCIJ9omN9z8jLTV1VOUvYlbaDIv',
            'tgPxn1rFujtuJe1H0j4PQTSjbLthvjyO',
            'Z0XHjxUHj6QXJblxACLxDhJoQAUreqt4',
            'iOSObqKAYAliBWRglH4gpZb7JN0KF91j',
            'E42w9TME0Ykm1WMsVwdzS6DxV9q2Xhgx',
            'uQmDdzM1nrw3cbmksP1BWhRnjOKGLWi1',
            'AhRiKjuHaXm4WccJ0BDI0NRBJ1X9RxSE',
            'mtddNQsqAMbL5GNHroRNRsSOwOr8vusH',
            'tpxt5xsU7jetD1x9u9r0IiKxqwajlTXO',
            'bSJCJVsREAKubT9AgGfgYL6pojIzoK11',
            'icjd6jfH7hmPxNMSEyD70UKg13kbtaB5',
            'Oxz67oTMVHw48CJhJZOKLkHw1eAlTwki',
            'zVrmIUh4z2wEjS1ze9XpJtfbGQTGognI',
            'ez9voi9pZb7CwPbqrglrTSL59GgUZFCX',
            'BQUqloQ3WynP4ySfHhTOxz44Diidniq1'
            ]

model = "mistral-large-latest"

dfs = {'bank' : {'unique' : 'Набор данных: classification/bank.csv. Используя библиотеку PyTorch, pешите задачу классификации (столбец deposit). Разделите набор данных на обучающее и тестовое множество. Выполните предобработку данных (корректно обработайте случаи категориальных и нечисловых столбцов, при наличии). Отобразите график значений функции потерь на обучающем множестве по эпохам. Отобразите confusion matrix и classification report, pассчитанные на основе тестового множества',
                 'example' : '\n        \tage\tjob\tmarital\teducation\tdefault\tbalance\thousing\tloan\tcontact\tday\tmonth\tduration\tcampaign\tpdays\tprevious\tpoutcome\tdeposit\n0\t58\tmanagement\tmarried\ttertiary\tno\t2143\tyes\tno\tunknown\t5\tmay\t261\t1\t-1\t0\tunknown\tno\n1\t44\ttechnician\tsingle\tsecondary\tno\t29\tyes\tno\tunknown\t5\tmay\t151\t1\t-1\t0\tunknown\tno\n2\t33\tentrepreneur\tmarried\tsecondary\tno\t2\tyes\tyes\tunknown\t5\tmay\t76\t1\t-1\t0\tunknown\tno\n3\t47\tblue-collar\tmarried\tunknown\tno\t1506\tyes\tno\tunknown\t5\tmay\t92\t1\t-1\t0\tunknown\tno\n4\t33\tunknown\tsingle\tunknown\tno\t1\tno\tno\tunknown\t5\tmay\t198\t1\t-1\t0\tunknown\tno\n        '},
        'bike_cnt' : {'unique' : 'Набор данных: regression/bike_cnt.csv. Используя библиотеку PyTorch, решите задачу предсказания столбца cnt (задача регрессии). Разделите набор данных на обучающее и тестовое множество. Выполните предобработку данных (корректно обработайте случаи категориальных и нечисловых столбцов, при наличии). Отобразите графики значений функции потерь и метрики R*2 на обучающем множестве по эпохам. Рассчитайте значение метрики R 2 на тестовом множестве.',
                 'example' : '\n        \t\tinstant\tdteday\tseason\tyr\tmnth\thr\tholiday\tweekday\tworkingday\tweathersit\ttemp\tatemp\thum\twindspeed\tcnt\n0\t1\t2011-01-01\t1\t0\t1\t0\t0\t6\t0\t1\t0.24\t0.2879\t0.81\t0.0\t16\n1\t2\t2011-01-01\t1\t0\t1\t1\t0\t6\t0\t1\t0.22\t0.2727\t0.80\t0.0\t40\n2\t3\t2011-01-01\t1\t0\t1\t2\t0\t6\t0\t1\t0.22\t0.2727\t0.80\t0.0\t32\n3\t4\t2011-01-01\t1\t0\t1\t3\t0\t6\t0\t1\t0.24\t0.2879\t0.75\t0.0\t13\n4\t5\t2011-01-01\t1\t0\t1\t4\t0\t6\t0\t1\t0.24\t0.2879\t0.75\t0.0\t1\n\n        '},
        'gold' : {'unique' : 'Набор данных: regression/gold.csv. Используя библиотеку PyTorch, решите задачу одновременного предсказания столбцов Gold_T-7, Gold_T-14, Gold_T-22 и Gold_T+22 (задача регрессии). Разделите набор данных на обучающее и тестовое множество. Выполните предобработку данных (корректно обработайте случаи категориальных и нечисловых столбцов, при наличии).',
                 'example' : '\n\t3M Libor_T-1\tEM Bond_T-1\tUS Bond ETF_T-1\tWheat 1_T-1\tWheat 2_T-1\tCrude Palm Oil_T-1\tBrent 1_T-1\tBrent 2_T-1\tLight Crude_T-1\tWhite Sugar_T-1\t...\tMSCI EM_T-22\tShanghai Index_T-22\tNikkei Ind_T-22\tMSCI APAC ex J_T-22\tEMEquity ETF_T-22\tSilver_T-22\tPlatinum_T-22\tPalladium_T-22\tGold_T-22\tGold_T+22\n0\t0.000000\t0.000879\t0.001050\t0.013158\t0.013562\t0.003163\t-0.007879\t-0.004945\t-0.003486\t-0.024706\t...\t-0.009862\t0.073090\t-0.028816\t-0.001613\t-0.012274\t-0.025210\t0.024300\t0.082695\t0.053534\t0.004648\n1\t-0.004587\t0.004096\t0.002383\t0.043599\t0.041035\t0.040991\t0.042356\t0.039759\t0.029667\t0.047906\t...\t0.006801\t0.099088\t-0.043704\t0.007670\t0.018151\t0.060624\t0.068595\t0.151468\t0.085822\t-0.002105\n2\t-0.003465\t-0.002040\t0.000856\t0.020444\t0.017995\t0.003462\t0.006222\t0.006541\t0.007172\t0.005262\t...\t0.014873\t0.102887\t-0.051438\t0.007525\t0.028712\t0.056289\t0.083520\t0.134763\t0.083915\t-0.037438\n3\t-0.004624\t0.000584\t-0.002090\t-0.015679\t-0.012626\t0.021992\t0.003029\t0.004249\t0.002498\t0.020775\t...\t0.004931\t0.087208\t-0.054678\t0.005445\t0.001445\t0.041620\t0.061601\t0.109940\t0.076738\t-0.028097\n4\t-0.009292\t-0.000486\t0.001047\t-0.003540\t-0.002558\t-0.012658\t-0.024912\t-0.022026\t-0.023551\t-0.025000\t...\t-0.003838\t0.076564\t-0.076171\t-0.009558\t-0.003904\t0.058352\t0.069414\t0.094311\t0.084487\t-0.045466\n        '},
        'chars' : {'unique' : 'Набор данных: images/chars.zip. Реализовав сверточную нейронную сеть при помощи библиотеки РyTorch, решите задачу классификации изображений. Разделите набор данных на обучающее и тестовое множество. Выполните предобработку данных (приведите изображения к одному размеру, нормализуйте и преобразуйте в тензоры).',
                 'example' : 'Данные устроены так: chars.zip -> chars -> папка 1 marvel (внутри изображения типа .jpg) папка 2 star-wars (внутри изображения типа .jpg)'},
        'clothes_multi' : {'unique' : 'Набор данных: images/clothes_multi.zip. Реализовав сверточную нейронную сеть при помощи библиотеки PyTorch, решите задачу множественной (multi-label) классификации изображений. Для каждого изображения модель должна предсказывать два класса: цвет и предмет одежды. Разделите набор данных на обучающее и тестовое множество. Выполните предобработку данных (приведите изображения к одному размеру, нормализуйте и преобразуйте в тензоры).',
                 'example' : "\n        Данные устроены так: \n        clothes_multi/\n    green_shirt/\n    black_shoes/\n    silver_skirt/\n    red_dress/\n    pink_hoodie/\n    black_shirt/\n    white_dress/\n    green_shoes/\n    green_shorts/\n    white_shorts/\n    green_suit/\n    black_pants/\n    pink_pants/\n    brown_hoodie/\n    blue_dress/\n    green_pants/\n    black_suit/\n    red_hoodie/\n    red_shoes/\n    brown_pants/\n    black_dress/\n    yellow_shorts/\n    blue_pants/\n    black_shorts/\n    red_shirt/\n    white_shoes/\n    yellow_skirt/\n    pink_skirt/\n    silver_shoes/\n    blue_shirt/\n    white_suit/\n    brown_shoes/\n    red_pants/\n    yellow_dress/\n    blue_shorts/\n    white_pants/\n    blue_shoes/\n\n    В каждой из папок изображение типа '.jpg\n        "},
        'eng_handwritten' : {'unique' : 'Набор данных: images/eng handwritten.zip. Реализовав сверточную нейронную сеть при помощи библиотеки PyTorch, решите задачу классификации изображений. Разделите набор данных на обучающее, валидационное и тестовое множество. Выполните предобработку данных (вырежьте центральную область изображений одинакового размера и преобразуйте изображения в тензоры).',
                 'example' : "\n        Данные устроены так: \n        eng_handwritten/\n    R/\n    U/\n    I/\n    N/\n    G/\n    Z/\n    T/\n    S/\n    A/\n    F/\n    O/\n    H/\n    M/\n    J/\n    C/\n    D/\n    V/\n    Q/\n    X/\n    E/\n    B/\n    K/\n    L/\n    Y/\n    P/\n    W/\n    В каждой из папок изображения типа '.png'\n        "},
        'sign_language' : {'unique' : '3. Набор данных: images/sign_language.zip. Реализовав сверточную нейронную сеть при помощи библиотеки PyTorch, решите задачу классификации изображений. Разделите набор данных на обучающее и тестовое множество. Выполните предобработку данных (приведите изображения к одному размеру, нормализуйте и преобразуйте изображения в тензоры). Отобразите графики значений функции потерь по эпохам на обучающем множестве. Отобразите confusion matrix и classification report, рассчитанные на основе тестового множества. ',
                 'example' : "\n        Данные устроены так: \n        sign_language/\n    9/\n    0/\n    7/\n    6/\n    1/\n    8/\n    4/\n    3/\n    2/\n    5/\n\n    В каждой из папок изображения типа '.JPG'\n        "},
        
}

def info():
    '''
    Добавляет в буфер обмена список тем, по которым потом обращаться при помощи функции get(n, m), где n - номер темы, m = 0 => теория, m = 1 => практика
    '''
    pyperclip.copy(themes)

class info_cl():
    '''
    Создает класс, в документации которого список тем, по которым потом обращаться при помощи функции get(n, m), где n - номер темы, m = 0 => теория, m = 1 => практика
    '''
    __doc__ = themes
    

def get(n):
    '''
    Добавляет в буфер обмена ответ по теме (n - номер темы)
    '''
    if 0 < n < len(questions) + 1:
        pyperclip.copy(questions[n])
    else:
        pyperclip.copy('Неправильный выбор номера темы')

class get_cl:
    '''
    Добавляет в документацию класса ответ по теме (n - номер темы)
    '''
    def __init__(self, n):
        self.n = n
        self.doc = questions[self.n]

    @property
    def __doc__(self):
        return self.doc  

def m_get(message, dataset : str = None):
    '''
    message: запрос, dataset : название вашего датасета (если не вводить, то запрос делается без указания датасета)
    '''

    api_key = random.choice(api_keys)
    client = Mistral(api_key=api_key)
    if dataset:
        unique = dfs[dataset]['unique']
        example = dfs[dataset]['example']
        try:
            chat_response = client.chat.complete(
                model=model,
                messages=[
                    {
                        "role": "user",
                        "content": unique + message + example,
                    },
                ]
            )
            response_content = chat_response.choices[0].message.content
        except:
            response_content = 'Либо неправильное название датасета, либо ошибка на стороне мистраля'
    
    else:
        try:
            chat_response = client.chat.complete(
                model=model,
                messages=[
                    {
                        "role": "user",
                        "content": message ,
                    },
                ]
            )
            response_content = chat_response.choices[0].message.content
        except:
            response_content = 'Ошибка на стороне мистраля'

    pyperclip.copy(response_content)

class m_get_cl:
    def __init__(self, message, dataset : str = None):
        '''
        message: запрос, dataset : название вашего датасета (если не вводить, то запрос делается без указания датасета)
        '''

        api_key = random.choice(api_keys)
        client = Mistral(api_key=api_key)
        if dataset:
            unique = dfs[dataset]['unique']
            example = dfs[dataset]['example']
            try:
                chat_response = client.chat.complete(
                    model=model,
                    messages=[
                        {
                            "role": "user",
                            "content": unique + message + example,
                        },
                    ]
                )
                response_content = chat_response.choices[0].message.content
            except:
                response_content = 'Либо неправильное название датасета, либо ошибка на стороне мистраля'
        
        else:
            try:
                chat_response = client.chat.complete(
                    model=model,
                    messages=[
                        {
                            "role": "user",
                            "content": message ,
                        },
                    ]
                )
                response_content = chat_response.choices[0].message.content
            except:
                response_content = 'Ошибка на стороне мистраля'

        self.answer = response_content

    @property
    def __doc__(self):
        return self.answer
